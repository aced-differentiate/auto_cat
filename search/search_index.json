{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AutoCat Documentation AutoCat is a suite of python tools for sequential learning for materials applications and automating structure generation for DFT catalysis studies. Development of this package stems from ACED , as part of the ARPA-E DIFFERENTIATE program. Below we provide an overview of the key functionalities of AutoCat. For additional details please see the User Guide, Tutorials, and API sections. Sequential Learning One of the core philosophies of AutoCat is to provide modular and extensible tooling to facilitate closed-loop computational materials discovery workflows. Within this submodule are classes for defining a design space, featurization, regression, selecting candidate systems, and defining a closed-loop sequential learning iterator. The key classes intended for each of these purposes are: DesignSpace : define a design space to explore Featurizer : featurize the systems for regression Predictor : a regressor for predicting materials properties CandidateSelector : propose candidate system(s) for evaluation SequentialLearner : define a closed-loop iterator Structure Generation This submodule contains functions for automating atomic structure generation within the context of a catalysis study using density functional theory. Specifically, this includes generating bulk structures, surfaces, and placing adsorbates. In addition, functions for generating the single-atom alloys material class are also included. These functions are organized within AutoCat as follows: autocat.bulk : generation of periodic mono-elemental bulk structures autocat.surface : mono-elemental surface slab generation autocat.adsorption : placement of adsorbates onto surfaces autocat.saa : generation of single-atom alloy surfaces Structures generated or read with this package are typically of the form of ase.Atoms objects. When opting to write structures to disk using these functions, they are automatically organized into a clean, scalable directory organization. All structures are written in the ase.io.Trajectory file format. For further details on the directory structure, see the User Guide. Installation There are two options for installation, either via pip or from the repo directly. pip (recommended) If you are planning on strictly using AutoCat rather than contributing to development, we recommend using pip within a virtual environment (e.g. conda ). This can be done as follows: pip install autocat Github (for developers) Alternatively, if you would like to contribute to the development of this software, AutoCat can be installed via a clone from Github. First, you'll need to clone the github repo to your local machine (or wherever you'd like to use AutoCat) using git clone . Once the repo has been cloned, you can install AutoCat as an editable package by changing into the created directory (the one with setup.py ) and installing via: pip install -e . Contributing Contributions through issues, feature requests, and pull requests are welcome. Guidelines are provided here . Acknowledgements The code presented herein was funded by the Advanced Research Projects Agency-Energy (ARPA-E), U.S. Department of Energy, under Award Number DE-AR0001211 and in part by the National Science Foundation, under Award Number CBET-1554273. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.","title":"Home"},{"location":"#autocat-documentation","text":"AutoCat is a suite of python tools for sequential learning for materials applications and automating structure generation for DFT catalysis studies. Development of this package stems from ACED , as part of the ARPA-E DIFFERENTIATE program. Below we provide an overview of the key functionalities of AutoCat. For additional details please see the User Guide, Tutorials, and API sections.","title":"AutoCat Documentation"},{"location":"#sequential-learning","text":"One of the core philosophies of AutoCat is to provide modular and extensible tooling to facilitate closed-loop computational materials discovery workflows. Within this submodule are classes for defining a design space, featurization, regression, selecting candidate systems, and defining a closed-loop sequential learning iterator. The key classes intended for each of these purposes are: DesignSpace : define a design space to explore Featurizer : featurize the systems for regression Predictor : a regressor for predicting materials properties CandidateSelector : propose candidate system(s) for evaluation SequentialLearner : define a closed-loop iterator","title":"Sequential Learning"},{"location":"#structure-generation","text":"This submodule contains functions for automating atomic structure generation within the context of a catalysis study using density functional theory. Specifically, this includes generating bulk structures, surfaces, and placing adsorbates. In addition, functions for generating the single-atom alloys material class are also included. These functions are organized within AutoCat as follows: autocat.bulk : generation of periodic mono-elemental bulk structures autocat.surface : mono-elemental surface slab generation autocat.adsorption : placement of adsorbates onto surfaces autocat.saa : generation of single-atom alloy surfaces Structures generated or read with this package are typically of the form of ase.Atoms objects. When opting to write structures to disk using these functions, they are automatically organized into a clean, scalable directory organization. All structures are written in the ase.io.Trajectory file format. For further details on the directory structure, see the User Guide.","title":"Structure Generation"},{"location":"#installation","text":"There are two options for installation, either via pip or from the repo directly.","title":"Installation"},{"location":"#pip-recommended","text":"If you are planning on strictly using AutoCat rather than contributing to development, we recommend using pip within a virtual environment (e.g. conda ). This can be done as follows: pip install autocat","title":"pip (recommended)"},{"location":"#github-for-developers","text":"Alternatively, if you would like to contribute to the development of this software, AutoCat can be installed via a clone from Github. First, you'll need to clone the github repo to your local machine (or wherever you'd like to use AutoCat) using git clone . Once the repo has been cloned, you can install AutoCat as an editable package by changing into the created directory (the one with setup.py ) and installing via: pip install -e .","title":"Github (for developers)"},{"location":"#contributing","text":"Contributions through issues, feature requests, and pull requests are welcome. Guidelines are provided here .","title":"Contributing"},{"location":"#acknowledgements","text":"The code presented herein was funded by the Advanced Research Projects Agency-Energy (ARPA-E), U.S. Department of Energy, under Award Number DE-AR0001211 and in part by the National Science Foundation, under Award Number CBET-1554273. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.","title":"Acknowledgements"},{"location":"API/Learning/featurizers/","text":"Featurizer Source code in autocat/learning/featurizers.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 class Featurizer : def __init__ ( self , featurizer_class = None , # black design_space_structures : List [ Atoms ] = None , species_list : List [ str ] = None , max_size : int = None , preset : str = None , kwargs : Dict = None , ): self . _featurizer_class = SineMatrix self . featurizer_class = featurizer_class self . _preset = None self . preset = preset self . _kwargs = None self . kwargs = kwargs self . _max_size = 100 self . max_size = max_size self . _species_list = [ \"Fe\" , \"Ni\" , \"Pt\" , \"Pd\" , \"Cu\" , \"C\" , \"N\" , \"O\" , \"H\" ] self . species_list = species_list # overrides max_size and species_list if given self . _design_space_structures = None self . design_space_structures = design_space_structures def __eq__ ( self , other : object ) -> bool : if isinstance ( other , Featurizer ): for attr in [ \"featurizer_class\" , \"species_list\" , \"max_size\" , \"preset\" , \"kwargs\" , ]: if getattr ( self , attr ) != getattr ( other , attr ): return False return True return False def __repr__ ( self ) -> str : pt = PrettyTable () pt . field_names = [ \"\" , \"Featurizer\" ] class_name = ( self . featurizer_class . __module__ + \".\" + self . featurizer_class . __name__ ) pt . add_row ([ \"class\" , class_name ]) pt . add_row ([ \"kwargs\" , self . kwargs ]) pt . add_row ([ \"species list\" , self . species_list ]) pt . add_row ([ \"maximum structure size\" , self . max_size ]) pt . add_row ([ \"preset\" , self . preset ]) pt . add_row ( [ \"design space structures provided?\" , self . design_space_structures is not None , ] ) pt . max_width = 70 return str ( pt ) def copy ( self ): \"\"\" Returns a copy of the featurizer \"\"\" ds_structs_copy = ( [ struct . copy () for struct in self . design_space_structures ] if self . design_space_structures else None ) feat = self . __class__ ( featurizer_class = self . featurizer_class , design_space_structures = ds_structs_copy , species_list = self . species_list . copy (), max_size = self . max_size , preset = self . preset , kwargs = copy . deepcopy ( self . kwargs ) if self . kwargs else None , ) return feat def to_jsonified_dict ( self ) -> Dict : # collect design space structures if self . design_space_structures is not None : collected_structs = [] for struct in self . design_space_structures : collected_structs . append ( atoms_encoder ( struct )) else : collected_structs = None mod_string = self . featurizer_class . __module__ class_string = self . featurizer_class . __name__ return { \"design_space_structures\" : collected_structs , \"species_list\" : self . species_list if self . species_list else None , \"preset\" : self . preset if self . preset else None , \"kwargs\" : self . kwargs if self . kwargs else None , \"max_size\" : self . max_size if self . max_size else None , \"featurizer_class\" : { \"module_string\" : mod_string , \"class_string\" : class_string , }, } def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `Featurizer` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"featurizer.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f ) @staticmethod def from_jsonified_dict ( all_data : Dict ): if all_data . get ( \"design_space_structures\" ) is not None : # ensure structures are properly encoded using `ase.io.jsonio.encode` try : structures = [] for encoded_atoms in all_data . get ( \"design_space_structures\" ): structures . append ( atoms_decoder ( encoded_atoms )) except ( json . JSONDecodeError , TypeError ): msg = \"Please ensure design space structures encoded using `ase.io.jsonio.encode`\" raise FeaturizerError ( msg ) else : structures = None if all_data . get ( \"featurizer_class\" ) is None : # allow not providing featurizer class (will use default) featurizer_class = None elif not ( isinstance ( all_data . get ( \"featurizer_class\" ), dict ) and len ( all_data . get ( \"featurizer_class\" )) == 2 and all_data [ \"featurizer_class\" ] . get ( \"module_string\" ) is not None and all_data [ \"featurizer_class\" ] . get ( \"class_string\" ) is not None ): # check featurizer class, if provided, is done so in correct format msg = f \"featurizer_class must be provided \\ in the form {{ 'module_string': module name, 'class_string': class name }} , \\ got { all_data . get ( 'featurizer_class' ) } \" raise FeaturizerError ( msg ) else : mod = importlib . import_module ( all_data [ \"featurizer_class\" ] . get ( \"module_string\" ) ) featurizer_class = getattr ( mod , all_data [ \"featurizer_class\" ] . get ( \"class_string\" ) ) return Featurizer ( featurizer_class = featurizer_class , design_space_structures = structures , species_list = all_data . get ( \"species_list\" ), preset = all_data . get ( \"preset\" ), max_size = all_data . get ( \"max_size\" ), kwargs = all_data . get ( \"kwargs\" ), ) @staticmethod def from_json ( json_name : str ): with open ( json_name , \"r\" ) as f : all_data = json . load ( f ) return Featurizer . from_jsonified_dict ( all_data = all_data ) @property def featurizer_class ( self ): return self . _featurizer_class @featurizer_class . setter def featurizer_class ( self , featurizer_class ): if ( featurizer_class in SUPPORTED_MATMINER_CLASSES or featurizer_class in SUPPORTED_DSCRIBE_CLASSES ): self . _featurizer_class = featurizer_class self . _preset = None self . _kwargs = None elif featurizer_class is None : pass else : msg = f \"Featurization class { featurizer_class } is not currently supported.\" raise FeaturizerError ( msg ) @property def preset ( self ): return self . _preset @preset . setter def preset ( self , preset ): if self . featurizer_class in [ CrystalNNFingerprint , ElementProperty ]: self . _preset = preset elif preset is None : self . _preset = preset else : msg = f \"Presets are not supported for { self . featurizer_class . __module__ } \" raise FeaturizerError ( msg ) @property def kwargs ( self ): return self . _kwargs @kwargs . setter def kwargs ( self , kwargs ): if kwargs is not None : self . _kwargs = kwargs . copy () @property def design_space_structures ( self ): return self . _design_space_structures @design_space_structures . setter def design_space_structures ( self , design_space_structures : List [ Atoms ]): if design_space_structures is not None : self . _design_space_structures = [ struct . copy () for struct in design_space_structures ] # analyze new design space ds_structs = design_space_structures _species_list = [] for s in ds_structs : # get all unique species found_species = np . unique ( s . get_chemical_symbols ()) . tolist () new_species = [ spec for spec in found_species if spec not in _species_list ] _species_list . extend ( new_species ) # sort species list sorted_species_list = sorted ( _species_list , key = lambda el : Element ( el ) . mendeleev_no ) self . _max_size = max ([ len ( s ) for s in ds_structs ]) self . _species_list = sorted_species_list @property def max_size ( self ): return self . _max_size @max_size . setter def max_size ( self , max_size ): if max_size is not None : self . _max_size = max_size @property def species_list ( self ): return self . _species_list @species_list . setter def species_list ( self , species_list : List [ str ]): if species_list is not None : _species_list = species_list . copy () # sort species list by mendeleev number sorted_species_list = sorted ( _species_list , key = lambda el : Element ( el ) . mendeleev_no ) self . _species_list = sorted_species_list # TODO: \"get_featurization_object\" -> \"get_featurizer\" @property def featurization_object ( self ): return self . _get_featurization_object () def _get_featurization_object ( self ): # instantiate featurizer object if hasattr ( self . featurizer_class , \"from_preset\" ) and self . preset is not None : return self . featurizer_class . from_preset ( self . preset ) if self . featurizer_class in [ SineMatrix , CoulombMatrix ]: return self . featurizer_class ( n_atoms_max = self . max_size , permutation = \"none\" , ** self . kwargs or {}, ) if self . featurizer_class in [ SOAP , ACSF ]: return self . featurizer_class ( species = self . species_list , ** self . kwargs or {}) return self . featurizer_class ( ** self . kwargs or {}) def featurize_single ( self , structure : Atoms ): \"\"\" Featurize a single structure. Returns a single vector Parameters ---------- structure: ase.Atoms object of structure to be featurized Returns ------- representation: Numpy array of feature vector (not flattened) \"\"\" feat_class = self . featurizer_class featurization_object = self . featurization_object # dscribe classes if feat_class in [ SOAP , ACSF ]: adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () return featurization_object . create ( structure , positions = adsorbate_indices ,) if feat_class in [ SineMatrix , CoulombMatrix ]: return featurization_object . create ( structure ) . reshape ( - 1 ,) # matminer classes pym_struct = AseAtomsAdaptor () . get_structure ( structure ) if feat_class == ElementProperty : return np . array ( featurization_object . featurize ( pym_struct . composition )) representation = np . array ([]) if feat_class in [ CrystalNNFingerprint , OPSiteFingerprint ]: adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () for idx in adsorbate_indices : feat = featurization_object . featurize ( pym_struct , idx ) representation = np . concatenate (( representation , feat )) return representation if feat_class == ChemicalSRO : adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () formatted_list = [[ pym_struct , idx ] for idx in adsorbate_indices ] featurization_object . fit ( formatted_list ) for idx in adsorbate_indices : feat = featurization_object . featurize ( pym_struct , idx ) representation = np . concatenate (( representation , feat )) return representation return None def featurize_multiple ( self , structures : List [ Atoms ]): \"\"\" Featurize multiple structures. Returns a matrix where each row is the flattened feature vector of each system Parameters ---------- structures: List of ase.Atoms structures to be featurized Returns ------- X: Numpy array of shape (number of structures, number of features) \"\"\" first_vec = self . featurize_single ( structures [ 0 ]) . flatten () num_features = len ( first_vec ) # if adsorbate featurization, assumes only 1 adsorbate in design space # (otherwise would require padding) X = np . zeros (( len ( structures ), num_features )) X [ 0 , :] = first_vec . copy () for i in range ( 1 , len ( structures )): X [ i , :] = self . featurize_single ( structures [ i ]) . flatten () return X copy () Returns a copy of the featurizer Source code in autocat/learning/featurizers.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def copy ( self ): \"\"\" Returns a copy of the featurizer \"\"\" ds_structs_copy = ( [ struct . copy () for struct in self . design_space_structures ] if self . design_space_structures else None ) feat = self . __class__ ( featurizer_class = self . featurizer_class , design_space_structures = ds_structs_copy , species_list = self . species_list . copy (), max_size = self . max_size , preset = self . preset , kwargs = copy . deepcopy ( self . kwargs ) if self . kwargs else None , ) return feat featurize_multiple ( structures ) Featurize multiple structures. Returns a matrix where each row is the flattened feature vector of each system Parameters: Name Type Description Default structures List [ Atoms ] List of ase.Atoms structures to be featurized required Returns: Name Type Description X Numpy array of shape (number of structures, number of features) Source code in autocat/learning/featurizers.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 def featurize_multiple ( self , structures : List [ Atoms ]): \"\"\" Featurize multiple structures. Returns a matrix where each row is the flattened feature vector of each system Parameters ---------- structures: List of ase.Atoms structures to be featurized Returns ------- X: Numpy array of shape (number of structures, number of features) \"\"\" first_vec = self . featurize_single ( structures [ 0 ]) . flatten () num_features = len ( first_vec ) # if adsorbate featurization, assumes only 1 adsorbate in design space # (otherwise would require padding) X = np . zeros (( len ( structures ), num_features )) X [ 0 , :] = first_vec . copy () for i in range ( 1 , len ( structures )): X [ i , :] = self . featurize_single ( structures [ i ]) . flatten () return X featurize_single ( structure ) Featurize a single structure. Returns a single vector Parameters: Name Type Description Default structure Atoms ase.Atoms object of structure to be featurized required Returns: Name Type Description representation Numpy array of feature vector (not flattened) Source code in autocat/learning/featurizers.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def featurize_single ( self , structure : Atoms ): \"\"\" Featurize a single structure. Returns a single vector Parameters ---------- structure: ase.Atoms object of structure to be featurized Returns ------- representation: Numpy array of feature vector (not flattened) \"\"\" feat_class = self . featurizer_class featurization_object = self . featurization_object # dscribe classes if feat_class in [ SOAP , ACSF ]: adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () return featurization_object . create ( structure , positions = adsorbate_indices ,) if feat_class in [ SineMatrix , CoulombMatrix ]: return featurization_object . create ( structure ) . reshape ( - 1 ,) # matminer classes pym_struct = AseAtomsAdaptor () . get_structure ( structure ) if feat_class == ElementProperty : return np . array ( featurization_object . featurize ( pym_struct . composition )) representation = np . array ([]) if feat_class in [ CrystalNNFingerprint , OPSiteFingerprint ]: adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () for idx in adsorbate_indices : feat = featurization_object . featurize ( pym_struct , idx ) representation = np . concatenate (( representation , feat )) return representation if feat_class == ChemicalSRO : adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () formatted_list = [[ pym_struct , idx ] for idx in adsorbate_indices ] featurization_object . fit ( formatted_list ) for idx in adsorbate_indices : feat = featurization_object . featurize ( pym_struct , idx ) representation = np . concatenate (( representation , feat )) return representation return None write_json_to_disk ( write_location = '.' , json_name = None ) Writes Featurizer to disk as a json Source code in autocat/learning/featurizers.py 144 145 146 147 148 149 150 151 152 153 154 155 156 def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `Featurizer` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"featurizer.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f )","title":"autocat.learning.featurizers"},{"location":"API/Learning/featurizers/#autocat.learning.featurizers.Featurizer","text":"Source code in autocat/learning/featurizers.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 class Featurizer : def __init__ ( self , featurizer_class = None , # black design_space_structures : List [ Atoms ] = None , species_list : List [ str ] = None , max_size : int = None , preset : str = None , kwargs : Dict = None , ): self . _featurizer_class = SineMatrix self . featurizer_class = featurizer_class self . _preset = None self . preset = preset self . _kwargs = None self . kwargs = kwargs self . _max_size = 100 self . max_size = max_size self . _species_list = [ \"Fe\" , \"Ni\" , \"Pt\" , \"Pd\" , \"Cu\" , \"C\" , \"N\" , \"O\" , \"H\" ] self . species_list = species_list # overrides max_size and species_list if given self . _design_space_structures = None self . design_space_structures = design_space_structures def __eq__ ( self , other : object ) -> bool : if isinstance ( other , Featurizer ): for attr in [ \"featurizer_class\" , \"species_list\" , \"max_size\" , \"preset\" , \"kwargs\" , ]: if getattr ( self , attr ) != getattr ( other , attr ): return False return True return False def __repr__ ( self ) -> str : pt = PrettyTable () pt . field_names = [ \"\" , \"Featurizer\" ] class_name = ( self . featurizer_class . __module__ + \".\" + self . featurizer_class . __name__ ) pt . add_row ([ \"class\" , class_name ]) pt . add_row ([ \"kwargs\" , self . kwargs ]) pt . add_row ([ \"species list\" , self . species_list ]) pt . add_row ([ \"maximum structure size\" , self . max_size ]) pt . add_row ([ \"preset\" , self . preset ]) pt . add_row ( [ \"design space structures provided?\" , self . design_space_structures is not None , ] ) pt . max_width = 70 return str ( pt ) def copy ( self ): \"\"\" Returns a copy of the featurizer \"\"\" ds_structs_copy = ( [ struct . copy () for struct in self . design_space_structures ] if self . design_space_structures else None ) feat = self . __class__ ( featurizer_class = self . featurizer_class , design_space_structures = ds_structs_copy , species_list = self . species_list . copy (), max_size = self . max_size , preset = self . preset , kwargs = copy . deepcopy ( self . kwargs ) if self . kwargs else None , ) return feat def to_jsonified_dict ( self ) -> Dict : # collect design space structures if self . design_space_structures is not None : collected_structs = [] for struct in self . design_space_structures : collected_structs . append ( atoms_encoder ( struct )) else : collected_structs = None mod_string = self . featurizer_class . __module__ class_string = self . featurizer_class . __name__ return { \"design_space_structures\" : collected_structs , \"species_list\" : self . species_list if self . species_list else None , \"preset\" : self . preset if self . preset else None , \"kwargs\" : self . kwargs if self . kwargs else None , \"max_size\" : self . max_size if self . max_size else None , \"featurizer_class\" : { \"module_string\" : mod_string , \"class_string\" : class_string , }, } def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `Featurizer` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"featurizer.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f ) @staticmethod def from_jsonified_dict ( all_data : Dict ): if all_data . get ( \"design_space_structures\" ) is not None : # ensure structures are properly encoded using `ase.io.jsonio.encode` try : structures = [] for encoded_atoms in all_data . get ( \"design_space_structures\" ): structures . append ( atoms_decoder ( encoded_atoms )) except ( json . JSONDecodeError , TypeError ): msg = \"Please ensure design space structures encoded using `ase.io.jsonio.encode`\" raise FeaturizerError ( msg ) else : structures = None if all_data . get ( \"featurizer_class\" ) is None : # allow not providing featurizer class (will use default) featurizer_class = None elif not ( isinstance ( all_data . get ( \"featurizer_class\" ), dict ) and len ( all_data . get ( \"featurizer_class\" )) == 2 and all_data [ \"featurizer_class\" ] . get ( \"module_string\" ) is not None and all_data [ \"featurizer_class\" ] . get ( \"class_string\" ) is not None ): # check featurizer class, if provided, is done so in correct format msg = f \"featurizer_class must be provided \\ in the form {{ 'module_string': module name, 'class_string': class name }} , \\ got { all_data . get ( 'featurizer_class' ) } \" raise FeaturizerError ( msg ) else : mod = importlib . import_module ( all_data [ \"featurizer_class\" ] . get ( \"module_string\" ) ) featurizer_class = getattr ( mod , all_data [ \"featurizer_class\" ] . get ( \"class_string\" ) ) return Featurizer ( featurizer_class = featurizer_class , design_space_structures = structures , species_list = all_data . get ( \"species_list\" ), preset = all_data . get ( \"preset\" ), max_size = all_data . get ( \"max_size\" ), kwargs = all_data . get ( \"kwargs\" ), ) @staticmethod def from_json ( json_name : str ): with open ( json_name , \"r\" ) as f : all_data = json . load ( f ) return Featurizer . from_jsonified_dict ( all_data = all_data ) @property def featurizer_class ( self ): return self . _featurizer_class @featurizer_class . setter def featurizer_class ( self , featurizer_class ): if ( featurizer_class in SUPPORTED_MATMINER_CLASSES or featurizer_class in SUPPORTED_DSCRIBE_CLASSES ): self . _featurizer_class = featurizer_class self . _preset = None self . _kwargs = None elif featurizer_class is None : pass else : msg = f \"Featurization class { featurizer_class } is not currently supported.\" raise FeaturizerError ( msg ) @property def preset ( self ): return self . _preset @preset . setter def preset ( self , preset ): if self . featurizer_class in [ CrystalNNFingerprint , ElementProperty ]: self . _preset = preset elif preset is None : self . _preset = preset else : msg = f \"Presets are not supported for { self . featurizer_class . __module__ } \" raise FeaturizerError ( msg ) @property def kwargs ( self ): return self . _kwargs @kwargs . setter def kwargs ( self , kwargs ): if kwargs is not None : self . _kwargs = kwargs . copy () @property def design_space_structures ( self ): return self . _design_space_structures @design_space_structures . setter def design_space_structures ( self , design_space_structures : List [ Atoms ]): if design_space_structures is not None : self . _design_space_structures = [ struct . copy () for struct in design_space_structures ] # analyze new design space ds_structs = design_space_structures _species_list = [] for s in ds_structs : # get all unique species found_species = np . unique ( s . get_chemical_symbols ()) . tolist () new_species = [ spec for spec in found_species if spec not in _species_list ] _species_list . extend ( new_species ) # sort species list sorted_species_list = sorted ( _species_list , key = lambda el : Element ( el ) . mendeleev_no ) self . _max_size = max ([ len ( s ) for s in ds_structs ]) self . _species_list = sorted_species_list @property def max_size ( self ): return self . _max_size @max_size . setter def max_size ( self , max_size ): if max_size is not None : self . _max_size = max_size @property def species_list ( self ): return self . _species_list @species_list . setter def species_list ( self , species_list : List [ str ]): if species_list is not None : _species_list = species_list . copy () # sort species list by mendeleev number sorted_species_list = sorted ( _species_list , key = lambda el : Element ( el ) . mendeleev_no ) self . _species_list = sorted_species_list # TODO: \"get_featurization_object\" -> \"get_featurizer\" @property def featurization_object ( self ): return self . _get_featurization_object () def _get_featurization_object ( self ): # instantiate featurizer object if hasattr ( self . featurizer_class , \"from_preset\" ) and self . preset is not None : return self . featurizer_class . from_preset ( self . preset ) if self . featurizer_class in [ SineMatrix , CoulombMatrix ]: return self . featurizer_class ( n_atoms_max = self . max_size , permutation = \"none\" , ** self . kwargs or {}, ) if self . featurizer_class in [ SOAP , ACSF ]: return self . featurizer_class ( species = self . species_list , ** self . kwargs or {}) return self . featurizer_class ( ** self . kwargs or {}) def featurize_single ( self , structure : Atoms ): \"\"\" Featurize a single structure. Returns a single vector Parameters ---------- structure: ase.Atoms object of structure to be featurized Returns ------- representation: Numpy array of feature vector (not flattened) \"\"\" feat_class = self . featurizer_class featurization_object = self . featurization_object # dscribe classes if feat_class in [ SOAP , ACSF ]: adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () return featurization_object . create ( structure , positions = adsorbate_indices ,) if feat_class in [ SineMatrix , CoulombMatrix ]: return featurization_object . create ( structure ) . reshape ( - 1 ,) # matminer classes pym_struct = AseAtomsAdaptor () . get_structure ( structure ) if feat_class == ElementProperty : return np . array ( featurization_object . featurize ( pym_struct . composition )) representation = np . array ([]) if feat_class in [ CrystalNNFingerprint , OPSiteFingerprint ]: adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () for idx in adsorbate_indices : feat = featurization_object . featurize ( pym_struct , idx ) representation = np . concatenate (( representation , feat )) return representation if feat_class == ChemicalSRO : adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () formatted_list = [[ pym_struct , idx ] for idx in adsorbate_indices ] featurization_object . fit ( formatted_list ) for idx in adsorbate_indices : feat = featurization_object . featurize ( pym_struct , idx ) representation = np . concatenate (( representation , feat )) return representation return None def featurize_multiple ( self , structures : List [ Atoms ]): \"\"\" Featurize multiple structures. Returns a matrix where each row is the flattened feature vector of each system Parameters ---------- structures: List of ase.Atoms structures to be featurized Returns ------- X: Numpy array of shape (number of structures, number of features) \"\"\" first_vec = self . featurize_single ( structures [ 0 ]) . flatten () num_features = len ( first_vec ) # if adsorbate featurization, assumes only 1 adsorbate in design space # (otherwise would require padding) X = np . zeros (( len ( structures ), num_features )) X [ 0 , :] = first_vec . copy () for i in range ( 1 , len ( structures )): X [ i , :] = self . featurize_single ( structures [ i ]) . flatten () return X","title":"Featurizer"},{"location":"API/Learning/featurizers/#autocat.learning.featurizers.Featurizer.copy","text":"Returns a copy of the featurizer Source code in autocat/learning/featurizers.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def copy ( self ): \"\"\" Returns a copy of the featurizer \"\"\" ds_structs_copy = ( [ struct . copy () for struct in self . design_space_structures ] if self . design_space_structures else None ) feat = self . __class__ ( featurizer_class = self . featurizer_class , design_space_structures = ds_structs_copy , species_list = self . species_list . copy (), max_size = self . max_size , preset = self . preset , kwargs = copy . deepcopy ( self . kwargs ) if self . kwargs else None , ) return feat","title":"copy()"},{"location":"API/Learning/featurizers/#autocat.learning.featurizers.Featurizer.featurize_multiple","text":"Featurize multiple structures. Returns a matrix where each row is the flattened feature vector of each system Parameters: Name Type Description Default structures List [ Atoms ] List of ase.Atoms structures to be featurized required Returns: Name Type Description X Numpy array of shape (number of structures, number of features) Source code in autocat/learning/featurizers.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 def featurize_multiple ( self , structures : List [ Atoms ]): \"\"\" Featurize multiple structures. Returns a matrix where each row is the flattened feature vector of each system Parameters ---------- structures: List of ase.Atoms structures to be featurized Returns ------- X: Numpy array of shape (number of structures, number of features) \"\"\" first_vec = self . featurize_single ( structures [ 0 ]) . flatten () num_features = len ( first_vec ) # if adsorbate featurization, assumes only 1 adsorbate in design space # (otherwise would require padding) X = np . zeros (( len ( structures ), num_features )) X [ 0 , :] = first_vec . copy () for i in range ( 1 , len ( structures )): X [ i , :] = self . featurize_single ( structures [ i ]) . flatten () return X","title":"featurize_multiple()"},{"location":"API/Learning/featurizers/#autocat.learning.featurizers.Featurizer.featurize_single","text":"Featurize a single structure. Returns a single vector Parameters: Name Type Description Default structure Atoms ase.Atoms object of structure to be featurized required Returns: Name Type Description representation Numpy array of feature vector (not flattened) Source code in autocat/learning/featurizers.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def featurize_single ( self , structure : Atoms ): \"\"\" Featurize a single structure. Returns a single vector Parameters ---------- structure: ase.Atoms object of structure to be featurized Returns ------- representation: Numpy array of feature vector (not flattened) \"\"\" feat_class = self . featurizer_class featurization_object = self . featurization_object # dscribe classes if feat_class in [ SOAP , ACSF ]: adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () return featurization_object . create ( structure , positions = adsorbate_indices ,) if feat_class in [ SineMatrix , CoulombMatrix ]: return featurization_object . create ( structure ) . reshape ( - 1 ,) # matminer classes pym_struct = AseAtomsAdaptor () . get_structure ( structure ) if feat_class == ElementProperty : return np . array ( featurization_object . featurize ( pym_struct . composition )) representation = np . array ([]) if feat_class in [ CrystalNNFingerprint , OPSiteFingerprint ]: adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () for idx in adsorbate_indices : feat = featurization_object . featurize ( pym_struct , idx ) representation = np . concatenate (( representation , feat )) return representation if feat_class == ChemicalSRO : adsorbate_indices = np . where ( structure . get_tags () <= 0 )[ 0 ] . tolist () formatted_list = [[ pym_struct , idx ] for idx in adsorbate_indices ] featurization_object . fit ( formatted_list ) for idx in adsorbate_indices : feat = featurization_object . featurize ( pym_struct , idx ) representation = np . concatenate (( representation , feat )) return representation return None","title":"featurize_single()"},{"location":"API/Learning/featurizers/#autocat.learning.featurizers.Featurizer.write_json_to_disk","text":"Writes Featurizer to disk as a json Source code in autocat/learning/featurizers.py 144 145 146 147 148 149 150 151 152 153 154 155 156 def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `Featurizer` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"featurizer.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f )","title":"write_json_to_disk()"},{"location":"API/Learning/predictors/","text":"Predictor Source code in autocat/learning/predictors.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 class Predictor : def __init__ ( self , regressor = None , featurizer : Featurizer = None , ): \"\"\" Constructor. Parameters ---------- regressor: Regressor object that can be used to make predictions (e.g. from scikit-learn) with `fit` and `predict` methods. **N.B**: If you want to make any changes to the parameters of this object after instantiation, please do so as follows: `predictor.regressor = updated_regressor` featurizer: `Featurizer` to be used for featurizing the structures when training and predicting. **N.B**: If you want to make any changes to the parameters of this object after instantiation, please do so as follows: `predictor.featurizer = updated_featurizer` \"\"\" self . is_fit = False self . _regressor = RandomForestRegressor () self . regressor = regressor self . _featurizer = Featurizer () self . featurizer = featurizer def __repr__ ( self ) -> str : pt = PrettyTable () pt . field_names = [ \"\" , \"Predictor\" ] regressor_name = type ( self . regressor ) pt . add_row ([ \"regressor\" , regressor_name ]) pt . add_row ([ \"is fit?\" , self . is_fit ]) feat_str = str ( self . featurizer ) return str ( pt ) + \" \\n \" + feat_str @property def regressor ( self ): return self . _regressor @regressor . setter def regressor ( self , regressor ): if regressor is not None : self . _regressor = copy . deepcopy ( regressor ) if self . is_fit : self . is_fit = False self . X_ = None self . y_ = None @property def featurizer ( self ): return self . _featurizer @featurizer . setter def featurizer ( self , featurizer ): if featurizer is not None and isinstance ( featurizer , Featurizer ): self . _featurizer = copy . deepcopy ( featurizer ) if self . is_fit : self . is_fit = False self . X_ = None self . y_ = None def copy ( self ): \"\"\" Returns a copy \"\"\" acp = self . __class__ ( regressor = self . regressor , featurizer = self . featurizer ,) acp . is_fit = self . is_fit return acp def to_jsonified_dict ( self ) -> Dict : featurizer_dict = self . featurizer . to_jsonified_dict () regressor = self . regressor name_string = regressor . __class__ . __name__ module_string = regressor . __module__ try : kwargs = regressor . get_params () _ = json . dumps ( kwargs ) except TypeError : print ( \"Warning: kwargs not saved\" ) kwargs = None return { \"featurizer\" : featurizer_dict , \"regressor\" : { \"name_string\" : name_string , \"module_string\" : module_string , \"kwargs\" : kwargs , }, } def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `Predictor` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"predictor.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f ) @staticmethod def from_jsonified_dict ( all_data : Dict ): # get regressor if all_data . get ( \"regressor\" ) is None : # allow not providing regressor (will use default) regressor = None elif not ( isinstance ( all_data . get ( \"regressor\" ), dict ) and all_data [ \"regressor\" ] . get ( \"module_string\" ) is not None and all_data [ \"regressor\" ] . get ( \"name_string\" ) is not None ): # check regressor is provided in the correct form msg = f \"regressor must be provided \\ in the form {{ 'module_string': module name, 'name_string': class name }} , \\ got { all_data . get ( 'featurizer_class' ) } \" raise PredictorError ( msg ) else : name_string = all_data [ \"regressor\" ] . get ( \"name_string\" ) module_string = all_data [ \"regressor\" ] . get ( \"module_string\" ) kwargs = all_data [ \"regressor\" ] . get ( \"kwargs\" ) mod = importlib . import_module ( module_string ) regressor_class = getattr ( mod , name_string ) if kwargs is not None : regressor = regressor_class ( ** kwargs ) else : regressor = regressor_class () # get featurizer featurizer = Featurizer . from_jsonified_dict ( all_data . get ( \"featurizer\" , {})) return Predictor ( regressor = regressor , featurizer = featurizer ) @staticmethod def from_json ( json_name : str ): with open ( json_name , \"r\" ) as f : all_data = json . load ( f ) return Predictor . from_jsonified_dict ( all_data = all_data ) def fit ( self , training_structures : List [ Union [ Atoms , str ]], y : np . ndarray , ): \"\"\" Given a list of structures and labels will featurize and train a regression model Parameters ---------- training_structures: List of structures to be trained upon y: Numpy array of labels corresponding to training structures of shape (# of training structures, # of targets) Returns ------- trained_model: Trained `sklearn` model object \"\"\" self . X_ = self . featurizer . featurize_multiple ( training_structures ) self . y_ = y self . regressor . fit ( self . X_ , self . y_ ) self . is_fit = True def predict ( self , testing_structures : List [ Atoms ], ): \"\"\" From a trained model, will predict on given structures Parameters ---------- testing_structures: List of Atoms objects to make predictions on Returns ------- predicted_labels: List of predicted labels for each input structure unc: List of uncertainties for each prediction if available. Otherwise returns `None` \"\"\" assert self . is_fit featurized_input = self . featurizer . featurize_multiple ( testing_structures ) try : predicted_labels , unc = self . regressor . predict ( featurized_input , return_std = True ) except TypeError : predicted_labels = self . regressor . predict ( featurized_input ,) unc = None return predicted_labels , unc # TODO: \"score\" -> \"get_scores\"? def score ( self , structures : List [ Atoms ], labels : np . ndarray , metric : str = \"mae\" , return_predictions : bool = False , ** kwargs , ): \"\"\" Returns a prediction score given the actual corrections. Parameters ---------- structures: List of Atoms objects of structures to be tested on labels: Labels for the testing structures metric: How the performance metric should be calculated Options: - mae - mse return_predictions: Bool indicating whether the predictions and uncertainties should be returned in addition to the score Returns ------- score: Float of calculated test score on the given data \"\"\" assert self . is_fit pred_label , unc = self . predict ( structures ) score_func = { \"mae\" : mean_absolute_error , \"mse\" : mean_squared_error } if metric not in score_func : msg = f \"Metric: { metric } is not supported\" raise PredictorError ( msg ) score = score_func [ metric ]( labels , pred_label , ** kwargs ) if return_predictions : return score , pred_label , unc return score __init__ ( regressor = None , featurizer = None ) Constructor. Parameters: Name Type Description Default regressor Regressor object that can be used to make predictions (e.g. from scikit-learn) with fit and predict methods. N.B : If you want to make any changes to the parameters of this object after instantiation, please do so as follows: predictor.regressor = updated_regressor None featurizer: Featurizer to be used for featurizing the structures when training and predicting. N.B : If you want to make any changes to the parameters of this object after instantiation, please do so as follows: predictor.featurizer = updated_featurizer Source code in autocat/learning/predictors.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , regressor = None , featurizer : Featurizer = None , ): \"\"\" Constructor. Parameters ---------- regressor: Regressor object that can be used to make predictions (e.g. from scikit-learn) with `fit` and `predict` methods. **N.B**: If you want to make any changes to the parameters of this object after instantiation, please do so as follows: `predictor.regressor = updated_regressor` featurizer: `Featurizer` to be used for featurizing the structures when training and predicting. **N.B**: If you want to make any changes to the parameters of this object after instantiation, please do so as follows: `predictor.featurizer = updated_featurizer` \"\"\" self . is_fit = False self . _regressor = RandomForestRegressor () self . regressor = regressor self . _featurizer = Featurizer () self . featurizer = featurizer copy () Returns a copy Source code in autocat/learning/predictors.py 93 94 95 96 97 98 99 100 def copy ( self ): \"\"\" Returns a copy \"\"\" acp = self . __class__ ( regressor = self . regressor , featurizer = self . featurizer ,) acp . is_fit = self . is_fit return acp fit ( training_structures , y ) Given a list of structures and labels will featurize and train a regression model Parameters: Name Type Description Default training_structures List [ Union [ Atoms , str ]] List of structures to be trained upon required y: Numpy array of labels corresponding to training structures of shape (# of training structures, # of targets) Returns: Name Type Description trained_model Trained sklearn model object Source code in autocat/learning/predictors.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def fit ( self , training_structures : List [ Union [ Atoms , str ]], y : np . ndarray , ): \"\"\" Given a list of structures and labels will featurize and train a regression model Parameters ---------- training_structures: List of structures to be trained upon y: Numpy array of labels corresponding to training structures of shape (# of training structures, # of targets) Returns ------- trained_model: Trained `sklearn` model object \"\"\" self . X_ = self . featurizer . featurize_multiple ( training_structures ) self . y_ = y self . regressor . fit ( self . X_ , self . y_ ) self . is_fit = True predict ( testing_structures ) From a trained model, will predict on given structures Parameters: Name Type Description Default testing_structures List [ Atoms ] List of Atoms objects to make predictions on required Returns: Name Type Description predicted_labels List of predicted labels for each input structure unc: List of uncertainties for each prediction if available. Otherwise returns None Source code in autocat/learning/predictors.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def predict ( self , testing_structures : List [ Atoms ], ): \"\"\" From a trained model, will predict on given structures Parameters ---------- testing_structures: List of Atoms objects to make predictions on Returns ------- predicted_labels: List of predicted labels for each input structure unc: List of uncertainties for each prediction if available. Otherwise returns `None` \"\"\" assert self . is_fit featurized_input = self . featurizer . featurize_multiple ( testing_structures ) try : predicted_labels , unc = self . regressor . predict ( featurized_input , return_std = True ) except TypeError : predicted_labels = self . regressor . predict ( featurized_input ,) unc = None return predicted_labels , unc score ( structures , labels , metric = 'mae' , return_predictions = False , ** kwargs ) Returns a prediction score given the actual corrections. Parameters: Name Type Description Default structures List [ Atoms ] List of Atoms objects of structures to be tested on required labels: Labels for the testing structures metric: How the performance metric should be calculated Options: - mae - mse return_predictions: Bool indicating whether the predictions and uncertainties should be returned in addition to the score Returns: Name Type Description score Float of calculated test score on the given data Source code in autocat/learning/predictors.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def score ( self , structures : List [ Atoms ], labels : np . ndarray , metric : str = \"mae\" , return_predictions : bool = False , ** kwargs , ): \"\"\" Returns a prediction score given the actual corrections. Parameters ---------- structures: List of Atoms objects of structures to be tested on labels: Labels for the testing structures metric: How the performance metric should be calculated Options: - mae - mse return_predictions: Bool indicating whether the predictions and uncertainties should be returned in addition to the score Returns ------- score: Float of calculated test score on the given data \"\"\" assert self . is_fit pred_label , unc = self . predict ( structures ) score_func = { \"mae\" : mean_absolute_error , \"mse\" : mean_squared_error } if metric not in score_func : msg = f \"Metric: { metric } is not supported\" raise PredictorError ( msg ) score = score_func [ metric ]( labels , pred_label , ** kwargs ) if return_predictions : return score , pred_label , unc return score write_json_to_disk ( write_location = '.' , json_name = None ) Writes Predictor to disk as a json Source code in autocat/learning/predictors.py 122 123 124 125 126 127 128 129 130 131 132 133 134 def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `Predictor` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"predictor.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f )","title":"autocat.learning.predictors"},{"location":"API/Learning/predictors/#autocat.learning.predictors.Predictor","text":"Source code in autocat/learning/predictors.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 class Predictor : def __init__ ( self , regressor = None , featurizer : Featurizer = None , ): \"\"\" Constructor. Parameters ---------- regressor: Regressor object that can be used to make predictions (e.g. from scikit-learn) with `fit` and `predict` methods. **N.B**: If you want to make any changes to the parameters of this object after instantiation, please do so as follows: `predictor.regressor = updated_regressor` featurizer: `Featurizer` to be used for featurizing the structures when training and predicting. **N.B**: If you want to make any changes to the parameters of this object after instantiation, please do so as follows: `predictor.featurizer = updated_featurizer` \"\"\" self . is_fit = False self . _regressor = RandomForestRegressor () self . regressor = regressor self . _featurizer = Featurizer () self . featurizer = featurizer def __repr__ ( self ) -> str : pt = PrettyTable () pt . field_names = [ \"\" , \"Predictor\" ] regressor_name = type ( self . regressor ) pt . add_row ([ \"regressor\" , regressor_name ]) pt . add_row ([ \"is fit?\" , self . is_fit ]) feat_str = str ( self . featurizer ) return str ( pt ) + \" \\n \" + feat_str @property def regressor ( self ): return self . _regressor @regressor . setter def regressor ( self , regressor ): if regressor is not None : self . _regressor = copy . deepcopy ( regressor ) if self . is_fit : self . is_fit = False self . X_ = None self . y_ = None @property def featurizer ( self ): return self . _featurizer @featurizer . setter def featurizer ( self , featurizer ): if featurizer is not None and isinstance ( featurizer , Featurizer ): self . _featurizer = copy . deepcopy ( featurizer ) if self . is_fit : self . is_fit = False self . X_ = None self . y_ = None def copy ( self ): \"\"\" Returns a copy \"\"\" acp = self . __class__ ( regressor = self . regressor , featurizer = self . featurizer ,) acp . is_fit = self . is_fit return acp def to_jsonified_dict ( self ) -> Dict : featurizer_dict = self . featurizer . to_jsonified_dict () regressor = self . regressor name_string = regressor . __class__ . __name__ module_string = regressor . __module__ try : kwargs = regressor . get_params () _ = json . dumps ( kwargs ) except TypeError : print ( \"Warning: kwargs not saved\" ) kwargs = None return { \"featurizer\" : featurizer_dict , \"regressor\" : { \"name_string\" : name_string , \"module_string\" : module_string , \"kwargs\" : kwargs , }, } def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `Predictor` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"predictor.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f ) @staticmethod def from_jsonified_dict ( all_data : Dict ): # get regressor if all_data . get ( \"regressor\" ) is None : # allow not providing regressor (will use default) regressor = None elif not ( isinstance ( all_data . get ( \"regressor\" ), dict ) and all_data [ \"regressor\" ] . get ( \"module_string\" ) is not None and all_data [ \"regressor\" ] . get ( \"name_string\" ) is not None ): # check regressor is provided in the correct form msg = f \"regressor must be provided \\ in the form {{ 'module_string': module name, 'name_string': class name }} , \\ got { all_data . get ( 'featurizer_class' ) } \" raise PredictorError ( msg ) else : name_string = all_data [ \"regressor\" ] . get ( \"name_string\" ) module_string = all_data [ \"regressor\" ] . get ( \"module_string\" ) kwargs = all_data [ \"regressor\" ] . get ( \"kwargs\" ) mod = importlib . import_module ( module_string ) regressor_class = getattr ( mod , name_string ) if kwargs is not None : regressor = regressor_class ( ** kwargs ) else : regressor = regressor_class () # get featurizer featurizer = Featurizer . from_jsonified_dict ( all_data . get ( \"featurizer\" , {})) return Predictor ( regressor = regressor , featurizer = featurizer ) @staticmethod def from_json ( json_name : str ): with open ( json_name , \"r\" ) as f : all_data = json . load ( f ) return Predictor . from_jsonified_dict ( all_data = all_data ) def fit ( self , training_structures : List [ Union [ Atoms , str ]], y : np . ndarray , ): \"\"\" Given a list of structures and labels will featurize and train a regression model Parameters ---------- training_structures: List of structures to be trained upon y: Numpy array of labels corresponding to training structures of shape (# of training structures, # of targets) Returns ------- trained_model: Trained `sklearn` model object \"\"\" self . X_ = self . featurizer . featurize_multiple ( training_structures ) self . y_ = y self . regressor . fit ( self . X_ , self . y_ ) self . is_fit = True def predict ( self , testing_structures : List [ Atoms ], ): \"\"\" From a trained model, will predict on given structures Parameters ---------- testing_structures: List of Atoms objects to make predictions on Returns ------- predicted_labels: List of predicted labels for each input structure unc: List of uncertainties for each prediction if available. Otherwise returns `None` \"\"\" assert self . is_fit featurized_input = self . featurizer . featurize_multiple ( testing_structures ) try : predicted_labels , unc = self . regressor . predict ( featurized_input , return_std = True ) except TypeError : predicted_labels = self . regressor . predict ( featurized_input ,) unc = None return predicted_labels , unc # TODO: \"score\" -> \"get_scores\"? def score ( self , structures : List [ Atoms ], labels : np . ndarray , metric : str = \"mae\" , return_predictions : bool = False , ** kwargs , ): \"\"\" Returns a prediction score given the actual corrections. Parameters ---------- structures: List of Atoms objects of structures to be tested on labels: Labels for the testing structures metric: How the performance metric should be calculated Options: - mae - mse return_predictions: Bool indicating whether the predictions and uncertainties should be returned in addition to the score Returns ------- score: Float of calculated test score on the given data \"\"\" assert self . is_fit pred_label , unc = self . predict ( structures ) score_func = { \"mae\" : mean_absolute_error , \"mse\" : mean_squared_error } if metric not in score_func : msg = f \"Metric: { metric } is not supported\" raise PredictorError ( msg ) score = score_func [ metric ]( labels , pred_label , ** kwargs ) if return_predictions : return score , pred_label , unc return score","title":"Predictor"},{"location":"API/Learning/predictors/#autocat.learning.predictors.Predictor.__init__","text":"Constructor. Parameters: Name Type Description Default regressor Regressor object that can be used to make predictions (e.g. from scikit-learn) with fit and predict methods. N.B : If you want to make any changes to the parameters of this object after instantiation, please do so as follows: predictor.regressor = updated_regressor None featurizer: Featurizer to be used for featurizing the structures when training and predicting. N.B : If you want to make any changes to the parameters of this object after instantiation, please do so as follows: predictor.featurizer = updated_featurizer Source code in autocat/learning/predictors.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , regressor = None , featurizer : Featurizer = None , ): \"\"\" Constructor. Parameters ---------- regressor: Regressor object that can be used to make predictions (e.g. from scikit-learn) with `fit` and `predict` methods. **N.B**: If you want to make any changes to the parameters of this object after instantiation, please do so as follows: `predictor.regressor = updated_regressor` featurizer: `Featurizer` to be used for featurizing the structures when training and predicting. **N.B**: If you want to make any changes to the parameters of this object after instantiation, please do so as follows: `predictor.featurizer = updated_featurizer` \"\"\" self . is_fit = False self . _regressor = RandomForestRegressor () self . regressor = regressor self . _featurizer = Featurizer () self . featurizer = featurizer","title":"__init__()"},{"location":"API/Learning/predictors/#autocat.learning.predictors.Predictor.copy","text":"Returns a copy Source code in autocat/learning/predictors.py 93 94 95 96 97 98 99 100 def copy ( self ): \"\"\" Returns a copy \"\"\" acp = self . __class__ ( regressor = self . regressor , featurizer = self . featurizer ,) acp . is_fit = self . is_fit return acp","title":"copy()"},{"location":"API/Learning/predictors/#autocat.learning.predictors.Predictor.fit","text":"Given a list of structures and labels will featurize and train a regression model Parameters: Name Type Description Default training_structures List [ Union [ Atoms , str ]] List of structures to be trained upon required y: Numpy array of labels corresponding to training structures of shape (# of training structures, # of targets) Returns: Name Type Description trained_model Trained sklearn model object Source code in autocat/learning/predictors.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def fit ( self , training_structures : List [ Union [ Atoms , str ]], y : np . ndarray , ): \"\"\" Given a list of structures and labels will featurize and train a regression model Parameters ---------- training_structures: List of structures to be trained upon y: Numpy array of labels corresponding to training structures of shape (# of training structures, # of targets) Returns ------- trained_model: Trained `sklearn` model object \"\"\" self . X_ = self . featurizer . featurize_multiple ( training_structures ) self . y_ = y self . regressor . fit ( self . X_ , self . y_ ) self . is_fit = True","title":"fit()"},{"location":"API/Learning/predictors/#autocat.learning.predictors.Predictor.predict","text":"From a trained model, will predict on given structures Parameters: Name Type Description Default testing_structures List [ Atoms ] List of Atoms objects to make predictions on required Returns: Name Type Description predicted_labels List of predicted labels for each input structure unc: List of uncertainties for each prediction if available. Otherwise returns None Source code in autocat/learning/predictors.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def predict ( self , testing_structures : List [ Atoms ], ): \"\"\" From a trained model, will predict on given structures Parameters ---------- testing_structures: List of Atoms objects to make predictions on Returns ------- predicted_labels: List of predicted labels for each input structure unc: List of uncertainties for each prediction if available. Otherwise returns `None` \"\"\" assert self . is_fit featurized_input = self . featurizer . featurize_multiple ( testing_structures ) try : predicted_labels , unc = self . regressor . predict ( featurized_input , return_std = True ) except TypeError : predicted_labels = self . regressor . predict ( featurized_input ,) unc = None return predicted_labels , unc","title":"predict()"},{"location":"API/Learning/predictors/#autocat.learning.predictors.Predictor.score","text":"Returns a prediction score given the actual corrections. Parameters: Name Type Description Default structures List [ Atoms ] List of Atoms objects of structures to be tested on required labels: Labels for the testing structures metric: How the performance metric should be calculated Options: - mae - mse return_predictions: Bool indicating whether the predictions and uncertainties should be returned in addition to the score Returns: Name Type Description score Float of calculated test score on the given data Source code in autocat/learning/predictors.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def score ( self , structures : List [ Atoms ], labels : np . ndarray , metric : str = \"mae\" , return_predictions : bool = False , ** kwargs , ): \"\"\" Returns a prediction score given the actual corrections. Parameters ---------- structures: List of Atoms objects of structures to be tested on labels: Labels for the testing structures metric: How the performance metric should be calculated Options: - mae - mse return_predictions: Bool indicating whether the predictions and uncertainties should be returned in addition to the score Returns ------- score: Float of calculated test score on the given data \"\"\" assert self . is_fit pred_label , unc = self . predict ( structures ) score_func = { \"mae\" : mean_absolute_error , \"mse\" : mean_squared_error } if metric not in score_func : msg = f \"Metric: { metric } is not supported\" raise PredictorError ( msg ) score = score_func [ metric ]( labels , pred_label , ** kwargs ) if return_predictions : return score , pred_label , unc return score","title":"score()"},{"location":"API/Learning/predictors/#autocat.learning.predictors.Predictor.write_json_to_disk","text":"Writes Predictor to disk as a json Source code in autocat/learning/predictors.py 122 123 124 125 126 127 128 129 130 131 132 133 134 def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `Predictor` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"predictor.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f )","title":"write_json_to_disk()"},{"location":"API/Learning/sequential/","text":"CandidateSelector Source code in autocat/learning/sequential.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 class CandidateSelector : def __init__ ( self , acquisition_function : str = None , num_candidates_to_pick : int = None , target_window : Array = None , include_hhi : bool = None , hhi_type : str = \"production\" , include_segregation_energies : bool = None , segregation_energy_data_source : str = None , ): \"\"\" Constructor. Parameters ---------- acquisition_function: Acquisition function to be used to select the next candidates Options - MLI: maximum likelihood of improvement (default) - Random - MU: maximum uncertainty num_candidates_to_pick: Number of candidates to choose from the dataset target_window: Target window that the candidate should ideally fall within include_hhi: Whether HHI scores should be used to weight aq scores hhi_type: Type of HHI index to be used for weighting Options - production (default) - reserves include_segregation_energies: Whether segregation energies should be used to weight aq scores segregation_energy_data_source: Which tabulated data should the segregation energies be pulled from. Options: - \"raban1999\": A.V. Raban, et. al. Phys. Rev. B 59, 15990 (1999) - \"rao2020\": K. K. Rao, et. al. Topics in Catalysis volume 63, pages728-741 (2020) \"\"\" self . _acquisition_function = \"Random\" self . acquisition_function = acquisition_function self . _num_candidates_to_pick = 1 self . num_candidates_to_pick = num_candidates_to_pick self . _target_window = None self . target_window = target_window self . _include_hhi = False self . include_hhi = include_hhi self . _hhi_type = \"production\" self . hhi_type = hhi_type self . _include_segregation_energies = False self . include_segregation_energies = include_segregation_energies self . _segregation_energy_data_source = \"raban1999\" self . segregation_energy_data_source = segregation_energy_data_source @property def acquisition_function ( self ): return self . _acquisition_function @acquisition_function . setter def acquisition_function ( self , acquisition_function ): if acquisition_function is not None : if acquisition_function in [ \"MLI\" , \"MU\" , \"Random\" ]: self . _acquisition_function = acquisition_function else : msg = f \"Unrecognized acquisition function { acquisition_function } \\ Please select one of 'MLI', 'MU', or 'Random'\" raise CandidateSelectorError ( msg ) @property def num_candidates_to_pick ( self ): return self . _num_candidates_to_pick @num_candidates_to_pick . setter def num_candidates_to_pick ( self , num_candidates_to_pick ): if num_candidates_to_pick is not None : self . _num_candidates_to_pick = num_candidates_to_pick @property def target_window ( self ): return self . _target_window @target_window . setter def target_window ( self , target_window ): if target_window is not None : assert len ( target_window ) == 2 # ensure not setting infinite window if np . array_equal ( target_window , np . array ([ - np . inf , np . inf ])): msg = \"Cannot have an inifite target window\" raise CandidateSelectorError ( msg ) # sorts window bounds so min is first entry sorted_window = np . sort ( target_window ) self . _target_window = sorted_window @property def include_hhi ( self ): return self . _include_hhi @include_hhi . setter def include_hhi ( self , include_hhi ): if include_hhi is not None : self . _include_hhi = include_hhi @property def hhi_type ( self ): return self . _hhi_type @hhi_type . setter def hhi_type ( self , hhi_type ): if hhi_type is not None : if hhi_type in [ \"production\" , \"reserves\" ]: self . _hhi_type = hhi_type else : msg = f \"Unrecognized HHI type { hhi_type } . \\ Please select one of 'production' or 'reserves'\" raise CandidateSelectorError ( msg ) @property def include_segregation_energies ( self ): return self . _include_segregation_energies @include_segregation_energies . setter def include_segregation_energies ( self , include_segregation_energies ): if include_segregation_energies is not None : self . _include_segregation_energies = include_segregation_energies @property def segregation_energy_data_source ( self ): return self . _segregation_energy_data_source @segregation_energy_data_source . setter def segregation_energy_data_source ( self , segregation_energy_data_source ): if segregation_energy_data_source is not None : if segregation_energy_data_source in [ \"raban1999\" , \"rao2020\" ]: self . _segregation_energy_data_source = segregation_energy_data_source else : msg = f \"Unrecognized segregation energy data source { segregation_energy_data_source } . \\ Please select one of 'raban1999' or 'rao2020'\" raise CandidateSelectorError ( msg ) def __repr__ ( self ) -> str : pt = PrettyTable () pt . field_names = [ \"\" , \"Candidate Selector\" ] pt . add_row ([ \"acquisition function\" , self . acquisition_function ]) pt . add_row ([ \"# of candidates to pick\" , self . num_candidates_to_pick ]) pt . add_row ([ \"target window\" , self . target_window ]) pt . add_row ([ \"include hhi?\" , self . include_hhi ]) if self . include_hhi : pt . add_row ([ \"hhi type\" , self . hhi_type ]) pt . add_row ([ \"include segregation energies?\" , self . include_segregation_energies ]) pt . add_row ( [ \"segregation energies data source\" , self . segregation_energy_data_source ] ) pt . max_width = 70 return str ( pt ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , CandidateSelector ): for prop in [ \"acquisition_function\" , \"num_candidates_to_pick\" , \"include_hhi\" , \"hhi_type\" , \"include_segregation_energies\" , \"segregation_energy_data_source\" , ]: if getattr ( self , prop ) != getattr ( other , prop ): return False return np . array_equal ( self . target_window , other . target_window ) return False def copy ( self ): \"\"\" Returns a copy of the CandidateSelector \"\"\" cs = self . __class__ ( acquisition_function = self . acquisition_function , num_candidates_to_pick = self . num_candidates_to_pick , target_window = self . target_window , include_hhi = self . include_hhi , hhi_type = self . hhi_type , include_segregation_energies = self . include_segregation_energies , ) return cs def choose_candidate ( self , design_space : DesignSpace , allowed_idx : Array = None , predictions : Array = None , uncertainties : Array = None , ): \"\"\" Choose the next candidate(s) from a design space Parameters ---------- design_space: DesignSpace where candidates will be selected from allowed_idx: Allowed indices that the selector can choose from when making a recommendation Defaults to only choosing from systems with `np.nan` labels if a `DesignSpace` with unknown labels is provided. Otherwise, all structures are considered predictions: Predictions for all structures in the DesignSpace uncertainties: Uncertainties for all structures in the DesignSpace Returns ------- parent_idx: Index/indices of the selected candidates max_scores: Maximum scores (corresponding to the selected candidates) aq_scores: Calculated scores using `acquisition_function` for the entire DesignSpace \"\"\" ds_size = len ( design_space ) if allowed_idx is None : if True in np . isnan ( design_space . design_space_labels ): allowed_idx = np . where ( np . isnan ( design_space . design_space_labels ))[ 0 ] else : allowed_idx = np . ones ( ds_size , dtype = bool ) hhi_scores = np . ones ( ds_size ) if self . include_hhi : hhi_scores = calculate_hhi_scores ( design_space . design_space_structures , self . hhi_type ) segreg_energy_scores = np . ones ( ds_size ) if self . include_segregation_energies : segreg_energy_scores = calculate_segregation_energy_scores ( design_space . design_space_structures ) aq = self . acquisition_function if aq == \"Random\" : aq_scores = ( np . random . choice ( ds_size , size = ds_size , replace = False ) * hhi_scores * segreg_energy_scores ) elif aq == \"MU\" : if uncertainties is None : msg = \"For 'MU', the uncertainties must be supplied\" raise CandidateSelectorError ( msg ) aq_scores = uncertainties . copy () * hhi_scores * segreg_energy_scores elif aq == \"MLI\" : if uncertainties is None or predictions is None : msg = \"For 'MLI', both uncertainties and predictions must be supplied\" raise CandidateSelectorError ( msg ) target_window = self . target_window aq_scores = ( np . array ( [ get_overlap_score ( mean , std , x2 = target_window [ 1 ], x1 = target_window [ 0 ] ) for mean , std in zip ( predictions , uncertainties ) ] ) * hhi_scores * segreg_energy_scores ) next_idx = np . argsort ( aq_scores [ allowed_idx ])[ - self . num_candidates_to_pick :] sorted_array = aq_scores [ allowed_idx ][ next_idx ] max_scores = list ( sorted_array [ - self . num_candidates_to_pick :]) parent_idx = np . arange ( aq_scores . shape [ 0 ])[ allowed_idx ][ next_idx ] return parent_idx , max_scores , aq_scores def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" target_window = self . target_window if target_window is not None : target_window = [ float ( x ) for x in target_window ] return { \"acquisition_function\" : self . acquisition_function , \"num_candidates_to_pick\" : self . num_candidates_to_pick , \"target_window\" : target_window , \"include_hhi\" : self . include_hhi , \"hhi_type\" : self . hhi_type , \"include_segregation_energies\" : self . include_segregation_energies , \"segregation_energy_data_source\" : self . segregation_energy_data_source , } def write_json_to_disk ( self , json_name : str = None , write_location : str = \".\" , ): \"\"\" Writes CandidateSelector to disk as a json \"\"\" collected_jsons = self . to_jsonified_dict () # set default json name if needed if json_name is None : json_name = \"candidate_selector.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( collected_jsons , f ) @staticmethod def from_jsonified_dict ( all_data : Dict ): target_window = all_data . get ( \"target_window\" ) if target_window is not None : target_window = np . array ( target_window ) return CandidateSelector ( acquisition_function = all_data . get ( \"acquisition_function\" ), num_candidates_to_pick = all_data . get ( \"num_candidates_to_pick\" ), target_window = target_window , include_hhi = all_data . get ( \"include_hhi\" ), hhi_type = all_data . get ( \"hhi_type\" ), include_segregation_energies = all_data . get ( \"include_segregation_energies\" ), segregation_energy_data_source = all_data . get ( \"segregation_energy_data_source\" ), ) @staticmethod def from_json ( json_name : str ): with open ( json_name , \"r\" ) as f : all_data = json . load ( f ) return CandidateSelector . from_jsonified_dict ( all_data ) __init__ ( acquisition_function = None , num_candidates_to_pick = None , target_window = None , include_hhi = None , hhi_type = 'production' , include_segregation_energies = None , segregation_energy_data_source = None ) Constructor. Parameters: Name Type Description Default acquisition_function str Acquisition function to be used to select the next candidates Options - MLI: maximum likelihood of improvement (default) - Random - MU: maximum uncertainty None num_candidates_to_pick: Number of candidates to choose from the dataset target_window: Target window that the candidate should ideally fall within include_hhi: Whether HHI scores should be used to weight aq scores hhi_type: Type of HHI index to be used for weighting Options - production (default) - reserves include_segregation_energies: Whether segregation energies should be used to weight aq scores segregation_energy_data_source: Which tabulated data should the segregation energies be pulled from. Options: - \"raban1999\": A.V. Raban, et. al. Phys. Rev. B 59, 15990 (1999) - \"rao2020\": K. K. Rao, et. al. Topics in Catalysis volume 63, pages728-741 (2020) Source code in autocat/learning/sequential.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def __init__ ( self , acquisition_function : str = None , num_candidates_to_pick : int = None , target_window : Array = None , include_hhi : bool = None , hhi_type : str = \"production\" , include_segregation_energies : bool = None , segregation_energy_data_source : str = None , ): \"\"\" Constructor. Parameters ---------- acquisition_function: Acquisition function to be used to select the next candidates Options - MLI: maximum likelihood of improvement (default) - Random - MU: maximum uncertainty num_candidates_to_pick: Number of candidates to choose from the dataset target_window: Target window that the candidate should ideally fall within include_hhi: Whether HHI scores should be used to weight aq scores hhi_type: Type of HHI index to be used for weighting Options - production (default) - reserves include_segregation_energies: Whether segregation energies should be used to weight aq scores segregation_energy_data_source: Which tabulated data should the segregation energies be pulled from. Options: - \"raban1999\": A.V. Raban, et. al. Phys. Rev. B 59, 15990 (1999) - \"rao2020\": K. K. Rao, et. al. Topics in Catalysis volume 63, pages728-741 (2020) \"\"\" self . _acquisition_function = \"Random\" self . acquisition_function = acquisition_function self . _num_candidates_to_pick = 1 self . num_candidates_to_pick = num_candidates_to_pick self . _target_window = None self . target_window = target_window self . _include_hhi = False self . include_hhi = include_hhi self . _hhi_type = \"production\" self . hhi_type = hhi_type self . _include_segregation_energies = False self . include_segregation_energies = include_segregation_energies self . _segregation_energy_data_source = \"raban1999\" self . segregation_energy_data_source = segregation_energy_data_source choose_candidate ( design_space , allowed_idx = None , predictions = None , uncertainties = None ) Choose the next candidate(s) from a design space Parameters: Name Type Description Default design_space DesignSpace DesignSpace where candidates will be selected from required allowed_idx: Allowed indices that the selector can choose from when making a recommendation Defaults to only choosing from systems with np.nan labels if a DesignSpace with unknown labels is provided. Otherwise, all structures are considered predictions: Predictions for all structures in the DesignSpace uncertainties: Uncertainties for all structures in the DesignSpace Returns: Name Type Description parent_idx Index/indices of the selected candidates max_scores: Maximum scores (corresponding to the selected candidates) aq_scores: Calculated scores using acquisition_function for the entire DesignSpace Source code in autocat/learning/sequential.py 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 def choose_candidate ( self , design_space : DesignSpace , allowed_idx : Array = None , predictions : Array = None , uncertainties : Array = None , ): \"\"\" Choose the next candidate(s) from a design space Parameters ---------- design_space: DesignSpace where candidates will be selected from allowed_idx: Allowed indices that the selector can choose from when making a recommendation Defaults to only choosing from systems with `np.nan` labels if a `DesignSpace` with unknown labels is provided. Otherwise, all structures are considered predictions: Predictions for all structures in the DesignSpace uncertainties: Uncertainties for all structures in the DesignSpace Returns ------- parent_idx: Index/indices of the selected candidates max_scores: Maximum scores (corresponding to the selected candidates) aq_scores: Calculated scores using `acquisition_function` for the entire DesignSpace \"\"\" ds_size = len ( design_space ) if allowed_idx is None : if True in np . isnan ( design_space . design_space_labels ): allowed_idx = np . where ( np . isnan ( design_space . design_space_labels ))[ 0 ] else : allowed_idx = np . ones ( ds_size , dtype = bool ) hhi_scores = np . ones ( ds_size ) if self . include_hhi : hhi_scores = calculate_hhi_scores ( design_space . design_space_structures , self . hhi_type ) segreg_energy_scores = np . ones ( ds_size ) if self . include_segregation_energies : segreg_energy_scores = calculate_segregation_energy_scores ( design_space . design_space_structures ) aq = self . acquisition_function if aq == \"Random\" : aq_scores = ( np . random . choice ( ds_size , size = ds_size , replace = False ) * hhi_scores * segreg_energy_scores ) elif aq == \"MU\" : if uncertainties is None : msg = \"For 'MU', the uncertainties must be supplied\" raise CandidateSelectorError ( msg ) aq_scores = uncertainties . copy () * hhi_scores * segreg_energy_scores elif aq == \"MLI\" : if uncertainties is None or predictions is None : msg = \"For 'MLI', both uncertainties and predictions must be supplied\" raise CandidateSelectorError ( msg ) target_window = self . target_window aq_scores = ( np . array ( [ get_overlap_score ( mean , std , x2 = target_window [ 1 ], x1 = target_window [ 0 ] ) for mean , std in zip ( predictions , uncertainties ) ] ) * hhi_scores * segreg_energy_scores ) next_idx = np . argsort ( aq_scores [ allowed_idx ])[ - self . num_candidates_to_pick :] sorted_array = aq_scores [ allowed_idx ][ next_idx ] max_scores = list ( sorted_array [ - self . num_candidates_to_pick :]) parent_idx = np . arange ( aq_scores . shape [ 0 ])[ allowed_idx ][ next_idx ] return parent_idx , max_scores , aq_scores copy () Returns a copy of the CandidateSelector Source code in autocat/learning/sequential.py 409 410 411 412 413 414 415 416 417 418 419 420 421 def copy ( self ): \"\"\" Returns a copy of the CandidateSelector \"\"\" cs = self . __class__ ( acquisition_function = self . acquisition_function , num_candidates_to_pick = self . num_candidates_to_pick , target_window = self . target_window , include_hhi = self . include_hhi , hhi_type = self . hhi_type , include_segregation_energies = self . include_segregation_energies , ) return cs to_jsonified_dict () Returns a jsonified dict representation Source code in autocat/learning/sequential.py 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" target_window = self . target_window if target_window is not None : target_window = [ float ( x ) for x in target_window ] return { \"acquisition_function\" : self . acquisition_function , \"num_candidates_to_pick\" : self . num_candidates_to_pick , \"target_window\" : target_window , \"include_hhi\" : self . include_hhi , \"hhi_type\" : self . hhi_type , \"include_segregation_energies\" : self . include_segregation_energies , \"segregation_energy_data_source\" : self . segregation_energy_data_source , } write_json_to_disk ( json_name = None , write_location = '.' ) Writes CandidateSelector to disk as a json Source code in autocat/learning/sequential.py 538 539 540 541 542 543 544 545 546 547 548 549 550 551 def write_json_to_disk ( self , json_name : str = None , write_location : str = \".\" , ): \"\"\" Writes CandidateSelector to disk as a json \"\"\" collected_jsons = self . to_jsonified_dict () # set default json name if needed if json_name is None : json_name = \"candidate_selector.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( collected_jsons , f ) DesignSpace Source code in autocat/learning/sequential.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 class DesignSpace : def __init__ ( self , design_space_structures : List [ Atoms ], design_space_labels : Array , ): \"\"\" Constructor. Parameters ---------- design_space_structures: List of all structures within the design space design_space_labels: Labels corresponding to all structures within the design space. If label not yet known, set to np.nan \"\"\" if len ( design_space_structures ) != design_space_labels . shape [ 0 ]: msg = f \"Number of structures ( { len ( design_space_structures ) } ) \\ and labels ( { design_space_labels . shape [ 0 ] } ) must match\" raise DesignSpaceError ( msg ) self . _design_space_structures = [ struct . copy () for struct in design_space_structures ] self . _design_space_labels = design_space_labels . copy () def __repr__ ( self ) -> str : pt = PrettyTable () pt . field_names = [ \"\" , \"DesignSpace\" ] pt . add_row ([ \"total # of systems\" , len ( self )]) num_unknown = sum ( np . isnan ( self . design_space_labels )) pt . add_row ([ \"# of unlabelled systems\" , num_unknown ]) pt . add_row ([ \"unique species present\" , self . species_list ]) max_label = max ( self . design_space_labels ) pt . add_row ([ \"maximum label\" , max_label ]) min_label = min ( self . design_space_labels ) pt . add_row ([ \"minimum label\" , min_label ]) pt . max_width = 70 return str ( pt ) def __len__ ( self ): return len ( self . design_space_structures ) # TODO: non-dunder method for deleting systems def __delitem__ ( self , i ): \"\"\" Deletes systems from the design space. If mask provided, deletes wherever True \"\"\" if isinstance ( i , list ): i = np . array ( i ) elif isinstance ( i , int ): i = [ i ] mask = np . ones ( len ( self ), dtype = bool ) mask [ i ] = 0 self . _design_space_labels = self . design_space_labels [ mask ] structs = self . design_space_structures masked_structs = [ structs [ j ] for j in range ( len ( self )) if mask [ j ]] self . _design_space_structures = masked_structs def __eq__ ( self , other : object ) -> bool : if isinstance ( other , DesignSpace ): # check that they are the same length if len ( self ) == len ( other ): # check all their structures are equal self_structs = self . design_space_structures o_structs = other . design_space_structures if not self_structs == o_structs : return False # check their labels are equal self_labels = self . design_space_labels o_labels = other . design_space_labels return np . array_equal ( self_labels , o_labels , equal_nan = True ) return False def copy ( self ): \"\"\" Returns a copy of the design space \"\"\" acds = self . __class__ ( design_space_structures = self . design_space_structures , design_space_labels = self . design_space_labels , ) return acds @property def design_space_structures ( self ): return self . _design_space_structures @design_space_structures . setter def design_space_structures ( self , design_space_structures ): msg = \"Please use `update` method to update the design space.\" raise DesignSpaceError ( msg ) @property def design_space_labels ( self ): return self . _design_space_labels @design_space_labels . setter def design_space_labels ( self , design_space_labels ): msg = \"Please use `update` method to update the design space.\" raise DesignSpaceError ( msg ) @property def species_list ( self ): species_list = [] for s in self . design_space_structures : # get all unique species found_species = np . unique ( s . get_chemical_symbols ()) . tolist () new_species = [ spec for spec in found_species if spec not in species_list ] species_list . extend ( new_species ) return species_list def update ( self , structures : List [ Atoms ], labels : Array ): \"\"\" Updates design space given structures and corresponding labels. If structure already in design space, the label is updated. Parameters ---------- structures: List of Atoms objects structures to be added labels: Corresponding labels to `structures` \"\"\" if ( structures is not None ) and ( labels is not None ): assert len ( structures ) == len ( labels ) assert all ( isinstance ( struct , Atoms ) for struct in structures ) for i , struct in enumerate ( structures ): # if structure already in design space, update label if struct in self . design_space_structures : idx = self . design_space_structures . index ( struct ) self . _design_space_labels [ idx ] = labels [ i ] # otherwise extend design space else : self . _design_space_structures . append ( struct ) self . _design_space_labels = np . append ( self . design_space_labels , labels [ i ] ) def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" collected_structs = [] for struct in self . design_space_structures : collected_structs . append ( atoms_encoder ( struct )) jsonified_labels = [ float ( x ) for x in self . design_space_labels ] return { \"structures\" : collected_structs , \"labels\" : jsonified_labels } def write_json_to_disk ( self , json_name : str = None , write_location : str = \".\" , ): \"\"\" Writes DesignSpace to disk as a json \"\"\" collected_jsons = self . to_jsonified_dict () # set default json name if needed if json_name is None : json_name = \"acds.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( collected_jsons , f ) @staticmethod def from_jsonified_dict ( all_data : Dict ): if all_data . get ( \"structures\" ) is None or all_data . get ( \"labels\" ) is None : msg = \"Both structures and labels must be provided\" raise DesignSpaceError ( msg ) try : structures = [] for encoded_atoms in all_data [ \"structures\" ]: structures . append ( atoms_decoder ( encoded_atoms )) except ( json . JSONDecodeError , TypeError ): msg = \"Please ensure design space structures encoded using `ase.io.jsonio.encode`\" raise DesignSpaceError ( msg ) labels = np . array ( all_data [ \"labels\" ]) return DesignSpace ( design_space_structures = structures , design_space_labels = labels , ) @staticmethod def from_json ( json_name : str ): with open ( json_name , \"r\" ) as f : all_data = json . load ( f ) return DesignSpace . from_jsonified_dict ( all_data ) __delitem__ ( i ) Deletes systems from the design space. If mask provided, deletes wherever True Source code in autocat/learning/sequential.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def __delitem__ ( self , i ): \"\"\" Deletes systems from the design space. If mask provided, deletes wherever True \"\"\" if isinstance ( i , list ): i = np . array ( i ) elif isinstance ( i , int ): i = [ i ] mask = np . ones ( len ( self ), dtype = bool ) mask [ i ] = 0 self . _design_space_labels = self . design_space_labels [ mask ] structs = self . design_space_structures masked_structs = [ structs [ j ] for j in range ( len ( self )) if mask [ j ]] self . _design_space_structures = masked_structs __init__ ( design_space_structures , design_space_labels ) Constructor. Parameters: Name Type Description Default design_space_structures List [ Atoms ] List of all structures within the design space required design_space_labels: Labels corresponding to all structures within the design space. If label not yet known, set to np.nan Source code in autocat/learning/sequential.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , design_space_structures : List [ Atoms ], design_space_labels : Array , ): \"\"\" Constructor. Parameters ---------- design_space_structures: List of all structures within the design space design_space_labels: Labels corresponding to all structures within the design space. If label not yet known, set to np.nan \"\"\" if len ( design_space_structures ) != design_space_labels . shape [ 0 ]: msg = f \"Number of structures ( { len ( design_space_structures ) } ) \\ and labels ( { design_space_labels . shape [ 0 ] } ) must match\" raise DesignSpaceError ( msg ) self . _design_space_structures = [ struct . copy () for struct in design_space_structures ] self . _design_space_labels = design_space_labels . copy () copy () Returns a copy of the design space Source code in autocat/learning/sequential.py 104 105 106 107 108 109 110 111 112 def copy ( self ): \"\"\" Returns a copy of the design space \"\"\" acds = self . __class__ ( design_space_structures = self . design_space_structures , design_space_labels = self . design_space_labels , ) return acds to_jsonified_dict () Returns a jsonified dict representation Source code in autocat/learning/sequential.py 171 172 173 174 175 176 177 178 179 def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" collected_structs = [] for struct in self . design_space_structures : collected_structs . append ( atoms_encoder ( struct )) jsonified_labels = [ float ( x ) for x in self . design_space_labels ] return { \"structures\" : collected_structs , \"labels\" : jsonified_labels } update ( structures , labels ) Updates design space given structures and corresponding labels. If structure already in design space, the label is updated. Parameters: Name Type Description Default structures List [ Atoms ] List of Atoms objects structures to be added required labels: Corresponding labels to structures Source code in autocat/learning/sequential.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def update ( self , structures : List [ Atoms ], labels : Array ): \"\"\" Updates design space given structures and corresponding labels. If structure already in design space, the label is updated. Parameters ---------- structures: List of Atoms objects structures to be added labels: Corresponding labels to `structures` \"\"\" if ( structures is not None ) and ( labels is not None ): assert len ( structures ) == len ( labels ) assert all ( isinstance ( struct , Atoms ) for struct in structures ) for i , struct in enumerate ( structures ): # if structure already in design space, update label if struct in self . design_space_structures : idx = self . design_space_structures . index ( struct ) self . _design_space_labels [ idx ] = labels [ i ] # otherwise extend design space else : self . _design_space_structures . append ( struct ) self . _design_space_labels = np . append ( self . design_space_labels , labels [ i ] ) write_json_to_disk ( json_name = None , write_location = '.' ) Writes DesignSpace to disk as a json Source code in autocat/learning/sequential.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def write_json_to_disk ( self , json_name : str = None , write_location : str = \".\" , ): \"\"\" Writes DesignSpace to disk as a json \"\"\" collected_jsons = self . to_jsonified_dict () # set default json name if needed if json_name is None : json_name = \"acds.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( collected_jsons , f ) SequentialLearner Source code in autocat/learning/sequential.py 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 class SequentialLearner : def __init__ ( self , design_space : DesignSpace , predictor : Predictor = None , candidate_selector : CandidateSelector = None , sl_kwargs : Dict [ str , int ] = None , ): \"\"\" Constructor. Parameters ---------- design_space: DesignSpace that is being explored predictor: Predictor used for training and predicting on the desired property candidate_selector: CandidateSelector used for calculating scores and selecting candidates for each iteration \"\"\" # TODO: move predefined attributes (train_idx, candidate_idxs) to a # different container (not kwargs) self . _design_space = None self . design_space = design_space . copy () self . _predictor = Predictor () self . predictor = predictor self . _candidate_selector = CandidateSelector () self . candidate_selector = candidate_selector # other miscellaneous kw arguments self . sl_kwargs = sl_kwargs if sl_kwargs else {} # variables that need to be propagated through the SL process if \"iteration_count\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"iteration_count\" : 0 }) if \"train_idx\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"train_idx\" : None }) if \"train_idx_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"train_idx_history\" : None }) if \"predictions\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"predictions\" : None }) if \"predictions_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"predictions_history\" : None }) if \"uncertainties\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"uncertainties\" : None }) if \"uncertainties_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"uncertainties_history\" : None }) if \"candidate_indices\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"candidate_indices\" : None }) if \"candidate_index_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"candidate_index_history\" : None }) if \"acquisition_scores\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"acquisition_scores\" : None }) def __repr__ ( self ) -> str : pt = PrettyTable () pt . field_names = [ \"\" , \"Sequential Learner\" ] pt . add_row ([ \"iteration count\" , self . iteration_count ]) if self . candidate_structures is not None : cand_formulas = [ s . get_chemical_formula () for s in self . candidate_structures ] else : cand_formulas = None pt . add_row ([ \"next candidate system structures\" , cand_formulas ]) pt . add_row ([ \"next candidate system indices\" , self . candidate_indices ]) return ( str ( pt ) + \" \\n \" + str ( self . candidate_selector ) + \" \\n \" + str ( self . design_space ) + \" \\n \" + str ( self . predictor ) ) @property def design_space ( self ): return self . _design_space @design_space . setter def design_space ( self , design_space ): if design_space is not None and isinstance ( design_space , DesignSpace ): self . _design_space = design_space @property def predictor ( self ): return self . _predictor @predictor . setter def predictor ( self , predictor ): if predictor is not None and isinstance ( predictor , Predictor ): feat = predictor . featurizer . copy () feat . design_space_structures = self . design_space . design_space_structures self . _predictor = Predictor ( regressor = predictor . regressor , featurizer = feat ) @property def candidate_selector ( self ): return self . _candidate_selector @candidate_selector . setter def candidate_selector ( self , candidate_selector ): if candidate_selector is not None and isinstance ( candidate_selector , CandidateSelector ): self . _candidate_selector = candidate_selector . copy () @property def iteration_count ( self ): return self . sl_kwargs . get ( \"iteration_count\" , 0 ) @property def train_idx ( self ): return self . sl_kwargs . get ( \"train_idx\" ) @property def train_idx_history ( self ): return self . sl_kwargs . get ( \"train_idx_history\" , None ) @property def predictions ( self ): return self . sl_kwargs . get ( \"predictions\" ) @property def uncertainties ( self ): return self . sl_kwargs . get ( \"uncertainties\" ) @property def candidate_indices ( self ): return self . sl_kwargs . get ( \"candidate_indices\" ) @property def acquisition_scores ( self ): return self . sl_kwargs . get ( \"acquisition_scores\" , None ) @property def candidate_structures ( self ): idxs = self . candidate_indices if idxs is not None : return [ self . design_space . design_space_structures [ i ] for i in idxs ] @property def candidate_index_history ( self ): return self . sl_kwargs . get ( \"candidate_index_history\" , None ) @property def predictions_history ( self ): return self . sl_kwargs . get ( \"predictions_history\" , None ) @property def uncertainties_history ( self ): return self . sl_kwargs . get ( \"uncertainties_history\" , None ) def copy ( self ): \"\"\" Returns a copy \"\"\" acsl = self . __class__ ( design_space = self . design_space , predictor = self . predictor , candidate_selector = self . candidate_selector , ) acsl . sl_kwargs = copy . deepcopy ( self . sl_kwargs ) return acsl def iterate ( self ): \"\"\"Runs the next iteration of sequential learning. This process consists of: - retraining the predictor - predicting candidate properties and calculating candidate scores (if fully explored returns None) - selecting the next batch of candidates for objective evaluation (if fully explored returns None) \"\"\" dstructs = self . design_space . design_space_structures dlabels = self . design_space . design_space_labels mask_nans = ~ np . isnan ( dlabels ) masked_structs = [ struct for i , struct in enumerate ( dstructs ) if mask_nans [ i ]] masked_labels = dlabels [ np . where ( mask_nans )] self . predictor . fit ( masked_structs , masked_labels ) train_idx = np . zeros ( len ( dlabels ), dtype = bool ) train_idx [ np . where ( mask_nans )] = 1 self . sl_kwargs . update ({ \"train_idx\" : train_idx }) train_idx_hist = self . sl_kwargs . get ( \"train_idx_history\" ) if train_idx_hist is None : train_idx_hist = [] train_idx_hist . append ( train_idx ) self . sl_kwargs . update ({ \"train_idx_history\" : train_idx_hist }) preds , unc = self . predictor . predict ( dstructs ) # update predictions and store in history self . sl_kwargs . update ({ \"predictions\" : preds }) pred_hist = self . sl_kwargs . get ( \"predictions_history\" ) if pred_hist is None : pred_hist = [] pred_hist . append ( preds ) self . sl_kwargs . update ({ \"predictions_history\" : pred_hist }) # update uncertainties and store in history self . sl_kwargs . update ({ \"uncertainties\" : unc }) unc_hist = self . sl_kwargs . get ( \"uncertainties_history\" ) if unc_hist is None : unc_hist = [] unc_hist . append ( unc ) self . sl_kwargs . update ({ \"uncertainties_history\" : unc_hist }) # make sure haven't fully searched design space if any ([ np . isnan ( label ) for label in dlabels ]): candidate_idx , _ , aq_scores = self . candidate_selector . choose_candidate ( design_space = self . design_space , allowed_idx =~ train_idx , predictions = preds , uncertainties = unc , ) # if fully searched, no more candidate structures else : candidate_idx = None aq_scores = None self . sl_kwargs . update ({ \"candidate_indices\" : candidate_idx }) self . sl_kwargs . update ({ \"acquisition_scores\" : aq_scores }) # update the candidate index history if new candidate if candidate_idx is not None : cand_idx_hist = self . sl_kwargs . get ( \"candidate_index_history\" ) if cand_idx_hist is None : cand_idx_hist = [] cand_idx_hist . append ( candidate_idx ) self . sl_kwargs . update ({ \"candidate_index_history\" : cand_idx_hist }) # update the SL iteration count itc = self . sl_kwargs . get ( \"iteration_count\" , 0 ) self . sl_kwargs . update ({ \"iteration_count\" : itc + 1 }) def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" # get jsonified design space jsonified_ds = self . design_space . to_jsonified_dict () # get jsonified predictor jsonified_pred = self . predictor . to_jsonified_dict () # get jsonified candidate selector jsonified_cs = self . candidate_selector . to_jsonified_dict () # jsonify the sl kwargs jsonified_sl_kwargs = {} for k in self . sl_kwargs : if k != \"iteration_count\" and self . sl_kwargs [ k ] is not None : jsonified_sl_kwargs [ k ] = [] for arr in self . sl_kwargs [ k ]: if arr is not None : jsonified_sl_kwargs [ k ] . append ( arr . tolist ()) else : jsonified_sl_kwargs [ k ] . append ( None ) elif k == \"iteration_count\" : jsonified_sl_kwargs [ \"iteration_count\" ] = self . sl_kwargs [ \"iteration_count\" ] elif self . sl_kwargs [ k ] is None : jsonified_sl_kwargs [ k ] = None return { \"design_space\" : jsonified_ds , \"predictor\" : jsonified_pred , \"candidate_selector\" : jsonified_cs , \"sl_kwargs\" : jsonified_sl_kwargs , } def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `SequentialLearner` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"sequential_learner.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f ) @staticmethod def from_jsonified_dict ( all_data : Dict ): if all_data . get ( \"design_space\" ) is None : msg = \"DesignSpace must be provided\" raise SequentialLearnerError ( msg ) design_space = DesignSpace . from_jsonified_dict ( all_data [ \"design_space\" ]) predictor = Predictor . from_jsonified_dict ( all_data . get ( \"predictor\" , {})) candidate_selector = CandidateSelector . from_jsonified_dict ( all_data . get ( \"candidate_selector\" , {}) ) raw_sl_kwargs = all_data . get ( \"sl_kwargs\" , {}) sl_kwargs = {} for k in raw_sl_kwargs : if raw_sl_kwargs [ k ] is not None : if k in [ \"predictions\" , \"uncertainties\" , \"acquisition_scores\" , \"candidate_indices\" , ]: sl_kwargs [ k ] = np . array ( raw_sl_kwargs [ k ]) elif k in [ \"predictions_history\" , \"uncertainties_history\" , \"candidate_index_history\" , ]: sl_kwargs [ k ] = [ np . array ( i ) for i in raw_sl_kwargs [ k ]] elif k == \"iteration_count\" : sl_kwargs [ k ] = raw_sl_kwargs [ k ] elif k == \"train_idx\" : sl_kwargs [ k ] = np . array ( raw_sl_kwargs [ k ], dtype = bool ) elif k == \"train_idx_history\" : sl_kwargs [ k ] = [ np . array ( i , dtype = bool ) for i in raw_sl_kwargs [ k ]] else : sl_kwargs [ k ] = None return SequentialLearner ( design_space = design_space , predictor = predictor , candidate_selector = candidate_selector , sl_kwargs = sl_kwargs , ) @staticmethod def from_json ( json_name : str ): with open ( json_name , \"r\" ) as f : all_data = json . load ( f ) return SequentialLearner . from_jsonified_dict ( all_data ) __init__ ( design_space , predictor = None , candidate_selector = None , sl_kwargs = None ) Constructor. Parameters: Name Type Description Default design_space DesignSpace DesignSpace that is being explored required predictor: Predictor used for training and predicting on the desired property candidate_selector: CandidateSelector used for calculating scores and selecting candidates for each iteration Source code in autocat/learning/sequential.py 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def __init__ ( self , design_space : DesignSpace , predictor : Predictor = None , candidate_selector : CandidateSelector = None , sl_kwargs : Dict [ str , int ] = None , ): \"\"\" Constructor. Parameters ---------- design_space: DesignSpace that is being explored predictor: Predictor used for training and predicting on the desired property candidate_selector: CandidateSelector used for calculating scores and selecting candidates for each iteration \"\"\" # TODO: move predefined attributes (train_idx, candidate_idxs) to a # different container (not kwargs) self . _design_space = None self . design_space = design_space . copy () self . _predictor = Predictor () self . predictor = predictor self . _candidate_selector = CandidateSelector () self . candidate_selector = candidate_selector # other miscellaneous kw arguments self . sl_kwargs = sl_kwargs if sl_kwargs else {} # variables that need to be propagated through the SL process if \"iteration_count\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"iteration_count\" : 0 }) if \"train_idx\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"train_idx\" : None }) if \"train_idx_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"train_idx_history\" : None }) if \"predictions\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"predictions\" : None }) if \"predictions_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"predictions_history\" : None }) if \"uncertainties\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"uncertainties\" : None }) if \"uncertainties_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"uncertainties_history\" : None }) if \"candidate_indices\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"candidate_indices\" : None }) if \"candidate_index_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"candidate_index_history\" : None }) if \"acquisition_scores\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"acquisition_scores\" : None }) copy () Returns a copy Source code in autocat/learning/sequential.py 742 743 744 745 746 747 748 749 750 751 752 def copy ( self ): \"\"\" Returns a copy \"\"\" acsl = self . __class__ ( design_space = self . design_space , predictor = self . predictor , candidate_selector = self . candidate_selector , ) acsl . sl_kwargs = copy . deepcopy ( self . sl_kwargs ) return acsl iterate () Runs the next iteration of sequential learning. This process consists of: - retraining the predictor - predicting candidate properties and calculating candidate scores (if fully explored returns None) - selecting the next batch of candidates for objective evaluation (if fully explored returns None) Source code in autocat/learning/sequential.py 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 def iterate ( self ): \"\"\"Runs the next iteration of sequential learning. This process consists of: - retraining the predictor - predicting candidate properties and calculating candidate scores (if fully explored returns None) - selecting the next batch of candidates for objective evaluation (if fully explored returns None) \"\"\" dstructs = self . design_space . design_space_structures dlabels = self . design_space . design_space_labels mask_nans = ~ np . isnan ( dlabels ) masked_structs = [ struct for i , struct in enumerate ( dstructs ) if mask_nans [ i ]] masked_labels = dlabels [ np . where ( mask_nans )] self . predictor . fit ( masked_structs , masked_labels ) train_idx = np . zeros ( len ( dlabels ), dtype = bool ) train_idx [ np . where ( mask_nans )] = 1 self . sl_kwargs . update ({ \"train_idx\" : train_idx }) train_idx_hist = self . sl_kwargs . get ( \"train_idx_history\" ) if train_idx_hist is None : train_idx_hist = [] train_idx_hist . append ( train_idx ) self . sl_kwargs . update ({ \"train_idx_history\" : train_idx_hist }) preds , unc = self . predictor . predict ( dstructs ) # update predictions and store in history self . sl_kwargs . update ({ \"predictions\" : preds }) pred_hist = self . sl_kwargs . get ( \"predictions_history\" ) if pred_hist is None : pred_hist = [] pred_hist . append ( preds ) self . sl_kwargs . update ({ \"predictions_history\" : pred_hist }) # update uncertainties and store in history self . sl_kwargs . update ({ \"uncertainties\" : unc }) unc_hist = self . sl_kwargs . get ( \"uncertainties_history\" ) if unc_hist is None : unc_hist = [] unc_hist . append ( unc ) self . sl_kwargs . update ({ \"uncertainties_history\" : unc_hist }) # make sure haven't fully searched design space if any ([ np . isnan ( label ) for label in dlabels ]): candidate_idx , _ , aq_scores = self . candidate_selector . choose_candidate ( design_space = self . design_space , allowed_idx =~ train_idx , predictions = preds , uncertainties = unc , ) # if fully searched, no more candidate structures else : candidate_idx = None aq_scores = None self . sl_kwargs . update ({ \"candidate_indices\" : candidate_idx }) self . sl_kwargs . update ({ \"acquisition_scores\" : aq_scores }) # update the candidate index history if new candidate if candidate_idx is not None : cand_idx_hist = self . sl_kwargs . get ( \"candidate_index_history\" ) if cand_idx_hist is None : cand_idx_hist = [] cand_idx_hist . append ( candidate_idx ) self . sl_kwargs . update ({ \"candidate_index_history\" : cand_idx_hist }) # update the SL iteration count itc = self . sl_kwargs . get ( \"iteration_count\" , 0 ) self . sl_kwargs . update ({ \"iteration_count\" : itc + 1 }) to_jsonified_dict () Returns a jsonified dict representation Source code in autocat/learning/sequential.py 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" # get jsonified design space jsonified_ds = self . design_space . to_jsonified_dict () # get jsonified predictor jsonified_pred = self . predictor . to_jsonified_dict () # get jsonified candidate selector jsonified_cs = self . candidate_selector . to_jsonified_dict () # jsonify the sl kwargs jsonified_sl_kwargs = {} for k in self . sl_kwargs : if k != \"iteration_count\" and self . sl_kwargs [ k ] is not None : jsonified_sl_kwargs [ k ] = [] for arr in self . sl_kwargs [ k ]: if arr is not None : jsonified_sl_kwargs [ k ] . append ( arr . tolist ()) else : jsonified_sl_kwargs [ k ] . append ( None ) elif k == \"iteration_count\" : jsonified_sl_kwargs [ \"iteration_count\" ] = self . sl_kwargs [ \"iteration_count\" ] elif self . sl_kwargs [ k ] is None : jsonified_sl_kwargs [ k ] = None return { \"design_space\" : jsonified_ds , \"predictor\" : jsonified_pred , \"candidate_selector\" : jsonified_cs , \"sl_kwargs\" : jsonified_sl_kwargs , } write_json_to_disk ( write_location = '.' , json_name = None ) Writes SequentialLearner to disk as a json Source code in autocat/learning/sequential.py 861 862 863 864 865 866 867 868 869 870 871 872 873 def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `SequentialLearner` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"sequential_learner.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f ) calculate_hhi_scores ( structures , hhi_type = 'production' , exclude_species = None ) Calculates HHI scores for structures weighted by their composition. The scores are normalized and inverted such that these should be maximized in the interest of finding a low cost system Parameters: Name Type Description Default structures List [ Atoms ] List of Atoms objects for which to calculate the scores required hhi_type: Type of HHI index to be used for the score Options - production (default) - reserves exclude_species: Species to be excluded when calculating the scores. An example use-case would be comparing transition-metal oxides where we can ignore the presence of O in each. Returns: Name Type Description hhi_scores Scores corresponding to each of the provided structures Source code in autocat/learning/sequential.py 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 def calculate_hhi_scores ( structures : List [ Atoms ], hhi_type : str = \"production\" , exclude_species : List [ str ] = None , ): \"\"\" Calculates HHI scores for structures weighted by their composition. The scores are normalized and inverted such that these should be maximized in the interest of finding a low cost system Parameters ---------- structures: List of Atoms objects for which to calculate the scores hhi_type: Type of HHI index to be used for the score Options - production (default) - reserves exclude_species: Species to be excluded when calculating the scores. An example use-case would be comparing transition-metal oxides where we can ignore the presence of O in each. Returns ------- hhi_scores: Scores corresponding to each of the provided structures \"\"\" if structures is None : msg = \"To include HHI, the structures must be provided\" raise SequentialLearnerError ( msg ) raw_hhi_data = HHI max_hhi = np . max ([ raw_hhi_data [ hhi_type ][ r ] for r in raw_hhi_data [ hhi_type ]]) min_hhi = np . min ([ raw_hhi_data [ hhi_type ][ r ] for r in raw_hhi_data [ hhi_type ]]) # normalize and invert (so that this score is to be maximized) norm_hhi_data = { el : 1.0 - ( raw_hhi_data [ hhi_type ][ el ] - min_hhi ) / ( max_hhi - min_hhi ) for el in raw_hhi_data [ hhi_type ] } hhi_scores = np . zeros ( len ( structures )) for idx , struct in enumerate ( structures ): hhi = 0 el_counts = struct . symbols . formula . count () if exclude_species is not None : for species in exclude_species : el_counts [ species ] = 0 tot_size = sum ( el_counts . values ()) # weight calculated hhi score by composition for el in el_counts : hhi += norm_hhi_data [ el ] * el_counts [ el ] / tot_size hhi_scores [ idx ] = hhi return hhi_scores calculate_segregation_energy_scores ( structures , data_source = 'raban1999' ) Calculates HHI scores for structures weighted by their composition. The scores are normalized and inverted such that these should be maximized in the interest of finding a low cost system Parameters: Name Type Description Default structures List [ Atoms ] List of Atoms objects for which to calculate the scores required data_source: Which tabulated data should the segregation energies be pulled from. Options: - \"raban1999\": A.V. Raban, et. al. Phys. Rev. B 59, 15990 - \"rao2020\": K. K. Rao, et. al. Topics in Catalysis volume 63, pages728-741 (2020) Returns: Name Type Description hhi_scores Scores corresponding to each of the provided structures Source code in autocat/learning/sequential.py 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 def calculate_segregation_energy_scores ( structures : List [ Atoms ], data_source : str = \"raban1999\" ): \"\"\" Calculates HHI scores for structures weighted by their composition. The scores are normalized and inverted such that these should be maximized in the interest of finding a low cost system Parameters ---------- structures: List of Atoms objects for which to calculate the scores data_source: Which tabulated data should the segregation energies be pulled from. Options: - \"raban1999\": A.V. Raban, et. al. Phys. Rev. B 59, 15990 - \"rao2020\": K. K. Rao, et. al. Topics in Catalysis volume 63, pages728-741 (2020) Returns ------- hhi_scores: Scores corresponding to each of the provided structures \"\"\" if structures is None : msg = \"To include segregation energies, the structures must be provided\" raise SequentialLearnerError ( msg ) if data_source == \"raban1999\" : # won't consider surface energies (ie. dop == host) for normalization max_seg_ener = SEGREGATION_ENERGIES [ \"raban1999\" ][ \"Pd\" ][ \"W\" ] min_seg_ener = SEGREGATION_ENERGIES [ \"raban1999\" ][ \"Fe_100\" ][ \"Ag\" ] # normalize and invert (so that this score is to be maximized) norm_seg_ener_data = {} for hsp in SEGREGATION_ENERGIES [ \"raban1999\" ]: norm_seg_ener_data [ hsp ] = {} for dsp in SEGREGATION_ENERGIES [ \"raban1999\" ][ hsp ]: norm_seg_ener_data [ hsp ][ dsp ] = 1.0 - ( SEGREGATION_ENERGIES [ \"raban1999\" ][ hsp ][ dsp ] - min_seg_ener ) / ( max_seg_ener - min_seg_ener ) elif data_source == \"rao2020\" : norm_seg_ener_data = SEGREGATION_ENERGIES [ \"rao2020\" ] else : msg = f \"Unknown data source { data_source } \" raise SequentialLearnerError ( msg ) seg_ener_scores = np . zeros ( len ( structures )) for idx , struct in enumerate ( structures ): el_counts = struct . symbols . formula . count () assert len ( el_counts ) == 2 for el in el_counts : if el_counts [ el ] == 1 : dsp = el else : hsp = el seg_ener_scores [ idx ] = norm_seg_ener_data [ hsp ][ dsp ] return seg_ener_scores get_overlap_score ( mean , std , x2 = None , x1 = None ) Calculate overlap score given targets x2 (max) and x1 (min) Source code in autocat/learning/sequential.py 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 def get_overlap_score ( mean : float , std : float , x2 : float = None , x1 : float = None ): \"\"\"Calculate overlap score given targets x2 (max) and x1 (min)\"\"\" if x1 is None and x2 is None : msg = \"Please specify at least either a minimum or maximum target for MLI\" raise SequentialLearnerError ( msg ) if x1 is None : x1 = - np . inf if x2 is None : x2 = np . inf norm_dist = stats . norm ( loc = mean , scale = std ) return norm_dist . cdf ( x2 ) - norm_dist . cdf ( x1 ) multiple_simulated_sequential_learning_runs ( full_design_space , number_of_runs = 5 , number_parallel_jobs = None , predictor = None , candidate_selector = None , init_training_size = 10 , number_of_sl_loops = None , write_to_disk = False , write_location = '.' , json_name_prefix = None ) Conducts multiple simulated sequential learning runs Parameters: Name Type Description Default full_design_space DesignSpace Fully labelled DesignSpace to simulate being searched over required predictor: Predictor to be used for predicting properties while iterating. candidate_selector: CandidateSelector that specifies settings for candidate selection. This is where acquisition function, targets, etc. are specified. init_training_size: Size of the initial training set to be selected from the full space. Default: 10 number_of_sl_loops: Integer specifying the number of sequential learning loops to be conducted. This value cannot be greater than (DESIGN_SPACE_SIZE - init_training_size)/batch_size_to_add Default: maximum number of sl loops calculated above number_of_runs: Integer of number of runs to be done Default: 5 number_parallel_jobs: Integer giving the number of cores to be paralellized across using joblib Default: None (ie. will run in serial) write_to_disk: Boolean specifying whether runs history should be written to disk as jsons. Default: False write_location: String with the location where runs history jsons should be written to disk. Default: current directory json_name_prefix: Prefix used when writing out each simulated run as a json The naming convention is {json_name_prefix}_{run #}.json Default: acsl_run Returns: Name Type Description runs_history List [ SequentialLearner ] List of SequentialLearner objects for each simulated run Source code in autocat/learning/sequential.py 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 def multiple_simulated_sequential_learning_runs ( full_design_space : DesignSpace , number_of_runs : int = 5 , number_parallel_jobs : int = None , predictor : Predictor = None , candidate_selector : CandidateSelector = None , init_training_size : int = 10 , number_of_sl_loops : int = None , write_to_disk : bool = False , write_location : str = \".\" , json_name_prefix : str = None , ) -> List [ SequentialLearner ]: \"\"\" Conducts multiple simulated sequential learning runs Parameters ---------- full_design_space: Fully labelled DesignSpace to simulate being searched over predictor: Predictor to be used for predicting properties while iterating. candidate_selector: CandidateSelector that specifies settings for candidate selection. This is where acquisition function, targets, etc. are specified. init_training_size: Size of the initial training set to be selected from the full space. Default: 10 number_of_sl_loops: Integer specifying the number of sequential learning loops to be conducted. This value cannot be greater than `(DESIGN_SPACE_SIZE - init_training_size)/batch_size_to_add` Default: maximum number of sl loops calculated above number_of_runs: Integer of number of runs to be done Default: 5 number_parallel_jobs: Integer giving the number of cores to be paralellized across using `joblib` Default: None (ie. will run in serial) write_to_disk: Boolean specifying whether runs history should be written to disk as jsons. Default: False write_location: String with the location where runs history jsons should be written to disk. Default: current directory json_name_prefix: Prefix used when writing out each simulated run as a json The naming convention is `{json_name_prefix}_{run #}.json` Default: acsl_run Returns ------- runs_history: List of SequentialLearner objects for each simulated run \"\"\" if number_parallel_jobs is not None : runs_history = Parallel ( n_jobs = number_parallel_jobs )( delayed ( simulated_sequential_learning )( full_design_space = full_design_space , predictor = predictor , candidate_selector = candidate_selector , number_of_sl_loops = number_of_sl_loops , init_training_size = init_training_size , ) for i in range ( number_of_runs ) ) else : runs_history = [ simulated_sequential_learning ( full_design_space = full_design_space , predictor = predictor , candidate_selector = candidate_selector , number_of_sl_loops = number_of_sl_loops , init_training_size = init_training_size , ) for i in range ( number_of_runs ) ] # TODO: separate dictionary representation and writing to disk if write_to_disk : if not os . path . isdir ( write_location ): os . makedirs ( write_location ) if json_name_prefix is None : json_name_prefix = \"acsl_run\" for i , run in enumerate ( runs_history ): name = json_name_prefix + \"_\" + str ( i ) + \".json\" run . write_json_to_disk ( write_location = write_location , json_name = name ) print ( f \"SL histories written to { write_location } \" ) return runs_history simulated_sequential_learning ( full_design_space , predictor = None , candidate_selector = None , init_training_size = 10 , number_of_sl_loops = None , write_to_disk = False , write_location = '.' , json_name = None ) Conducts a simulated sequential learning loop for a fully labelled design space to explore. Parameters: Name Type Description Default full_design_space DesignSpace Fully labelled DesignSpace to simulate being searched over required predictor: Predictor to be used for predicting properties while iterating. candidate_selector: CandidateSelector that specifies settings for candidate selection. This is where acquisition function, targets, etc. are specified. init_training_size: Size of the initial training set to be selected from the full space. Default: 10 number_of_sl_loops: Integer specifying the number of sequential learning loops to be conducted. This value cannot be greater than (DESIGN_SPACE_SIZE - init_training_size)/batch_size_to_add Default: maximum number of sl loops calculated above write_to_disk: Boolean specifying whether the resulting sequential learner should be written to disk as a json. Defaults to False. write_location: String with the location where the resulting sequential learner should be written to disk. Defaults to current directory. Returns: Name Type Description sl SequentialLearner Sequential Learner after having been iterated as specified by the input settings. Contains candidate, prediction, and uncertainty histories for further analysis as desired. Source code in autocat/learning/sequential.py 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 def simulated_sequential_learning ( full_design_space : DesignSpace , predictor : Predictor = None , candidate_selector : CandidateSelector = None , init_training_size : int = 10 , number_of_sl_loops : int = None , write_to_disk : bool = False , write_location : str = \".\" , json_name : str = None , ) -> SequentialLearner : \"\"\" Conducts a simulated sequential learning loop for a fully labelled design space to explore. Parameters ---------- full_design_space: Fully labelled DesignSpace to simulate being searched over predictor: Predictor to be used for predicting properties while iterating. candidate_selector: CandidateSelector that specifies settings for candidate selection. This is where acquisition function, targets, etc. are specified. init_training_size: Size of the initial training set to be selected from the full space. Default: 10 number_of_sl_loops: Integer specifying the number of sequential learning loops to be conducted. This value cannot be greater than `(DESIGN_SPACE_SIZE - init_training_size)/batch_size_to_add` Default: maximum number of sl loops calculated above write_to_disk: Boolean specifying whether the resulting sequential learner should be written to disk as a json. Defaults to False. write_location: String with the location where the resulting sequential learner should be written to disk. Defaults to current directory. Returns ------- sl: Sequential Learner after having been iterated as specified by the input settings. Contains candidate, prediction, and uncertainty histories for further analysis as desired. \"\"\" ds_size = len ( full_design_space ) # check fully explored if True in np . isnan ( full_design_space . design_space_labels ): missing_label_idx = np . where ( np . isnan ( full_design_space . design_space_labels ))[ 0 ] msg = ( f \"Design space must be fully explored.\" f \" Missing labels at indices: { missing_label_idx } \" ) raise SequentialLearnerError ( msg ) # check that specified initial training size makes sense if init_training_size > ds_size : msg = f \"Initial training size ( { init_training_size } ) \\ larger than design space ( { ds_size } )\" raise SequentialLearnerError ( msg ) batch_size_to_add = candidate_selector . num_candidates_to_pick max_num_sl_loops = int ( np . ceil (( ds_size - init_training_size ) / batch_size_to_add )) if number_of_sl_loops is None : number_of_sl_loops = max_num_sl_loops # check that specified number of loops is feasible if number_of_sl_loops > max_num_sl_loops : msg = ( f \"Number of SL loops ( { number_of_sl_loops } ) cannot be greater than\" f \" ( { max_num_sl_loops } )\" ) raise SequentialLearnerError ( msg ) # generate initial training set init_idx = np . zeros ( ds_size , dtype = bool ) init_idx [ np . random . choice ( ds_size , init_training_size , replace = False )] = 1 init_structs = [ full_design_space . design_space_structures [ idx ] for idx , b in enumerate ( init_idx ) if b ] init_labels = full_design_space . design_space_labels . copy () init_labels = init_labels [ np . where ( init_idx )] # default predictor settings if predictor is None : predictor = Predictor () # set up learner that is used for iteration dummy_labels = np . empty ( len ( full_design_space )) dummy_labels [:] = np . nan ds = DesignSpace ( full_design_space . design_space_structures , dummy_labels ) ds . update ( init_structs , init_labels ) sl = SequentialLearner ( design_space = ds , predictor = predictor , candidate_selector = candidate_selector , ) # first iteration on initial dataset sl . iterate () # start simulated sequential learning loop for i in range ( number_of_sl_loops ): print ( f \"Sequential Learning Iteration # { i + 1 } \" ) if sl . candidate_indices is not None : next_structs = sl . candidate_structures next_labels = full_design_space . design_space_labels . take ( sl . candidate_indices ) sl . design_space . update ( next_structs , next_labels ) sl . iterate () if write_to_disk : sl . write_json_to_disk ( write_location = write_location , json_name = json_name ) print ( f \"SL dictionary written to { write_location } \" ) return sl","title":"autocat.learning.sequential"},{"location":"API/Learning/sequential/#autocat.learning.sequential.CandidateSelector","text":"Source code in autocat/learning/sequential.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 class CandidateSelector : def __init__ ( self , acquisition_function : str = None , num_candidates_to_pick : int = None , target_window : Array = None , include_hhi : bool = None , hhi_type : str = \"production\" , include_segregation_energies : bool = None , segregation_energy_data_source : str = None , ): \"\"\" Constructor. Parameters ---------- acquisition_function: Acquisition function to be used to select the next candidates Options - MLI: maximum likelihood of improvement (default) - Random - MU: maximum uncertainty num_candidates_to_pick: Number of candidates to choose from the dataset target_window: Target window that the candidate should ideally fall within include_hhi: Whether HHI scores should be used to weight aq scores hhi_type: Type of HHI index to be used for weighting Options - production (default) - reserves include_segregation_energies: Whether segregation energies should be used to weight aq scores segregation_energy_data_source: Which tabulated data should the segregation energies be pulled from. Options: - \"raban1999\": A.V. Raban, et. al. Phys. Rev. B 59, 15990 (1999) - \"rao2020\": K. K. Rao, et. al. Topics in Catalysis volume 63, pages728-741 (2020) \"\"\" self . _acquisition_function = \"Random\" self . acquisition_function = acquisition_function self . _num_candidates_to_pick = 1 self . num_candidates_to_pick = num_candidates_to_pick self . _target_window = None self . target_window = target_window self . _include_hhi = False self . include_hhi = include_hhi self . _hhi_type = \"production\" self . hhi_type = hhi_type self . _include_segregation_energies = False self . include_segregation_energies = include_segregation_energies self . _segregation_energy_data_source = \"raban1999\" self . segregation_energy_data_source = segregation_energy_data_source @property def acquisition_function ( self ): return self . _acquisition_function @acquisition_function . setter def acquisition_function ( self , acquisition_function ): if acquisition_function is not None : if acquisition_function in [ \"MLI\" , \"MU\" , \"Random\" ]: self . _acquisition_function = acquisition_function else : msg = f \"Unrecognized acquisition function { acquisition_function } \\ Please select one of 'MLI', 'MU', or 'Random'\" raise CandidateSelectorError ( msg ) @property def num_candidates_to_pick ( self ): return self . _num_candidates_to_pick @num_candidates_to_pick . setter def num_candidates_to_pick ( self , num_candidates_to_pick ): if num_candidates_to_pick is not None : self . _num_candidates_to_pick = num_candidates_to_pick @property def target_window ( self ): return self . _target_window @target_window . setter def target_window ( self , target_window ): if target_window is not None : assert len ( target_window ) == 2 # ensure not setting infinite window if np . array_equal ( target_window , np . array ([ - np . inf , np . inf ])): msg = \"Cannot have an inifite target window\" raise CandidateSelectorError ( msg ) # sorts window bounds so min is first entry sorted_window = np . sort ( target_window ) self . _target_window = sorted_window @property def include_hhi ( self ): return self . _include_hhi @include_hhi . setter def include_hhi ( self , include_hhi ): if include_hhi is not None : self . _include_hhi = include_hhi @property def hhi_type ( self ): return self . _hhi_type @hhi_type . setter def hhi_type ( self , hhi_type ): if hhi_type is not None : if hhi_type in [ \"production\" , \"reserves\" ]: self . _hhi_type = hhi_type else : msg = f \"Unrecognized HHI type { hhi_type } . \\ Please select one of 'production' or 'reserves'\" raise CandidateSelectorError ( msg ) @property def include_segregation_energies ( self ): return self . _include_segregation_energies @include_segregation_energies . setter def include_segregation_energies ( self , include_segregation_energies ): if include_segregation_energies is not None : self . _include_segregation_energies = include_segregation_energies @property def segregation_energy_data_source ( self ): return self . _segregation_energy_data_source @segregation_energy_data_source . setter def segregation_energy_data_source ( self , segregation_energy_data_source ): if segregation_energy_data_source is not None : if segregation_energy_data_source in [ \"raban1999\" , \"rao2020\" ]: self . _segregation_energy_data_source = segregation_energy_data_source else : msg = f \"Unrecognized segregation energy data source { segregation_energy_data_source } . \\ Please select one of 'raban1999' or 'rao2020'\" raise CandidateSelectorError ( msg ) def __repr__ ( self ) -> str : pt = PrettyTable () pt . field_names = [ \"\" , \"Candidate Selector\" ] pt . add_row ([ \"acquisition function\" , self . acquisition_function ]) pt . add_row ([ \"# of candidates to pick\" , self . num_candidates_to_pick ]) pt . add_row ([ \"target window\" , self . target_window ]) pt . add_row ([ \"include hhi?\" , self . include_hhi ]) if self . include_hhi : pt . add_row ([ \"hhi type\" , self . hhi_type ]) pt . add_row ([ \"include segregation energies?\" , self . include_segregation_energies ]) pt . add_row ( [ \"segregation energies data source\" , self . segregation_energy_data_source ] ) pt . max_width = 70 return str ( pt ) def __eq__ ( self , other : object ) -> bool : if isinstance ( other , CandidateSelector ): for prop in [ \"acquisition_function\" , \"num_candidates_to_pick\" , \"include_hhi\" , \"hhi_type\" , \"include_segregation_energies\" , \"segregation_energy_data_source\" , ]: if getattr ( self , prop ) != getattr ( other , prop ): return False return np . array_equal ( self . target_window , other . target_window ) return False def copy ( self ): \"\"\" Returns a copy of the CandidateSelector \"\"\" cs = self . __class__ ( acquisition_function = self . acquisition_function , num_candidates_to_pick = self . num_candidates_to_pick , target_window = self . target_window , include_hhi = self . include_hhi , hhi_type = self . hhi_type , include_segregation_energies = self . include_segregation_energies , ) return cs def choose_candidate ( self , design_space : DesignSpace , allowed_idx : Array = None , predictions : Array = None , uncertainties : Array = None , ): \"\"\" Choose the next candidate(s) from a design space Parameters ---------- design_space: DesignSpace where candidates will be selected from allowed_idx: Allowed indices that the selector can choose from when making a recommendation Defaults to only choosing from systems with `np.nan` labels if a `DesignSpace` with unknown labels is provided. Otherwise, all structures are considered predictions: Predictions for all structures in the DesignSpace uncertainties: Uncertainties for all structures in the DesignSpace Returns ------- parent_idx: Index/indices of the selected candidates max_scores: Maximum scores (corresponding to the selected candidates) aq_scores: Calculated scores using `acquisition_function` for the entire DesignSpace \"\"\" ds_size = len ( design_space ) if allowed_idx is None : if True in np . isnan ( design_space . design_space_labels ): allowed_idx = np . where ( np . isnan ( design_space . design_space_labels ))[ 0 ] else : allowed_idx = np . ones ( ds_size , dtype = bool ) hhi_scores = np . ones ( ds_size ) if self . include_hhi : hhi_scores = calculate_hhi_scores ( design_space . design_space_structures , self . hhi_type ) segreg_energy_scores = np . ones ( ds_size ) if self . include_segregation_energies : segreg_energy_scores = calculate_segregation_energy_scores ( design_space . design_space_structures ) aq = self . acquisition_function if aq == \"Random\" : aq_scores = ( np . random . choice ( ds_size , size = ds_size , replace = False ) * hhi_scores * segreg_energy_scores ) elif aq == \"MU\" : if uncertainties is None : msg = \"For 'MU', the uncertainties must be supplied\" raise CandidateSelectorError ( msg ) aq_scores = uncertainties . copy () * hhi_scores * segreg_energy_scores elif aq == \"MLI\" : if uncertainties is None or predictions is None : msg = \"For 'MLI', both uncertainties and predictions must be supplied\" raise CandidateSelectorError ( msg ) target_window = self . target_window aq_scores = ( np . array ( [ get_overlap_score ( mean , std , x2 = target_window [ 1 ], x1 = target_window [ 0 ] ) for mean , std in zip ( predictions , uncertainties ) ] ) * hhi_scores * segreg_energy_scores ) next_idx = np . argsort ( aq_scores [ allowed_idx ])[ - self . num_candidates_to_pick :] sorted_array = aq_scores [ allowed_idx ][ next_idx ] max_scores = list ( sorted_array [ - self . num_candidates_to_pick :]) parent_idx = np . arange ( aq_scores . shape [ 0 ])[ allowed_idx ][ next_idx ] return parent_idx , max_scores , aq_scores def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" target_window = self . target_window if target_window is not None : target_window = [ float ( x ) for x in target_window ] return { \"acquisition_function\" : self . acquisition_function , \"num_candidates_to_pick\" : self . num_candidates_to_pick , \"target_window\" : target_window , \"include_hhi\" : self . include_hhi , \"hhi_type\" : self . hhi_type , \"include_segregation_energies\" : self . include_segregation_energies , \"segregation_energy_data_source\" : self . segregation_energy_data_source , } def write_json_to_disk ( self , json_name : str = None , write_location : str = \".\" , ): \"\"\" Writes CandidateSelector to disk as a json \"\"\" collected_jsons = self . to_jsonified_dict () # set default json name if needed if json_name is None : json_name = \"candidate_selector.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( collected_jsons , f ) @staticmethod def from_jsonified_dict ( all_data : Dict ): target_window = all_data . get ( \"target_window\" ) if target_window is not None : target_window = np . array ( target_window ) return CandidateSelector ( acquisition_function = all_data . get ( \"acquisition_function\" ), num_candidates_to_pick = all_data . get ( \"num_candidates_to_pick\" ), target_window = target_window , include_hhi = all_data . get ( \"include_hhi\" ), hhi_type = all_data . get ( \"hhi_type\" ), include_segregation_energies = all_data . get ( \"include_segregation_energies\" ), segregation_energy_data_source = all_data . get ( \"segregation_energy_data_source\" ), ) @staticmethod def from_json ( json_name : str ): with open ( json_name , \"r\" ) as f : all_data = json . load ( f ) return CandidateSelector . from_jsonified_dict ( all_data )","title":"CandidateSelector"},{"location":"API/Learning/sequential/#autocat.learning.sequential.CandidateSelector.__init__","text":"Constructor. Parameters: Name Type Description Default acquisition_function str Acquisition function to be used to select the next candidates Options - MLI: maximum likelihood of improvement (default) - Random - MU: maximum uncertainty None num_candidates_to_pick: Number of candidates to choose from the dataset target_window: Target window that the candidate should ideally fall within include_hhi: Whether HHI scores should be used to weight aq scores hhi_type: Type of HHI index to be used for weighting Options - production (default) - reserves include_segregation_energies: Whether segregation energies should be used to weight aq scores segregation_energy_data_source: Which tabulated data should the segregation energies be pulled from. Options: - \"raban1999\": A.V. Raban, et. al. Phys. Rev. B 59, 15990 (1999) - \"rao2020\": K. K. Rao, et. al. Topics in Catalysis volume 63, pages728-741 (2020) Source code in autocat/learning/sequential.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def __init__ ( self , acquisition_function : str = None , num_candidates_to_pick : int = None , target_window : Array = None , include_hhi : bool = None , hhi_type : str = \"production\" , include_segregation_energies : bool = None , segregation_energy_data_source : str = None , ): \"\"\" Constructor. Parameters ---------- acquisition_function: Acquisition function to be used to select the next candidates Options - MLI: maximum likelihood of improvement (default) - Random - MU: maximum uncertainty num_candidates_to_pick: Number of candidates to choose from the dataset target_window: Target window that the candidate should ideally fall within include_hhi: Whether HHI scores should be used to weight aq scores hhi_type: Type of HHI index to be used for weighting Options - production (default) - reserves include_segregation_energies: Whether segregation energies should be used to weight aq scores segregation_energy_data_source: Which tabulated data should the segregation energies be pulled from. Options: - \"raban1999\": A.V. Raban, et. al. Phys. Rev. B 59, 15990 (1999) - \"rao2020\": K. K. Rao, et. al. Topics in Catalysis volume 63, pages728-741 (2020) \"\"\" self . _acquisition_function = \"Random\" self . acquisition_function = acquisition_function self . _num_candidates_to_pick = 1 self . num_candidates_to_pick = num_candidates_to_pick self . _target_window = None self . target_window = target_window self . _include_hhi = False self . include_hhi = include_hhi self . _hhi_type = \"production\" self . hhi_type = hhi_type self . _include_segregation_energies = False self . include_segregation_energies = include_segregation_energies self . _segregation_energy_data_source = \"raban1999\" self . segregation_energy_data_source = segregation_energy_data_source","title":"__init__()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.CandidateSelector.choose_candidate","text":"Choose the next candidate(s) from a design space Parameters: Name Type Description Default design_space DesignSpace DesignSpace where candidates will be selected from required allowed_idx: Allowed indices that the selector can choose from when making a recommendation Defaults to only choosing from systems with np.nan labels if a DesignSpace with unknown labels is provided. Otherwise, all structures are considered predictions: Predictions for all structures in the DesignSpace uncertainties: Uncertainties for all structures in the DesignSpace Returns: Name Type Description parent_idx Index/indices of the selected candidates max_scores: Maximum scores (corresponding to the selected candidates) aq_scores: Calculated scores using acquisition_function for the entire DesignSpace Source code in autocat/learning/sequential.py 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 def choose_candidate ( self , design_space : DesignSpace , allowed_idx : Array = None , predictions : Array = None , uncertainties : Array = None , ): \"\"\" Choose the next candidate(s) from a design space Parameters ---------- design_space: DesignSpace where candidates will be selected from allowed_idx: Allowed indices that the selector can choose from when making a recommendation Defaults to only choosing from systems with `np.nan` labels if a `DesignSpace` with unknown labels is provided. Otherwise, all structures are considered predictions: Predictions for all structures in the DesignSpace uncertainties: Uncertainties for all structures in the DesignSpace Returns ------- parent_idx: Index/indices of the selected candidates max_scores: Maximum scores (corresponding to the selected candidates) aq_scores: Calculated scores using `acquisition_function` for the entire DesignSpace \"\"\" ds_size = len ( design_space ) if allowed_idx is None : if True in np . isnan ( design_space . design_space_labels ): allowed_idx = np . where ( np . isnan ( design_space . design_space_labels ))[ 0 ] else : allowed_idx = np . ones ( ds_size , dtype = bool ) hhi_scores = np . ones ( ds_size ) if self . include_hhi : hhi_scores = calculate_hhi_scores ( design_space . design_space_structures , self . hhi_type ) segreg_energy_scores = np . ones ( ds_size ) if self . include_segregation_energies : segreg_energy_scores = calculate_segregation_energy_scores ( design_space . design_space_structures ) aq = self . acquisition_function if aq == \"Random\" : aq_scores = ( np . random . choice ( ds_size , size = ds_size , replace = False ) * hhi_scores * segreg_energy_scores ) elif aq == \"MU\" : if uncertainties is None : msg = \"For 'MU', the uncertainties must be supplied\" raise CandidateSelectorError ( msg ) aq_scores = uncertainties . copy () * hhi_scores * segreg_energy_scores elif aq == \"MLI\" : if uncertainties is None or predictions is None : msg = \"For 'MLI', both uncertainties and predictions must be supplied\" raise CandidateSelectorError ( msg ) target_window = self . target_window aq_scores = ( np . array ( [ get_overlap_score ( mean , std , x2 = target_window [ 1 ], x1 = target_window [ 0 ] ) for mean , std in zip ( predictions , uncertainties ) ] ) * hhi_scores * segreg_energy_scores ) next_idx = np . argsort ( aq_scores [ allowed_idx ])[ - self . num_candidates_to_pick :] sorted_array = aq_scores [ allowed_idx ][ next_idx ] max_scores = list ( sorted_array [ - self . num_candidates_to_pick :]) parent_idx = np . arange ( aq_scores . shape [ 0 ])[ allowed_idx ][ next_idx ] return parent_idx , max_scores , aq_scores","title":"choose_candidate()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.CandidateSelector.copy","text":"Returns a copy of the CandidateSelector Source code in autocat/learning/sequential.py 409 410 411 412 413 414 415 416 417 418 419 420 421 def copy ( self ): \"\"\" Returns a copy of the CandidateSelector \"\"\" cs = self . __class__ ( acquisition_function = self . acquisition_function , num_candidates_to_pick = self . num_candidates_to_pick , target_window = self . target_window , include_hhi = self . include_hhi , hhi_type = self . hhi_type , include_segregation_energies = self . include_segregation_energies , ) return cs","title":"copy()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.CandidateSelector.to_jsonified_dict","text":"Returns a jsonified dict representation Source code in autocat/learning/sequential.py 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" target_window = self . target_window if target_window is not None : target_window = [ float ( x ) for x in target_window ] return { \"acquisition_function\" : self . acquisition_function , \"num_candidates_to_pick\" : self . num_candidates_to_pick , \"target_window\" : target_window , \"include_hhi\" : self . include_hhi , \"hhi_type\" : self . hhi_type , \"include_segregation_energies\" : self . include_segregation_energies , \"segregation_energy_data_source\" : self . segregation_energy_data_source , }","title":"to_jsonified_dict()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.CandidateSelector.write_json_to_disk","text":"Writes CandidateSelector to disk as a json Source code in autocat/learning/sequential.py 538 539 540 541 542 543 544 545 546 547 548 549 550 551 def write_json_to_disk ( self , json_name : str = None , write_location : str = \".\" , ): \"\"\" Writes CandidateSelector to disk as a json \"\"\" collected_jsons = self . to_jsonified_dict () # set default json name if needed if json_name is None : json_name = \"candidate_selector.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( collected_jsons , f )","title":"write_json_to_disk()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.DesignSpace","text":"Source code in autocat/learning/sequential.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 class DesignSpace : def __init__ ( self , design_space_structures : List [ Atoms ], design_space_labels : Array , ): \"\"\" Constructor. Parameters ---------- design_space_structures: List of all structures within the design space design_space_labels: Labels corresponding to all structures within the design space. If label not yet known, set to np.nan \"\"\" if len ( design_space_structures ) != design_space_labels . shape [ 0 ]: msg = f \"Number of structures ( { len ( design_space_structures ) } ) \\ and labels ( { design_space_labels . shape [ 0 ] } ) must match\" raise DesignSpaceError ( msg ) self . _design_space_structures = [ struct . copy () for struct in design_space_structures ] self . _design_space_labels = design_space_labels . copy () def __repr__ ( self ) -> str : pt = PrettyTable () pt . field_names = [ \"\" , \"DesignSpace\" ] pt . add_row ([ \"total # of systems\" , len ( self )]) num_unknown = sum ( np . isnan ( self . design_space_labels )) pt . add_row ([ \"# of unlabelled systems\" , num_unknown ]) pt . add_row ([ \"unique species present\" , self . species_list ]) max_label = max ( self . design_space_labels ) pt . add_row ([ \"maximum label\" , max_label ]) min_label = min ( self . design_space_labels ) pt . add_row ([ \"minimum label\" , min_label ]) pt . max_width = 70 return str ( pt ) def __len__ ( self ): return len ( self . design_space_structures ) # TODO: non-dunder method for deleting systems def __delitem__ ( self , i ): \"\"\" Deletes systems from the design space. If mask provided, deletes wherever True \"\"\" if isinstance ( i , list ): i = np . array ( i ) elif isinstance ( i , int ): i = [ i ] mask = np . ones ( len ( self ), dtype = bool ) mask [ i ] = 0 self . _design_space_labels = self . design_space_labels [ mask ] structs = self . design_space_structures masked_structs = [ structs [ j ] for j in range ( len ( self )) if mask [ j ]] self . _design_space_structures = masked_structs def __eq__ ( self , other : object ) -> bool : if isinstance ( other , DesignSpace ): # check that they are the same length if len ( self ) == len ( other ): # check all their structures are equal self_structs = self . design_space_structures o_structs = other . design_space_structures if not self_structs == o_structs : return False # check their labels are equal self_labels = self . design_space_labels o_labels = other . design_space_labels return np . array_equal ( self_labels , o_labels , equal_nan = True ) return False def copy ( self ): \"\"\" Returns a copy of the design space \"\"\" acds = self . __class__ ( design_space_structures = self . design_space_structures , design_space_labels = self . design_space_labels , ) return acds @property def design_space_structures ( self ): return self . _design_space_structures @design_space_structures . setter def design_space_structures ( self , design_space_structures ): msg = \"Please use `update` method to update the design space.\" raise DesignSpaceError ( msg ) @property def design_space_labels ( self ): return self . _design_space_labels @design_space_labels . setter def design_space_labels ( self , design_space_labels ): msg = \"Please use `update` method to update the design space.\" raise DesignSpaceError ( msg ) @property def species_list ( self ): species_list = [] for s in self . design_space_structures : # get all unique species found_species = np . unique ( s . get_chemical_symbols ()) . tolist () new_species = [ spec for spec in found_species if spec not in species_list ] species_list . extend ( new_species ) return species_list def update ( self , structures : List [ Atoms ], labels : Array ): \"\"\" Updates design space given structures and corresponding labels. If structure already in design space, the label is updated. Parameters ---------- structures: List of Atoms objects structures to be added labels: Corresponding labels to `structures` \"\"\" if ( structures is not None ) and ( labels is not None ): assert len ( structures ) == len ( labels ) assert all ( isinstance ( struct , Atoms ) for struct in structures ) for i , struct in enumerate ( structures ): # if structure already in design space, update label if struct in self . design_space_structures : idx = self . design_space_structures . index ( struct ) self . _design_space_labels [ idx ] = labels [ i ] # otherwise extend design space else : self . _design_space_structures . append ( struct ) self . _design_space_labels = np . append ( self . design_space_labels , labels [ i ] ) def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" collected_structs = [] for struct in self . design_space_structures : collected_structs . append ( atoms_encoder ( struct )) jsonified_labels = [ float ( x ) for x in self . design_space_labels ] return { \"structures\" : collected_structs , \"labels\" : jsonified_labels } def write_json_to_disk ( self , json_name : str = None , write_location : str = \".\" , ): \"\"\" Writes DesignSpace to disk as a json \"\"\" collected_jsons = self . to_jsonified_dict () # set default json name if needed if json_name is None : json_name = \"acds.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( collected_jsons , f ) @staticmethod def from_jsonified_dict ( all_data : Dict ): if all_data . get ( \"structures\" ) is None or all_data . get ( \"labels\" ) is None : msg = \"Both structures and labels must be provided\" raise DesignSpaceError ( msg ) try : structures = [] for encoded_atoms in all_data [ \"structures\" ]: structures . append ( atoms_decoder ( encoded_atoms )) except ( json . JSONDecodeError , TypeError ): msg = \"Please ensure design space structures encoded using `ase.io.jsonio.encode`\" raise DesignSpaceError ( msg ) labels = np . array ( all_data [ \"labels\" ]) return DesignSpace ( design_space_structures = structures , design_space_labels = labels , ) @staticmethod def from_json ( json_name : str ): with open ( json_name , \"r\" ) as f : all_data = json . load ( f ) return DesignSpace . from_jsonified_dict ( all_data )","title":"DesignSpace"},{"location":"API/Learning/sequential/#autocat.learning.sequential.DesignSpace.__delitem__","text":"Deletes systems from the design space. If mask provided, deletes wherever True Source code in autocat/learning/sequential.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def __delitem__ ( self , i ): \"\"\" Deletes systems from the design space. If mask provided, deletes wherever True \"\"\" if isinstance ( i , list ): i = np . array ( i ) elif isinstance ( i , int ): i = [ i ] mask = np . ones ( len ( self ), dtype = bool ) mask [ i ] = 0 self . _design_space_labels = self . design_space_labels [ mask ] structs = self . design_space_structures masked_structs = [ structs [ j ] for j in range ( len ( self )) if mask [ j ]] self . _design_space_structures = masked_structs","title":"__delitem__()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.DesignSpace.__init__","text":"Constructor. Parameters: Name Type Description Default design_space_structures List [ Atoms ] List of all structures within the design space required design_space_labels: Labels corresponding to all structures within the design space. If label not yet known, set to np.nan Source code in autocat/learning/sequential.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , design_space_structures : List [ Atoms ], design_space_labels : Array , ): \"\"\" Constructor. Parameters ---------- design_space_structures: List of all structures within the design space design_space_labels: Labels corresponding to all structures within the design space. If label not yet known, set to np.nan \"\"\" if len ( design_space_structures ) != design_space_labels . shape [ 0 ]: msg = f \"Number of structures ( { len ( design_space_structures ) } ) \\ and labels ( { design_space_labels . shape [ 0 ] } ) must match\" raise DesignSpaceError ( msg ) self . _design_space_structures = [ struct . copy () for struct in design_space_structures ] self . _design_space_labels = design_space_labels . copy ()","title":"__init__()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.DesignSpace.copy","text":"Returns a copy of the design space Source code in autocat/learning/sequential.py 104 105 106 107 108 109 110 111 112 def copy ( self ): \"\"\" Returns a copy of the design space \"\"\" acds = self . __class__ ( design_space_structures = self . design_space_structures , design_space_labels = self . design_space_labels , ) return acds","title":"copy()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.DesignSpace.to_jsonified_dict","text":"Returns a jsonified dict representation Source code in autocat/learning/sequential.py 171 172 173 174 175 176 177 178 179 def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" collected_structs = [] for struct in self . design_space_structures : collected_structs . append ( atoms_encoder ( struct )) jsonified_labels = [ float ( x ) for x in self . design_space_labels ] return { \"structures\" : collected_structs , \"labels\" : jsonified_labels }","title":"to_jsonified_dict()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.DesignSpace.update","text":"Updates design space given structures and corresponding labels. If structure already in design space, the label is updated. Parameters: Name Type Description Default structures List [ Atoms ] List of Atoms objects structures to be added required labels: Corresponding labels to structures Source code in autocat/learning/sequential.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def update ( self , structures : List [ Atoms ], labels : Array ): \"\"\" Updates design space given structures and corresponding labels. If structure already in design space, the label is updated. Parameters ---------- structures: List of Atoms objects structures to be added labels: Corresponding labels to `structures` \"\"\" if ( structures is not None ) and ( labels is not None ): assert len ( structures ) == len ( labels ) assert all ( isinstance ( struct , Atoms ) for struct in structures ) for i , struct in enumerate ( structures ): # if structure already in design space, update label if struct in self . design_space_structures : idx = self . design_space_structures . index ( struct ) self . _design_space_labels [ idx ] = labels [ i ] # otherwise extend design space else : self . _design_space_structures . append ( struct ) self . _design_space_labels = np . append ( self . design_space_labels , labels [ i ] )","title":"update()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.DesignSpace.write_json_to_disk","text":"Writes DesignSpace to disk as a json Source code in autocat/learning/sequential.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def write_json_to_disk ( self , json_name : str = None , write_location : str = \".\" , ): \"\"\" Writes DesignSpace to disk as a json \"\"\" collected_jsons = self . to_jsonified_dict () # set default json name if needed if json_name is None : json_name = \"acds.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( collected_jsons , f )","title":"write_json_to_disk()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.SequentialLearner","text":"Source code in autocat/learning/sequential.py 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 class SequentialLearner : def __init__ ( self , design_space : DesignSpace , predictor : Predictor = None , candidate_selector : CandidateSelector = None , sl_kwargs : Dict [ str , int ] = None , ): \"\"\" Constructor. Parameters ---------- design_space: DesignSpace that is being explored predictor: Predictor used for training and predicting on the desired property candidate_selector: CandidateSelector used for calculating scores and selecting candidates for each iteration \"\"\" # TODO: move predefined attributes (train_idx, candidate_idxs) to a # different container (not kwargs) self . _design_space = None self . design_space = design_space . copy () self . _predictor = Predictor () self . predictor = predictor self . _candidate_selector = CandidateSelector () self . candidate_selector = candidate_selector # other miscellaneous kw arguments self . sl_kwargs = sl_kwargs if sl_kwargs else {} # variables that need to be propagated through the SL process if \"iteration_count\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"iteration_count\" : 0 }) if \"train_idx\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"train_idx\" : None }) if \"train_idx_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"train_idx_history\" : None }) if \"predictions\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"predictions\" : None }) if \"predictions_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"predictions_history\" : None }) if \"uncertainties\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"uncertainties\" : None }) if \"uncertainties_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"uncertainties_history\" : None }) if \"candidate_indices\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"candidate_indices\" : None }) if \"candidate_index_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"candidate_index_history\" : None }) if \"acquisition_scores\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"acquisition_scores\" : None }) def __repr__ ( self ) -> str : pt = PrettyTable () pt . field_names = [ \"\" , \"Sequential Learner\" ] pt . add_row ([ \"iteration count\" , self . iteration_count ]) if self . candidate_structures is not None : cand_formulas = [ s . get_chemical_formula () for s in self . candidate_structures ] else : cand_formulas = None pt . add_row ([ \"next candidate system structures\" , cand_formulas ]) pt . add_row ([ \"next candidate system indices\" , self . candidate_indices ]) return ( str ( pt ) + \" \\n \" + str ( self . candidate_selector ) + \" \\n \" + str ( self . design_space ) + \" \\n \" + str ( self . predictor ) ) @property def design_space ( self ): return self . _design_space @design_space . setter def design_space ( self , design_space ): if design_space is not None and isinstance ( design_space , DesignSpace ): self . _design_space = design_space @property def predictor ( self ): return self . _predictor @predictor . setter def predictor ( self , predictor ): if predictor is not None and isinstance ( predictor , Predictor ): feat = predictor . featurizer . copy () feat . design_space_structures = self . design_space . design_space_structures self . _predictor = Predictor ( regressor = predictor . regressor , featurizer = feat ) @property def candidate_selector ( self ): return self . _candidate_selector @candidate_selector . setter def candidate_selector ( self , candidate_selector ): if candidate_selector is not None and isinstance ( candidate_selector , CandidateSelector ): self . _candidate_selector = candidate_selector . copy () @property def iteration_count ( self ): return self . sl_kwargs . get ( \"iteration_count\" , 0 ) @property def train_idx ( self ): return self . sl_kwargs . get ( \"train_idx\" ) @property def train_idx_history ( self ): return self . sl_kwargs . get ( \"train_idx_history\" , None ) @property def predictions ( self ): return self . sl_kwargs . get ( \"predictions\" ) @property def uncertainties ( self ): return self . sl_kwargs . get ( \"uncertainties\" ) @property def candidate_indices ( self ): return self . sl_kwargs . get ( \"candidate_indices\" ) @property def acquisition_scores ( self ): return self . sl_kwargs . get ( \"acquisition_scores\" , None ) @property def candidate_structures ( self ): idxs = self . candidate_indices if idxs is not None : return [ self . design_space . design_space_structures [ i ] for i in idxs ] @property def candidate_index_history ( self ): return self . sl_kwargs . get ( \"candidate_index_history\" , None ) @property def predictions_history ( self ): return self . sl_kwargs . get ( \"predictions_history\" , None ) @property def uncertainties_history ( self ): return self . sl_kwargs . get ( \"uncertainties_history\" , None ) def copy ( self ): \"\"\" Returns a copy \"\"\" acsl = self . __class__ ( design_space = self . design_space , predictor = self . predictor , candidate_selector = self . candidate_selector , ) acsl . sl_kwargs = copy . deepcopy ( self . sl_kwargs ) return acsl def iterate ( self ): \"\"\"Runs the next iteration of sequential learning. This process consists of: - retraining the predictor - predicting candidate properties and calculating candidate scores (if fully explored returns None) - selecting the next batch of candidates for objective evaluation (if fully explored returns None) \"\"\" dstructs = self . design_space . design_space_structures dlabels = self . design_space . design_space_labels mask_nans = ~ np . isnan ( dlabels ) masked_structs = [ struct for i , struct in enumerate ( dstructs ) if mask_nans [ i ]] masked_labels = dlabels [ np . where ( mask_nans )] self . predictor . fit ( masked_structs , masked_labels ) train_idx = np . zeros ( len ( dlabels ), dtype = bool ) train_idx [ np . where ( mask_nans )] = 1 self . sl_kwargs . update ({ \"train_idx\" : train_idx }) train_idx_hist = self . sl_kwargs . get ( \"train_idx_history\" ) if train_idx_hist is None : train_idx_hist = [] train_idx_hist . append ( train_idx ) self . sl_kwargs . update ({ \"train_idx_history\" : train_idx_hist }) preds , unc = self . predictor . predict ( dstructs ) # update predictions and store in history self . sl_kwargs . update ({ \"predictions\" : preds }) pred_hist = self . sl_kwargs . get ( \"predictions_history\" ) if pred_hist is None : pred_hist = [] pred_hist . append ( preds ) self . sl_kwargs . update ({ \"predictions_history\" : pred_hist }) # update uncertainties and store in history self . sl_kwargs . update ({ \"uncertainties\" : unc }) unc_hist = self . sl_kwargs . get ( \"uncertainties_history\" ) if unc_hist is None : unc_hist = [] unc_hist . append ( unc ) self . sl_kwargs . update ({ \"uncertainties_history\" : unc_hist }) # make sure haven't fully searched design space if any ([ np . isnan ( label ) for label in dlabels ]): candidate_idx , _ , aq_scores = self . candidate_selector . choose_candidate ( design_space = self . design_space , allowed_idx =~ train_idx , predictions = preds , uncertainties = unc , ) # if fully searched, no more candidate structures else : candidate_idx = None aq_scores = None self . sl_kwargs . update ({ \"candidate_indices\" : candidate_idx }) self . sl_kwargs . update ({ \"acquisition_scores\" : aq_scores }) # update the candidate index history if new candidate if candidate_idx is not None : cand_idx_hist = self . sl_kwargs . get ( \"candidate_index_history\" ) if cand_idx_hist is None : cand_idx_hist = [] cand_idx_hist . append ( candidate_idx ) self . sl_kwargs . update ({ \"candidate_index_history\" : cand_idx_hist }) # update the SL iteration count itc = self . sl_kwargs . get ( \"iteration_count\" , 0 ) self . sl_kwargs . update ({ \"iteration_count\" : itc + 1 }) def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" # get jsonified design space jsonified_ds = self . design_space . to_jsonified_dict () # get jsonified predictor jsonified_pred = self . predictor . to_jsonified_dict () # get jsonified candidate selector jsonified_cs = self . candidate_selector . to_jsonified_dict () # jsonify the sl kwargs jsonified_sl_kwargs = {} for k in self . sl_kwargs : if k != \"iteration_count\" and self . sl_kwargs [ k ] is not None : jsonified_sl_kwargs [ k ] = [] for arr in self . sl_kwargs [ k ]: if arr is not None : jsonified_sl_kwargs [ k ] . append ( arr . tolist ()) else : jsonified_sl_kwargs [ k ] . append ( None ) elif k == \"iteration_count\" : jsonified_sl_kwargs [ \"iteration_count\" ] = self . sl_kwargs [ \"iteration_count\" ] elif self . sl_kwargs [ k ] is None : jsonified_sl_kwargs [ k ] = None return { \"design_space\" : jsonified_ds , \"predictor\" : jsonified_pred , \"candidate_selector\" : jsonified_cs , \"sl_kwargs\" : jsonified_sl_kwargs , } def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `SequentialLearner` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"sequential_learner.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f ) @staticmethod def from_jsonified_dict ( all_data : Dict ): if all_data . get ( \"design_space\" ) is None : msg = \"DesignSpace must be provided\" raise SequentialLearnerError ( msg ) design_space = DesignSpace . from_jsonified_dict ( all_data [ \"design_space\" ]) predictor = Predictor . from_jsonified_dict ( all_data . get ( \"predictor\" , {})) candidate_selector = CandidateSelector . from_jsonified_dict ( all_data . get ( \"candidate_selector\" , {}) ) raw_sl_kwargs = all_data . get ( \"sl_kwargs\" , {}) sl_kwargs = {} for k in raw_sl_kwargs : if raw_sl_kwargs [ k ] is not None : if k in [ \"predictions\" , \"uncertainties\" , \"acquisition_scores\" , \"candidate_indices\" , ]: sl_kwargs [ k ] = np . array ( raw_sl_kwargs [ k ]) elif k in [ \"predictions_history\" , \"uncertainties_history\" , \"candidate_index_history\" , ]: sl_kwargs [ k ] = [ np . array ( i ) for i in raw_sl_kwargs [ k ]] elif k == \"iteration_count\" : sl_kwargs [ k ] = raw_sl_kwargs [ k ] elif k == \"train_idx\" : sl_kwargs [ k ] = np . array ( raw_sl_kwargs [ k ], dtype = bool ) elif k == \"train_idx_history\" : sl_kwargs [ k ] = [ np . array ( i , dtype = bool ) for i in raw_sl_kwargs [ k ]] else : sl_kwargs [ k ] = None return SequentialLearner ( design_space = design_space , predictor = predictor , candidate_selector = candidate_selector , sl_kwargs = sl_kwargs , ) @staticmethod def from_json ( json_name : str ): with open ( json_name , \"r\" ) as f : all_data = json . load ( f ) return SequentialLearner . from_jsonified_dict ( all_data )","title":"SequentialLearner"},{"location":"API/Learning/sequential/#autocat.learning.sequential.SequentialLearner.__init__","text":"Constructor. Parameters: Name Type Description Default design_space DesignSpace DesignSpace that is being explored required predictor: Predictor used for training and predicting on the desired property candidate_selector: CandidateSelector used for calculating scores and selecting candidates for each iteration Source code in autocat/learning/sequential.py 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def __init__ ( self , design_space : DesignSpace , predictor : Predictor = None , candidate_selector : CandidateSelector = None , sl_kwargs : Dict [ str , int ] = None , ): \"\"\" Constructor. Parameters ---------- design_space: DesignSpace that is being explored predictor: Predictor used for training and predicting on the desired property candidate_selector: CandidateSelector used for calculating scores and selecting candidates for each iteration \"\"\" # TODO: move predefined attributes (train_idx, candidate_idxs) to a # different container (not kwargs) self . _design_space = None self . design_space = design_space . copy () self . _predictor = Predictor () self . predictor = predictor self . _candidate_selector = CandidateSelector () self . candidate_selector = candidate_selector # other miscellaneous kw arguments self . sl_kwargs = sl_kwargs if sl_kwargs else {} # variables that need to be propagated through the SL process if \"iteration_count\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"iteration_count\" : 0 }) if \"train_idx\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"train_idx\" : None }) if \"train_idx_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"train_idx_history\" : None }) if \"predictions\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"predictions\" : None }) if \"predictions_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"predictions_history\" : None }) if \"uncertainties\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"uncertainties\" : None }) if \"uncertainties_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"uncertainties_history\" : None }) if \"candidate_indices\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"candidate_indices\" : None }) if \"candidate_index_history\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"candidate_index_history\" : None }) if \"acquisition_scores\" not in self . sl_kwargs : self . sl_kwargs . update ({ \"acquisition_scores\" : None })","title":"__init__()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.SequentialLearner.copy","text":"Returns a copy Source code in autocat/learning/sequential.py 742 743 744 745 746 747 748 749 750 751 752 def copy ( self ): \"\"\" Returns a copy \"\"\" acsl = self . __class__ ( design_space = self . design_space , predictor = self . predictor , candidate_selector = self . candidate_selector , ) acsl . sl_kwargs = copy . deepcopy ( self . sl_kwargs ) return acsl","title":"copy()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.SequentialLearner.iterate","text":"Runs the next iteration of sequential learning. This process consists of: - retraining the predictor - predicting candidate properties and calculating candidate scores (if fully explored returns None) - selecting the next batch of candidates for objective evaluation (if fully explored returns None) Source code in autocat/learning/sequential.py 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 def iterate ( self ): \"\"\"Runs the next iteration of sequential learning. This process consists of: - retraining the predictor - predicting candidate properties and calculating candidate scores (if fully explored returns None) - selecting the next batch of candidates for objective evaluation (if fully explored returns None) \"\"\" dstructs = self . design_space . design_space_structures dlabels = self . design_space . design_space_labels mask_nans = ~ np . isnan ( dlabels ) masked_structs = [ struct for i , struct in enumerate ( dstructs ) if mask_nans [ i ]] masked_labels = dlabels [ np . where ( mask_nans )] self . predictor . fit ( masked_structs , masked_labels ) train_idx = np . zeros ( len ( dlabels ), dtype = bool ) train_idx [ np . where ( mask_nans )] = 1 self . sl_kwargs . update ({ \"train_idx\" : train_idx }) train_idx_hist = self . sl_kwargs . get ( \"train_idx_history\" ) if train_idx_hist is None : train_idx_hist = [] train_idx_hist . append ( train_idx ) self . sl_kwargs . update ({ \"train_idx_history\" : train_idx_hist }) preds , unc = self . predictor . predict ( dstructs ) # update predictions and store in history self . sl_kwargs . update ({ \"predictions\" : preds }) pred_hist = self . sl_kwargs . get ( \"predictions_history\" ) if pred_hist is None : pred_hist = [] pred_hist . append ( preds ) self . sl_kwargs . update ({ \"predictions_history\" : pred_hist }) # update uncertainties and store in history self . sl_kwargs . update ({ \"uncertainties\" : unc }) unc_hist = self . sl_kwargs . get ( \"uncertainties_history\" ) if unc_hist is None : unc_hist = [] unc_hist . append ( unc ) self . sl_kwargs . update ({ \"uncertainties_history\" : unc_hist }) # make sure haven't fully searched design space if any ([ np . isnan ( label ) for label in dlabels ]): candidate_idx , _ , aq_scores = self . candidate_selector . choose_candidate ( design_space = self . design_space , allowed_idx =~ train_idx , predictions = preds , uncertainties = unc , ) # if fully searched, no more candidate structures else : candidate_idx = None aq_scores = None self . sl_kwargs . update ({ \"candidate_indices\" : candidate_idx }) self . sl_kwargs . update ({ \"acquisition_scores\" : aq_scores }) # update the candidate index history if new candidate if candidate_idx is not None : cand_idx_hist = self . sl_kwargs . get ( \"candidate_index_history\" ) if cand_idx_hist is None : cand_idx_hist = [] cand_idx_hist . append ( candidate_idx ) self . sl_kwargs . update ({ \"candidate_index_history\" : cand_idx_hist }) # update the SL iteration count itc = self . sl_kwargs . get ( \"iteration_count\" , 0 ) self . sl_kwargs . update ({ \"iteration_count\" : itc + 1 })","title":"iterate()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.SequentialLearner.to_jsonified_dict","text":"Returns a jsonified dict representation Source code in autocat/learning/sequential.py 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 def to_jsonified_dict ( self ) -> Dict : \"\"\" Returns a jsonified dict representation \"\"\" # get jsonified design space jsonified_ds = self . design_space . to_jsonified_dict () # get jsonified predictor jsonified_pred = self . predictor . to_jsonified_dict () # get jsonified candidate selector jsonified_cs = self . candidate_selector . to_jsonified_dict () # jsonify the sl kwargs jsonified_sl_kwargs = {} for k in self . sl_kwargs : if k != \"iteration_count\" and self . sl_kwargs [ k ] is not None : jsonified_sl_kwargs [ k ] = [] for arr in self . sl_kwargs [ k ]: if arr is not None : jsonified_sl_kwargs [ k ] . append ( arr . tolist ()) else : jsonified_sl_kwargs [ k ] . append ( None ) elif k == \"iteration_count\" : jsonified_sl_kwargs [ \"iteration_count\" ] = self . sl_kwargs [ \"iteration_count\" ] elif self . sl_kwargs [ k ] is None : jsonified_sl_kwargs [ k ] = None return { \"design_space\" : jsonified_ds , \"predictor\" : jsonified_pred , \"candidate_selector\" : jsonified_cs , \"sl_kwargs\" : jsonified_sl_kwargs , }","title":"to_jsonified_dict()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.SequentialLearner.write_json_to_disk","text":"Writes SequentialLearner to disk as a json Source code in autocat/learning/sequential.py 861 862 863 864 865 866 867 868 869 870 871 872 873 def write_json_to_disk ( self , write_location : str = \".\" , json_name : str = None ): \"\"\" Writes `SequentialLearner` to disk as a json \"\"\" jsonified_list = self . to_jsonified_dict () if json_name is None : json_name = \"sequential_learner.json\" json_path = os . path . join ( write_location , json_name ) with open ( json_path , \"w\" ) as f : json . dump ( jsonified_list , f )","title":"write_json_to_disk()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.calculate_hhi_scores","text":"Calculates HHI scores for structures weighted by their composition. The scores are normalized and inverted such that these should be maximized in the interest of finding a low cost system Parameters: Name Type Description Default structures List [ Atoms ] List of Atoms objects for which to calculate the scores required hhi_type: Type of HHI index to be used for the score Options - production (default) - reserves exclude_species: Species to be excluded when calculating the scores. An example use-case would be comparing transition-metal oxides where we can ignore the presence of O in each. Returns: Name Type Description hhi_scores Scores corresponding to each of the provided structures Source code in autocat/learning/sequential.py 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 def calculate_hhi_scores ( structures : List [ Atoms ], hhi_type : str = \"production\" , exclude_species : List [ str ] = None , ): \"\"\" Calculates HHI scores for structures weighted by their composition. The scores are normalized and inverted such that these should be maximized in the interest of finding a low cost system Parameters ---------- structures: List of Atoms objects for which to calculate the scores hhi_type: Type of HHI index to be used for the score Options - production (default) - reserves exclude_species: Species to be excluded when calculating the scores. An example use-case would be comparing transition-metal oxides where we can ignore the presence of O in each. Returns ------- hhi_scores: Scores corresponding to each of the provided structures \"\"\" if structures is None : msg = \"To include HHI, the structures must be provided\" raise SequentialLearnerError ( msg ) raw_hhi_data = HHI max_hhi = np . max ([ raw_hhi_data [ hhi_type ][ r ] for r in raw_hhi_data [ hhi_type ]]) min_hhi = np . min ([ raw_hhi_data [ hhi_type ][ r ] for r in raw_hhi_data [ hhi_type ]]) # normalize and invert (so that this score is to be maximized) norm_hhi_data = { el : 1.0 - ( raw_hhi_data [ hhi_type ][ el ] - min_hhi ) / ( max_hhi - min_hhi ) for el in raw_hhi_data [ hhi_type ] } hhi_scores = np . zeros ( len ( structures )) for idx , struct in enumerate ( structures ): hhi = 0 el_counts = struct . symbols . formula . count () if exclude_species is not None : for species in exclude_species : el_counts [ species ] = 0 tot_size = sum ( el_counts . values ()) # weight calculated hhi score by composition for el in el_counts : hhi += norm_hhi_data [ el ] * el_counts [ el ] / tot_size hhi_scores [ idx ] = hhi return hhi_scores","title":"calculate_hhi_scores()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.calculate_segregation_energy_scores","text":"Calculates HHI scores for structures weighted by their composition. The scores are normalized and inverted such that these should be maximized in the interest of finding a low cost system Parameters: Name Type Description Default structures List [ Atoms ] List of Atoms objects for which to calculate the scores required data_source: Which tabulated data should the segregation energies be pulled from. Options: - \"raban1999\": A.V. Raban, et. al. Phys. Rev. B 59, 15990 - \"rao2020\": K. K. Rao, et. al. Topics in Catalysis volume 63, pages728-741 (2020) Returns: Name Type Description hhi_scores Scores corresponding to each of the provided structures Source code in autocat/learning/sequential.py 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 def calculate_segregation_energy_scores ( structures : List [ Atoms ], data_source : str = \"raban1999\" ): \"\"\" Calculates HHI scores for structures weighted by their composition. The scores are normalized and inverted such that these should be maximized in the interest of finding a low cost system Parameters ---------- structures: List of Atoms objects for which to calculate the scores data_source: Which tabulated data should the segregation energies be pulled from. Options: - \"raban1999\": A.V. Raban, et. al. Phys. Rev. B 59, 15990 - \"rao2020\": K. K. Rao, et. al. Topics in Catalysis volume 63, pages728-741 (2020) Returns ------- hhi_scores: Scores corresponding to each of the provided structures \"\"\" if structures is None : msg = \"To include segregation energies, the structures must be provided\" raise SequentialLearnerError ( msg ) if data_source == \"raban1999\" : # won't consider surface energies (ie. dop == host) for normalization max_seg_ener = SEGREGATION_ENERGIES [ \"raban1999\" ][ \"Pd\" ][ \"W\" ] min_seg_ener = SEGREGATION_ENERGIES [ \"raban1999\" ][ \"Fe_100\" ][ \"Ag\" ] # normalize and invert (so that this score is to be maximized) norm_seg_ener_data = {} for hsp in SEGREGATION_ENERGIES [ \"raban1999\" ]: norm_seg_ener_data [ hsp ] = {} for dsp in SEGREGATION_ENERGIES [ \"raban1999\" ][ hsp ]: norm_seg_ener_data [ hsp ][ dsp ] = 1.0 - ( SEGREGATION_ENERGIES [ \"raban1999\" ][ hsp ][ dsp ] - min_seg_ener ) / ( max_seg_ener - min_seg_ener ) elif data_source == \"rao2020\" : norm_seg_ener_data = SEGREGATION_ENERGIES [ \"rao2020\" ] else : msg = f \"Unknown data source { data_source } \" raise SequentialLearnerError ( msg ) seg_ener_scores = np . zeros ( len ( structures )) for idx , struct in enumerate ( structures ): el_counts = struct . symbols . formula . count () assert len ( el_counts ) == 2 for el in el_counts : if el_counts [ el ] == 1 : dsp = el else : hsp = el seg_ener_scores [ idx ] = norm_seg_ener_data [ hsp ][ dsp ] return seg_ener_scores","title":"calculate_segregation_energy_scores()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.get_overlap_score","text":"Calculate overlap score given targets x2 (max) and x1 (min) Source code in autocat/learning/sequential.py 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 def get_overlap_score ( mean : float , std : float , x2 : float = None , x1 : float = None ): \"\"\"Calculate overlap score given targets x2 (max) and x1 (min)\"\"\" if x1 is None and x2 is None : msg = \"Please specify at least either a minimum or maximum target for MLI\" raise SequentialLearnerError ( msg ) if x1 is None : x1 = - np . inf if x2 is None : x2 = np . inf norm_dist = stats . norm ( loc = mean , scale = std ) return norm_dist . cdf ( x2 ) - norm_dist . cdf ( x1 )","title":"get_overlap_score()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.multiple_simulated_sequential_learning_runs","text":"Conducts multiple simulated sequential learning runs Parameters: Name Type Description Default full_design_space DesignSpace Fully labelled DesignSpace to simulate being searched over required predictor: Predictor to be used for predicting properties while iterating. candidate_selector: CandidateSelector that specifies settings for candidate selection. This is where acquisition function, targets, etc. are specified. init_training_size: Size of the initial training set to be selected from the full space. Default: 10 number_of_sl_loops: Integer specifying the number of sequential learning loops to be conducted. This value cannot be greater than (DESIGN_SPACE_SIZE - init_training_size)/batch_size_to_add Default: maximum number of sl loops calculated above number_of_runs: Integer of number of runs to be done Default: 5 number_parallel_jobs: Integer giving the number of cores to be paralellized across using joblib Default: None (ie. will run in serial) write_to_disk: Boolean specifying whether runs history should be written to disk as jsons. Default: False write_location: String with the location where runs history jsons should be written to disk. Default: current directory json_name_prefix: Prefix used when writing out each simulated run as a json The naming convention is {json_name_prefix}_{run #}.json Default: acsl_run Returns: Name Type Description runs_history List [ SequentialLearner ] List of SequentialLearner objects for each simulated run Source code in autocat/learning/sequential.py 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 def multiple_simulated_sequential_learning_runs ( full_design_space : DesignSpace , number_of_runs : int = 5 , number_parallel_jobs : int = None , predictor : Predictor = None , candidate_selector : CandidateSelector = None , init_training_size : int = 10 , number_of_sl_loops : int = None , write_to_disk : bool = False , write_location : str = \".\" , json_name_prefix : str = None , ) -> List [ SequentialLearner ]: \"\"\" Conducts multiple simulated sequential learning runs Parameters ---------- full_design_space: Fully labelled DesignSpace to simulate being searched over predictor: Predictor to be used for predicting properties while iterating. candidate_selector: CandidateSelector that specifies settings for candidate selection. This is where acquisition function, targets, etc. are specified. init_training_size: Size of the initial training set to be selected from the full space. Default: 10 number_of_sl_loops: Integer specifying the number of sequential learning loops to be conducted. This value cannot be greater than `(DESIGN_SPACE_SIZE - init_training_size)/batch_size_to_add` Default: maximum number of sl loops calculated above number_of_runs: Integer of number of runs to be done Default: 5 number_parallel_jobs: Integer giving the number of cores to be paralellized across using `joblib` Default: None (ie. will run in serial) write_to_disk: Boolean specifying whether runs history should be written to disk as jsons. Default: False write_location: String with the location where runs history jsons should be written to disk. Default: current directory json_name_prefix: Prefix used when writing out each simulated run as a json The naming convention is `{json_name_prefix}_{run #}.json` Default: acsl_run Returns ------- runs_history: List of SequentialLearner objects for each simulated run \"\"\" if number_parallel_jobs is not None : runs_history = Parallel ( n_jobs = number_parallel_jobs )( delayed ( simulated_sequential_learning )( full_design_space = full_design_space , predictor = predictor , candidate_selector = candidate_selector , number_of_sl_loops = number_of_sl_loops , init_training_size = init_training_size , ) for i in range ( number_of_runs ) ) else : runs_history = [ simulated_sequential_learning ( full_design_space = full_design_space , predictor = predictor , candidate_selector = candidate_selector , number_of_sl_loops = number_of_sl_loops , init_training_size = init_training_size , ) for i in range ( number_of_runs ) ] # TODO: separate dictionary representation and writing to disk if write_to_disk : if not os . path . isdir ( write_location ): os . makedirs ( write_location ) if json_name_prefix is None : json_name_prefix = \"acsl_run\" for i , run in enumerate ( runs_history ): name = json_name_prefix + \"_\" + str ( i ) + \".json\" run . write_json_to_disk ( write_location = write_location , json_name = name ) print ( f \"SL histories written to { write_location } \" ) return runs_history","title":"multiple_simulated_sequential_learning_runs()"},{"location":"API/Learning/sequential/#autocat.learning.sequential.simulated_sequential_learning","text":"Conducts a simulated sequential learning loop for a fully labelled design space to explore. Parameters: Name Type Description Default full_design_space DesignSpace Fully labelled DesignSpace to simulate being searched over required predictor: Predictor to be used for predicting properties while iterating. candidate_selector: CandidateSelector that specifies settings for candidate selection. This is where acquisition function, targets, etc. are specified. init_training_size: Size of the initial training set to be selected from the full space. Default: 10 number_of_sl_loops: Integer specifying the number of sequential learning loops to be conducted. This value cannot be greater than (DESIGN_SPACE_SIZE - init_training_size)/batch_size_to_add Default: maximum number of sl loops calculated above write_to_disk: Boolean specifying whether the resulting sequential learner should be written to disk as a json. Defaults to False. write_location: String with the location where the resulting sequential learner should be written to disk. Defaults to current directory. Returns: Name Type Description sl SequentialLearner Sequential Learner after having been iterated as specified by the input settings. Contains candidate, prediction, and uncertainty histories for further analysis as desired. Source code in autocat/learning/sequential.py 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 def simulated_sequential_learning ( full_design_space : DesignSpace , predictor : Predictor = None , candidate_selector : CandidateSelector = None , init_training_size : int = 10 , number_of_sl_loops : int = None , write_to_disk : bool = False , write_location : str = \".\" , json_name : str = None , ) -> SequentialLearner : \"\"\" Conducts a simulated sequential learning loop for a fully labelled design space to explore. Parameters ---------- full_design_space: Fully labelled DesignSpace to simulate being searched over predictor: Predictor to be used for predicting properties while iterating. candidate_selector: CandidateSelector that specifies settings for candidate selection. This is where acquisition function, targets, etc. are specified. init_training_size: Size of the initial training set to be selected from the full space. Default: 10 number_of_sl_loops: Integer specifying the number of sequential learning loops to be conducted. This value cannot be greater than `(DESIGN_SPACE_SIZE - init_training_size)/batch_size_to_add` Default: maximum number of sl loops calculated above write_to_disk: Boolean specifying whether the resulting sequential learner should be written to disk as a json. Defaults to False. write_location: String with the location where the resulting sequential learner should be written to disk. Defaults to current directory. Returns ------- sl: Sequential Learner after having been iterated as specified by the input settings. Contains candidate, prediction, and uncertainty histories for further analysis as desired. \"\"\" ds_size = len ( full_design_space ) # check fully explored if True in np . isnan ( full_design_space . design_space_labels ): missing_label_idx = np . where ( np . isnan ( full_design_space . design_space_labels ))[ 0 ] msg = ( f \"Design space must be fully explored.\" f \" Missing labels at indices: { missing_label_idx } \" ) raise SequentialLearnerError ( msg ) # check that specified initial training size makes sense if init_training_size > ds_size : msg = f \"Initial training size ( { init_training_size } ) \\ larger than design space ( { ds_size } )\" raise SequentialLearnerError ( msg ) batch_size_to_add = candidate_selector . num_candidates_to_pick max_num_sl_loops = int ( np . ceil (( ds_size - init_training_size ) / batch_size_to_add )) if number_of_sl_loops is None : number_of_sl_loops = max_num_sl_loops # check that specified number of loops is feasible if number_of_sl_loops > max_num_sl_loops : msg = ( f \"Number of SL loops ( { number_of_sl_loops } ) cannot be greater than\" f \" ( { max_num_sl_loops } )\" ) raise SequentialLearnerError ( msg ) # generate initial training set init_idx = np . zeros ( ds_size , dtype = bool ) init_idx [ np . random . choice ( ds_size , init_training_size , replace = False )] = 1 init_structs = [ full_design_space . design_space_structures [ idx ] for idx , b in enumerate ( init_idx ) if b ] init_labels = full_design_space . design_space_labels . copy () init_labels = init_labels [ np . where ( init_idx )] # default predictor settings if predictor is None : predictor = Predictor () # set up learner that is used for iteration dummy_labels = np . empty ( len ( full_design_space )) dummy_labels [:] = np . nan ds = DesignSpace ( full_design_space . design_space_structures , dummy_labels ) ds . update ( init_structs , init_labels ) sl = SequentialLearner ( design_space = ds , predictor = predictor , candidate_selector = candidate_selector , ) # first iteration on initial dataset sl . iterate () # start simulated sequential learning loop for i in range ( number_of_sl_loops ): print ( f \"Sequential Learning Iteration # { i + 1 } \" ) if sl . candidate_indices is not None : next_structs = sl . candidate_structures next_labels = full_design_space . design_space_labels . take ( sl . candidate_indices ) sl . design_space . update ( next_structs , next_labels ) sl . iterate () if write_to_disk : sl . write_json_to_disk ( write_location = write_location , json_name = json_name ) print ( f \"SL dictionary written to { write_location } \" ) return sl","title":"simulated_sequential_learning()"},{"location":"API/Structure_Generation/adsorption/","text":"generate_adsorbed_structures ( surface = None , adsorbates = None , adsorption_sites = None , use_all_sites = None , site_types = None , heights = None , anchor_atom_indices = None , rotations = None , write_to_disk = False , write_location = '.' , dirs_exist_ok = False ) Builds structures for reaction intermediates by placing the adsorbate moeity on the input surface at specified positions. Parameters: Name Type Description Default surface Union [ str , Atoms ] Atoms object or path to a file on disk containing the structure of the host surface. Note that the format of the file must be readable with ase.io . None adsorbates (REQUIRED): Dictionary of adsorbate molecule/intermediate names and corresponding ase.Atoms object or string to be placed on the host surface. Note that the strings that appear as values must be in the list of supported molecules in `autocat.data.intermediates` or in the `ase` g2 database. Predefined data in `autocat.data` will take priority over that in `ase`. Alternatively, a list of strings can be provided as input. Note that each string has to be *unique* and available in `autocat.data.intermediates` or the `ase` g2 database. Example: { \"NH\": \"NH\", \"N*\": \"N\", \"NNH\": NNH_atoms_obj, ... } OR [\"NH\", \"NNH\"] rotations: Dictionary of the list of rotation operations to be applied to each adsorbate molecule/intermediate before being placed on the host surface. Alternatively, a single list of rotation operations can be provided as input to be used for all adsorbates. Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Example: { \"NH\": [(90.0, \"z\"), (45.0, \"y\")], \"NNH\": ... } Defaults to [(0, \"x\")] (i.e., no rotations applied) for each adsorbate molecule. adsorption_sites: Dictionary with labels + list of xy coordinates of sites on the surface where each adsorbate must be placed. Alternatively, a single dictionary with label and a list of xy coordinates can be provided as input to be used for all adsorbates. Example: { \"NH\": { \"my_awesome_site_1\": [(0.0, 0.0), (0.25, 0.25)], \"my_awesome_site_2\": [(0.0, 1.5)], ... }, \"NNH\": ... } OR { \"my_default_site\": [(0.5, 0.5)], } Defaults to {\"origin\": [(0, 0)]} for all adsorbates. use_all_sites: Dictionary specifying if all symmetrically unique sites on the surface must be used for placing each adsorbate. Will generate a number of adsorbed structures equal to the number of symmetrically unique sites identified using the AdsorbateSiteFinder module from pymatgen . Alternatively, a single Boolean value can be provided as input to be used for all adsorbates. NB: If True, overrides any sites defined in `adsorption_sites` for each adsorbate. Defaults to False for all adsorbates. site_types: Dictionary of the types of adsorption sites to be searched for for each adsorbate. Options are \"ontop\", \"bridge\", and \"hollow\". Alternatively, a single list of adsorption sites can be provided as input to be used for all adsorbates. Ignored if use_all_sites is False. Defaults to [\"ontop\", \"bridge\", \"hollow\"] (if `use_all_sites` is True) for all adsorbates. heights: Dictionary of the height above surface where each adsorbate should be placed. Alternatively, a single float value can be provided as input to be used for all adsorbates. If None, will estimate initial height based on covalent radii of the nearest neighbor atoms for each adsorbate. anchor_atom_indices: Dictionary of the integer index of the atom in each adsorbate molecule that should be used as anchor when placing it on the surface. Alternatively, a single integer index can be provided as input to be used for all adsorbates. Defaults to the atom at index 0 for each adsorbate molecule. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: adsorbates/[adsorbate_1]/[site_label_1]/[xy_site_coord_1]/input.traj adsorbates/[adsorbate_1]/[site_label_1]/[xy_site_coord_2]/input.traj ... adsorbates/[adsorbate_1]/[site_label_2]/[xy_site_coord_1]/input.traj ... adsorbates/[adsorbate_2]/[site_label_1]/[xy_site_coord_1]/input.traj ... dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the os.makedirs builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns: Name Type Description ads_structures Dict [ str , Dict [ str , Dict [ str , Dict [ str , Any ]]]] Dictionary containing all of the reaction structures (with the input molecule/intermediate adsorbed onto the surface) as ase.Atoms object for all adsorbates and adsorption sites. Example: { \"NH\": { \"ontop\": { \"5.345_2.342\": { \"structure\": NH_on_surface_obj, \"traj_file_path\": \"/path/to/NH/adsorbed/on/surface/traj/file\" }, \"1.478_1.230\": { ... }, }, \"hollow\": { ... }, ... \"NNH\": { ... } } Source code in autocat/adsorption.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 def generate_adsorbed_structures ( surface : Union [ str , Atoms ] = None , adsorbates : Union [ Dict [ str , Union [ str , Atoms ]], Sequence [ str ]] = None , adsorption_sites : Union [ Dict [ str , AdsorptionSite ], AdsorptionSite ] = None , use_all_sites : Union [ Dict [ str , bool ], bool ] = None , site_types : Union [ Dict [ str , Sequence [ str ]], Sequence [ str ]] = None , heights : Union [ Dict [ str , float ], float ] = None , anchor_atom_indices : Union [ Dict [ str , int ], int ] = None , rotations : Union [ Dict [ str , RotationOperations ], RotationOperations ] = None , write_to_disk : bool = False , write_location : str = \".\" , dirs_exist_ok : bool = False , ) -> Dict [ str , Dict [ str , Dict [ str , Dict [ str , Any ]]]]: \"\"\" Builds structures for reaction intermediates by placing the adsorbate moeity on the input surface at specified positions. Parameters ---------- surface (REQUIRED): Atoms object or path to a file on disk containing the structure of the host surface. Note that the format of the file must be readable with `ase.io`. adsorbates (REQUIRED): Dictionary of adsorbate molecule/intermediate names and corresponding `ase.Atoms` object or string to be placed on the host surface. Note that the strings that appear as values must be in the list of supported molecules in `autocat.data.intermediates` or in the `ase` g2 database. Predefined data in `autocat.data` will take priority over that in `ase`. Alternatively, a list of strings can be provided as input. Note that each string has to be *unique* and available in `autocat.data.intermediates` or the `ase` g2 database. Example: { \"NH\": \"NH\", \"N*\": \"N\", \"NNH\": NNH_atoms_obj, ... } OR [\"NH\", \"NNH\"] rotations: Dictionary of the list of rotation operations to be applied to each adsorbate molecule/intermediate before being placed on the host surface. Alternatively, a single list of rotation operations can be provided as input to be used for all adsorbates. Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Example: { \"NH\": [(90.0, \"z\"), (45.0, \"y\")], \"NNH\": ... } Defaults to [(0, \"x\")] (i.e., no rotations applied) for each adsorbate molecule. adsorption_sites: Dictionary with labels + list of xy coordinates of sites on the surface where each adsorbate must be placed. Alternatively, a single dictionary with label and a list of xy coordinates can be provided as input to be used for all adsorbates. Example: { \"NH\": { \"my_awesome_site_1\": [(0.0, 0.0), (0.25, 0.25)], \"my_awesome_site_2\": [(0.0, 1.5)], ... }, \"NNH\": ... } OR { \"my_default_site\": [(0.5, 0.5)], } Defaults to {\"origin\": [(0, 0)]} for all adsorbates. use_all_sites: Dictionary specifying if all symmetrically unique sites on the surface must be used for placing each adsorbate. Will generate a number of adsorbed structures equal to the number of symmetrically unique sites identified using the `AdsorbateSiteFinder` module from `pymatgen`. Alternatively, a single Boolean value can be provided as input to be used for all adsorbates. NB: If True, overrides any sites defined in `adsorption_sites` for each adsorbate. Defaults to False for all adsorbates. site_types: Dictionary of the types of adsorption sites to be searched for for each adsorbate. Options are \"ontop\", \"bridge\", and \"hollow\". Alternatively, a single list of adsorption sites can be provided as input to be used for all adsorbates. Ignored if `use_all_sites` is False. Defaults to [\"ontop\", \"bridge\", \"hollow\"] (if `use_all_sites` is True) for all adsorbates. heights: Dictionary of the height above surface where each adsorbate should be placed. Alternatively, a single float value can be provided as input to be used for all adsorbates. If None, will estimate initial height based on covalent radii of the nearest neighbor atoms for each adsorbate. anchor_atom_indices: Dictionary of the integer index of the atom in each adsorbate molecule that should be used as anchor when placing it on the surface. Alternatively, a single integer index can be provided as input to be used for all adsorbates. Defaults to the atom at index 0 for each adsorbate molecule. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: adsorbates/[adsorbate_1]/[site_label_1]/[xy_site_coord_1]/input.traj adsorbates/[adsorbate_1]/[site_label_1]/[xy_site_coord_2]/input.traj ... adsorbates/[adsorbate_1]/[site_label_2]/[xy_site_coord_1]/input.traj ... adsorbates/[adsorbate_2]/[site_label_1]/[xy_site_coord_1]/input.traj ... dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the `os.makedirs` builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns ------- ads_structures: Dictionary containing all of the reaction structures (with the input molecule/intermediate adsorbed onto the surface) as `ase.Atoms` object for all adsorbates and adsorption sites. Example: { \"NH\": { \"ontop\": { \"5.345_2.342\": { \"structure\": NH_on_surface_obj, \"traj_file_path\": \"/path/to/NH/adsorbed/on/surface/traj/file\" }, \"1.478_1.230\": { ... }, }, \"hollow\": { ... }, ... \"NNH\": { ... } } \"\"\" if surface is None : msg = \"Surface structure must be specified\" raise AutocatAdsorptionGenerationError ( msg ) elif not isinstance ( surface , Atoms ): msg = f \"Unrecognized input type for surface ( { type ( surface ) } )\" raise AutocatAdsorptionGenerationError ( msg ) # Input wrangling for the different types of parameter values allowed for # the same function arguments. if adsorbates is None or not adsorbates : msg = \"Adsorbate molecules/intermediates must be specified\" raise AutocatAdsorptionGenerationError ( msg ) elif isinstance ( adsorbates , ( list , tuple )): adsorbates = { ads_key : ads_key for ads_key in adsorbates } elif not isinstance ( adsorbates , dict ): msg = f \"Unrecognized input type for adsorbates ( { type ( adsorbates ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if rotations is None : rotations = {} elif isinstance ( rotations , ( list , tuple )): rotations = { ads_key : rotations for ads_key in adsorbates } elif not isinstance ( rotations , dict ): msg = f \"Unrecognized input type for rotations ( { type ( rotations ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if adsorption_sites is None : adsorption_sites = {} elif isinstance ( adsorption_sites , dict ): # check if the input is a single site for all adsorbates vs separate # sites for each adsorbate if all ([ ads_key not in adsorption_sites for ads_key in adsorbates ]): adsorption_sites = { ads_key : adsorption_sites for ads_key in adsorbates } elif not isinstance ( adsorption_sites , dict ): msg = f \"Unrecognized input type for adsorption_sites ( { type ( adsorption_sites ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if use_all_sites is None : use_all_sites = {} elif isinstance ( use_all_sites , bool ): use_all_sites = { ads_key : use_all_sites for ads_key in adsorbates } elif not isinstance ( use_all_sites , dict ): msg = f \"Unrecognized input type for use_all_sites ( { type ( use_all_sites ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if site_types is None : site_types = {} elif isinstance ( site_types , ( list , tuple )): site_types = { ads_key : site_types for ads_key in adsorbates } elif not isinstance ( site_types , dict ): msg = f \"Unrecognized input type for site_types ( { type ( site_types ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if heights is None : heights = {} elif isinstance ( heights , float ): heights = { ads_key : heights for ads_key in adsorbates } elif not isinstance ( heights , dict ): msg = f \"Unrecognized input type for heights ( { type ( heights ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if anchor_atom_indices is None : anchor_atom_indices = {} elif isinstance ( anchor_atom_indices , int ): anchor_atom_indices = { ads_key : anchor_atom_indices for ads_key in adsorbates } elif not isinstance ( anchor_atom_indices , dict ): msg = f \"Unrecognized input type for anchor_atom_indices ( { type ( anchor_atom_indices ) } )\" raise AutocatAdsorptionGenerationError ( msg ) ads_structures = {} for ads_key in adsorbates : ads_structures [ ads_key ] = {} # generate the molecule if not already input adsorbate = adsorbates . get ( ads_key ) if isinstance ( adsorbate , str ): adsorbate = generate_molecule ( molecule_name = adsorbate )[ \"structure\" ] # get all adsorption sites for the adsorbate ads_adsorption_sites = adsorption_sites . get ( ads_key , { \"origin\" : [( 0 , 0 )]}) ads_use_all_sites = use_all_sites . get ( ads_key , False ) if ads_use_all_sites : ads_site_types = site_types . get ( ads_key , [ \"ontop\" , \"bridge\" , \"hollow\" ]) ads_adsorption_sites = get_adsorption_sites ( surface , site_types = ads_site_types ) # for each adsorption site type, for each (xy) coordinate, generate the # adsorbated (surface + adsorbate) structure for site in ads_adsorption_sites : ads_structures [ ads_key ][ site ] = {} for coords in ads_adsorption_sites [ site ]: # use only xy coords and ignore z if given here (handled by ads_height) coords = coords [: 2 ] rcoords = np . around ( coords , 3 ) scoords = f \" { str ( rcoords [ 0 ]) } _ { str ( rcoords [ 1 ]) } \" ads_height = heights . get ( ads_key , None ) ads_anchor_atom_index = anchor_atom_indices . get ( ads_key , 0 ) ads_rotations = rotations . get ( ads_key , [( 0 , \"x\" )]) adsorbed_structure = place_adsorbate ( surface = surface , adsorbate = adsorbate , adsorption_site = coords , anchor_atom_index = ads_anchor_atom_index , height = ads_height , rotations = ads_rotations , ) traj_file_path = None if write_to_disk : dir_path = os . path . join ( write_location , \"adsorbates\" , ads_key , site , scoords ) os . makedirs ( dir_path , exist_ok = dirs_exist_ok ) traj_file_path = os . path . join ( dir_path , \"input.traj\" ) adsorbed_structure . write ( traj_file_path ) print ( f \"Structure with { ads_key } adsorbed at { site } / { scoords } \" f \" written to { traj_file_path } \" ) ads_structures [ ads_key ][ site ] . update ( { scoords : { \"structure\" : adsorbed_structure , \"traj_file_path\" : traj_file_path , } } ) return ads_structures generate_molecule ( molecule_name = None , rotations = None , cell = None , write_to_disk = False , write_location = '.' , dirs_exist_ok = False ) Generates an ase.Atoms object of an isolated molecule. If specified, can write out a .traj file containing the isolated molecule in a box. Parameters: Name Type Description Default molecule_name str String of the name of the molecule to be generated. Will search in autocat.intermediates data first and then ase g2 database. None rotations: List of rotation operations to be applied to the adsorbate molecule/intermediate before being placed on the host surface. Example: Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Defaults to [(0, \"x\")] (i.e., no rotations applied). cell: List of float specifying the dimensions of the box to place the molecule in, in Angstrom. Defaults to [15, 15, 15]. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the molecule structure must be written. The molecule is written to disk at [write_location]/references/[molecule_name]/input.traj dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the os.makedirs builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns: Type Description Dictionary containing Atoms object of the generated molecule and path to .traj file if written to disk. Example: { \"NH\": {\"structure\": NH_ase_obj, \"traj_file_path\": \"/path/to/NH/traj/file\"}, } Source code in autocat/adsorption.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def generate_molecule ( molecule_name : str = None , rotations : RotationOperations = None , cell : Sequence [ float ] = None , write_to_disk : bool = False , write_location : str = \".\" , dirs_exist_ok : bool = False , ) -> Dict [ str , Dict [ str , Any ]]: \"\"\" Generates an `ase.Atoms` object of an isolated molecule. If specified, can write out a .traj file containing the isolated molecule in a box. Parameters ---------- molecule_name (REQUIRED): String of the name of the molecule to be generated. Will search in `autocat.intermediates` data first and then `ase` g2 database. rotations: List of rotation operations to be applied to the adsorbate molecule/intermediate before being placed on the host surface. Example: Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Defaults to [(0, \"x\")] (i.e., no rotations applied). cell: List of float specifying the dimensions of the box to place the molecule in, in Angstrom. Defaults to [15, 15, 15]. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the molecule structure must be written. The molecule is written to disk at [write_location]/references/[molecule_name]/input.traj dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the `os.makedirs` builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns ------- Dictionary containing Atoms object of the generated molecule and path to .traj file if written to disk. Example: { \"NH\": {\"structure\": NH_ase_obj, \"traj_file_path\": \"/path/to/NH/traj/file\"}, } \"\"\" if molecule_name is None : msg = \"Molecule name must be specified\" raise AutocatAdsorptionGenerationError ( msg ) if rotations is None : rotations = [[ 0.0 , \"x\" ]] if cell is None : cell = [ 15 , 15 , 15 ] m = None if molecule_name in NRR_MOLS : m = NRR_MOLS [ molecule_name ] . copy () elif molecule_name in ORR_MOLS : m = ORR_MOLS [ molecule_name ] . copy () elif molecule_name in chemical_symbols : # atom-in-a-box m = Atoms ( molecule_name ) elif molecule_name in g2 . names : m = build_molecule ( molecule_name ) if m is None : msg = f \"Unable to construct molecule { molecule_name } \" raise NotImplementedError ( msg ) for r in rotations : m . rotate ( r [ 0 ], r [ 1 ]) m . cell = cell m . center () traj_file_path = None if write_to_disk : dir_path = os . path . join ( write_location , \"references\" , f \" { molecule_name } \" ) os . makedirs ( dir_path , exist_ok = dirs_exist_ok ) traj_file_path = os . path . join ( dir_path , \"input.traj\" ) m . write ( traj_file_path ) print ( f \" { molecule_name } molecule structure written to { traj_file_path } \" ) return { \"structure\" : m , \"traj_file_path\" : traj_file_path } get_adsorbate_height_estimate ( surface = None , adsorbate = None , adsorption_site = None , anchor_atom_index = 0 , scale = 1.0 ) Guess an initial height for the adsorbate to be placed on the surface, by summing covalent radii of the nearest neighbor atoms. Parameters: Name Type Description Default surface Atoms ase.Atoms object the surface for which adsorbate height must be estimated. None adsorbate (REQUIRED): ase.Atoms object of adsorbate to be placed on the surface. adsorption_site: Tuple or list of the xy cartesian coordinates for where the adsorbate would be placed. Defaults to [0, 0]. anchor_atom_index: Integer index of the atom in the adsorbate molecule that will be used as anchor when placing it on the surface. Defaults to the atom at index 0. scale: Float giving a scale factor to be applied to the calculated bond length. For example, scale = 1.1 -> bond length = 1.1 * (covalent_radius_1 + covalent_radius_2) Defaults to 1.0 (i.e., no scaling). Returns: Name Type Description height_estimate float Float with the estimated initial height for the adsorbate molecule from the surface. Returns a default of 1.5 Angstom if the guessing process using nearest-neighbor covalent radii fails. Source code in autocat/adsorption.py 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 def get_adsorbate_height_estimate ( surface : Atoms = None , adsorbate : Atoms = None , adsorption_site : Sequence [ float ] = None , anchor_atom_index : int = 0 , scale : float = 1.0 , ) -> float : \"\"\" Guess an initial height for the adsorbate to be placed on the surface, by summing covalent radii of the nearest neighbor atoms. Parameters ---------- surface (REQUIRED): `ase.Atoms` object the surface for which adsorbate height must be estimated. adsorbate (REQUIRED): `ase.Atoms` object of adsorbate to be placed on the surface. adsorption_site: Tuple or list of the xy cartesian coordinates for where the adsorbate would be placed. Defaults to [0, 0]. anchor_atom_index: Integer index of the atom in the adsorbate molecule that will be used as anchor when placing it on the surface. Defaults to the atom at index 0. scale: Float giving a scale factor to be applied to the calculated bond length. For example, scale = 1.1 -> bond length = 1.1 * (covalent_radius_1 + covalent_radius_2) Defaults to 1.0 (i.e., no scaling). Returns ------- height_estimate: Float with the estimated initial height for the adsorbate molecule from the surface. Returns a default of 1.5 Angstom if the guessing process using nearest-neighbor covalent radii fails. \"\"\" if adsorption_site is None : adsorption_site = ( 0 , 0 ) species , coords = get_adsorbate_slab_nn_list ( surface = surface , adsorption_site = adsorption_site ) ads_atom_radius = covalent_radii [ atomic_numbers [ adsorbate [ anchor_atom_index ] . symbol ] ] guessed_heights = [] for sp , coord in zip ( species , coords ): nn_radius = covalent_radii [ atomic_numbers [ sp ]] r_dist = scale * ( ads_atom_radius + nn_radius ) position_array = np . array ( adsorption_site ) if ( r_dist ** 2 - np . sum (( position_array - coord [: 2 ]) ** 2 )) >= 0.0 : height = np . sqrt ( r_dist ** 2 - np . sum (( position_array - coord [: 2 ]) ** 2 )) else : height = np . nan guessed_heights . append ( height ) if np . isnan ( np . nanmean ( guessed_heights )): height_estimate = 1.5 else : height_estimate = np . nanmean ( guessed_heights ) return height_estimate get_adsorbate_slab_nn_list ( surface = None , adsorption_site = None , height = 0.5 ) Get list of nearest neighbors for the adsorbate on the surface at the specified position. Parameters: Name Type Description Default surface Atoms Atoms object the surface for which nearest neighbors of the adsorbate should be identified. None adsorption_site: Tuple or list of the xy cartesian coordinates for where the adsorbate would be placed. Defaults to [0, 0]. height: Float with the height of the adsorbate molecule from the surface in Angstrom. Returns: Type Description species_list , coordinates_list Two lists of length equal to the number of nearest neighbors identified: first with the list of species names, and second with the list of coordinates. Example: ([\"Fe\", \"Sr\"], [[0.4, 0.6], [0.2, 0.9]]) Source code in autocat/adsorption.py 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 def get_adsorbate_slab_nn_list ( surface : Atoms = None , adsorption_site : Sequence [ float ] = None , height : float = 0.5 ) -> Tuple [ List [ str ], List [ List [ float ]]]: \"\"\" Get list of nearest neighbors for the adsorbate on the surface at the specified position. Parameters ---------- surface (REQUIRED): Atoms object the surface for which nearest neighbors of the adsorbate should be identified. adsorption_site: Tuple or list of the xy cartesian coordinates for where the adsorbate would be placed. Defaults to [0, 0]. height: Float with the height of the adsorbate molecule from the surface in Angstrom. Returns ------- species_list, coordinates_list: Two lists of length equal to the number of nearest neighbors identified: first with the list of species names, and second with the list of coordinates. Example: ([\"Fe\", \"Sr\"], [[0.4, 0.6], [0.2, 0.9]]) \"\"\" if adsorption_site is None : adsorption_site = ( 0 , 0 ) surf = surface . copy () conv = AseAtomsAdaptor () add_adsorbate ( surf , \"X\" , height = height , position = adsorption_site ) init_guess = conv . get_structure ( surf ) nn = get_neighbors_of_site_with_index ( init_guess , - 1 ) species_list = [ _nn . species . hill_formula for _nn in nn ] coord_list = [ _nn . coords for _nn in nn ] return species_list , coord_list get_adsorption_sites ( surface = None , site_types = None , ** kwargs ) For the given surface, returns all symmetrically unique sites of the specified type. Uses pymatgen.analysis.adsorption module to identify the sites. Parameters: Name Type Description Default surface Atoms Atoms object of the host surface for which the symmetrically unique surface sites should be identified. None site_types: List of types of adsorption sites to be searched for. Options are \"ontop\", \"bridge\", and \"hollow\". Defaults to [\"ontop\", \"bridge\", \"hollow\"]. **kwargs: Other miscellaneous keyword arguments. Will be passed on to AdsorbateSiteFinder.find_adsorption_sites in pymatgen . Returns: Name Type Description sites Dict [ str , Sequence [ float ]] Dictionary containing the reference structures with the identified sites for each desired site type Source code in autocat/adsorption.py 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 def get_adsorption_sites ( surface : Atoms = None , site_types : Sequence [ str ] = None , ** kwargs ) -> Dict [ str , Sequence [ float ]]: \"\"\" For the given surface, returns all symmetrically unique sites of the specified type. Uses `pymatgen.analysis.adsorption` module to identify the sites. Parameters ---------- surface (REQUIRED): Atoms object of the host surface for which the symmetrically unique surface sites should be identified. site_types: List of types of adsorption sites to be searched for. Options are \"ontop\", \"bridge\", and \"hollow\". Defaults to [\"ontop\", \"bridge\", \"hollow\"]. **kwargs: Other miscellaneous keyword arguments. Will be passed on to `AdsorbateSiteFinder.find_adsorption_sites` in `pymatgen`. Returns ------- sites: Dictionary containing the reference structures with the identified sites for each desired site type \"\"\" if site_types is None : site_types = [ \"ontop\" , \"bridge\" , \"hollow\" ] converter = AseAtomsAdaptor () pmg_structure = converter . get_structure ( surface ) finder = AdsorbateSiteFinder ( pmg_structure ) sites = finder . find_adsorption_sites ( positions = site_types , ** kwargs ) # Remove the extraneous \"all\" returned by default from pymatgen sites . pop ( \"all\" , None ) return sites place_adsorbate ( surface = None , adsorbate = None , adsorption_site = None , rotations = None , anchor_atom_index = 0 , height = None ) Places an adsorbate molecule/intermediate onto a given surface at the specified location. Parameters: Name Type Description Default surface Atoms Atoms object of the host surface. None adsorbate (REQUIRED): Atoms object of the adsorbate molecule/intermediate to be placed on the host surface. adsorption_site: Tuple or list of the xy cartesian coordinates on the surface where the adsorbate must be placed. Defaults to [0, 0]. rotations: List of rotation operations to be applied to the adsorbate molecule/intermediate before being placed on the host surface. Example: Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Defaults to [(0, \"x\")] (i.e., no rotations applied). anchor_atom_index: Integer index of the atom in the adsorbate molecule that will be used as anchor when placing it on the surface. Defaults to the atom at index 0. height: Float specifying the height above surface where adsorbate should be placed. If None, will estimate initial height based on covalent radii of the nearest neighbor atoms. Returns: Name Type Description surface Atoms Atoms object of the surface with the adsorbate molecule/intermediate placed on it as specified. Source code in autocat/adsorption.py 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 def place_adsorbate ( surface : Atoms = None , adsorbate : Atoms = None , adsorption_site : Sequence [ float ] = None , rotations : RotationOperations = None , anchor_atom_index : int = 0 , height : float = None , ) -> Atoms : \"\"\" Places an adsorbate molecule/intermediate onto a given surface at the specified location. Parameters ---------- surface (REQUIRED): Atoms object of the host surface. adsorbate (REQUIRED): Atoms object of the adsorbate molecule/intermediate to be placed on the host surface. adsorption_site: Tuple or list of the xy cartesian coordinates on the surface where the adsorbate must be placed. Defaults to [0, 0]. rotations: List of rotation operations to be applied to the adsorbate molecule/intermediate before being placed on the host surface. Example: Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Defaults to [(0, \"x\")] (i.e., no rotations applied). anchor_atom_index: Integer index of the atom in the adsorbate molecule that will be used as anchor when placing it on the surface. Defaults to the atom at index 0. height: Float specifying the height above surface where adsorbate should be placed. If None, will estimate initial height based on covalent radii of the nearest neighbor atoms. Returns ------- surface: Atoms object of the surface with the adsorbate molecule/intermediate placed on it as specified. \"\"\" if adsorption_site is None : adsorption_site = [ 0 , 0 ] if rotations is None : rotations = [( 0.0 , \"x\" )] _surface = surface . copy () _adsorbate = adsorbate . copy () for r in rotations : _adsorbate . rotate ( r [ 0 ], r [ 1 ]) if height is None : height = get_adsorbate_height_estimate ( _surface , _adsorbate , adsorption_site = adsorption_site , anchor_atom_index = anchor_atom_index , ) add_adsorbate ( _surface , _adsorbate , height , position = adsorption_site , mol_index = anchor_atom_index , ) return _surface","title":"autocat.adsorption"},{"location":"API/Structure_Generation/adsorption/#autocat.adsorption.generate_adsorbed_structures","text":"Builds structures for reaction intermediates by placing the adsorbate moeity on the input surface at specified positions. Parameters: Name Type Description Default surface Union [ str , Atoms ] Atoms object or path to a file on disk containing the structure of the host surface. Note that the format of the file must be readable with ase.io . None adsorbates (REQUIRED): Dictionary of adsorbate molecule/intermediate names and corresponding ase.Atoms object or string to be placed on the host surface. Note that the strings that appear as values must be in the list of supported molecules in `autocat.data.intermediates` or in the `ase` g2 database. Predefined data in `autocat.data` will take priority over that in `ase`. Alternatively, a list of strings can be provided as input. Note that each string has to be *unique* and available in `autocat.data.intermediates` or the `ase` g2 database. Example: { \"NH\": \"NH\", \"N*\": \"N\", \"NNH\": NNH_atoms_obj, ... } OR [\"NH\", \"NNH\"] rotations: Dictionary of the list of rotation operations to be applied to each adsorbate molecule/intermediate before being placed on the host surface. Alternatively, a single list of rotation operations can be provided as input to be used for all adsorbates. Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Example: { \"NH\": [(90.0, \"z\"), (45.0, \"y\")], \"NNH\": ... } Defaults to [(0, \"x\")] (i.e., no rotations applied) for each adsorbate molecule. adsorption_sites: Dictionary with labels + list of xy coordinates of sites on the surface where each adsorbate must be placed. Alternatively, a single dictionary with label and a list of xy coordinates can be provided as input to be used for all adsorbates. Example: { \"NH\": { \"my_awesome_site_1\": [(0.0, 0.0), (0.25, 0.25)], \"my_awesome_site_2\": [(0.0, 1.5)], ... }, \"NNH\": ... } OR { \"my_default_site\": [(0.5, 0.5)], } Defaults to {\"origin\": [(0, 0)]} for all adsorbates. use_all_sites: Dictionary specifying if all symmetrically unique sites on the surface must be used for placing each adsorbate. Will generate a number of adsorbed structures equal to the number of symmetrically unique sites identified using the AdsorbateSiteFinder module from pymatgen . Alternatively, a single Boolean value can be provided as input to be used for all adsorbates. NB: If True, overrides any sites defined in `adsorption_sites` for each adsorbate. Defaults to False for all adsorbates. site_types: Dictionary of the types of adsorption sites to be searched for for each adsorbate. Options are \"ontop\", \"bridge\", and \"hollow\". Alternatively, a single list of adsorption sites can be provided as input to be used for all adsorbates. Ignored if use_all_sites is False. Defaults to [\"ontop\", \"bridge\", \"hollow\"] (if `use_all_sites` is True) for all adsorbates. heights: Dictionary of the height above surface where each adsorbate should be placed. Alternatively, a single float value can be provided as input to be used for all adsorbates. If None, will estimate initial height based on covalent radii of the nearest neighbor atoms for each adsorbate. anchor_atom_indices: Dictionary of the integer index of the atom in each adsorbate molecule that should be used as anchor when placing it on the surface. Alternatively, a single integer index can be provided as input to be used for all adsorbates. Defaults to the atom at index 0 for each adsorbate molecule. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: adsorbates/[adsorbate_1]/[site_label_1]/[xy_site_coord_1]/input.traj adsorbates/[adsorbate_1]/[site_label_1]/[xy_site_coord_2]/input.traj ... adsorbates/[adsorbate_1]/[site_label_2]/[xy_site_coord_1]/input.traj ... adsorbates/[adsorbate_2]/[site_label_1]/[xy_site_coord_1]/input.traj ... dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the os.makedirs builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns: Name Type Description ads_structures Dict [ str , Dict [ str , Dict [ str , Dict [ str , Any ]]]] Dictionary containing all of the reaction structures (with the input molecule/intermediate adsorbed onto the surface) as ase.Atoms object for all adsorbates and adsorption sites. Example: { \"NH\": { \"ontop\": { \"5.345_2.342\": { \"structure\": NH_on_surface_obj, \"traj_file_path\": \"/path/to/NH/adsorbed/on/surface/traj/file\" }, \"1.478_1.230\": { ... }, }, \"hollow\": { ... }, ... \"NNH\": { ... } } Source code in autocat/adsorption.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 def generate_adsorbed_structures ( surface : Union [ str , Atoms ] = None , adsorbates : Union [ Dict [ str , Union [ str , Atoms ]], Sequence [ str ]] = None , adsorption_sites : Union [ Dict [ str , AdsorptionSite ], AdsorptionSite ] = None , use_all_sites : Union [ Dict [ str , bool ], bool ] = None , site_types : Union [ Dict [ str , Sequence [ str ]], Sequence [ str ]] = None , heights : Union [ Dict [ str , float ], float ] = None , anchor_atom_indices : Union [ Dict [ str , int ], int ] = None , rotations : Union [ Dict [ str , RotationOperations ], RotationOperations ] = None , write_to_disk : bool = False , write_location : str = \".\" , dirs_exist_ok : bool = False , ) -> Dict [ str , Dict [ str , Dict [ str , Dict [ str , Any ]]]]: \"\"\" Builds structures for reaction intermediates by placing the adsorbate moeity on the input surface at specified positions. Parameters ---------- surface (REQUIRED): Atoms object or path to a file on disk containing the structure of the host surface. Note that the format of the file must be readable with `ase.io`. adsorbates (REQUIRED): Dictionary of adsorbate molecule/intermediate names and corresponding `ase.Atoms` object or string to be placed on the host surface. Note that the strings that appear as values must be in the list of supported molecules in `autocat.data.intermediates` or in the `ase` g2 database. Predefined data in `autocat.data` will take priority over that in `ase`. Alternatively, a list of strings can be provided as input. Note that each string has to be *unique* and available in `autocat.data.intermediates` or the `ase` g2 database. Example: { \"NH\": \"NH\", \"N*\": \"N\", \"NNH\": NNH_atoms_obj, ... } OR [\"NH\", \"NNH\"] rotations: Dictionary of the list of rotation operations to be applied to each adsorbate molecule/intermediate before being placed on the host surface. Alternatively, a single list of rotation operations can be provided as input to be used for all adsorbates. Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Example: { \"NH\": [(90.0, \"z\"), (45.0, \"y\")], \"NNH\": ... } Defaults to [(0, \"x\")] (i.e., no rotations applied) for each adsorbate molecule. adsorption_sites: Dictionary with labels + list of xy coordinates of sites on the surface where each adsorbate must be placed. Alternatively, a single dictionary with label and a list of xy coordinates can be provided as input to be used for all adsorbates. Example: { \"NH\": { \"my_awesome_site_1\": [(0.0, 0.0), (0.25, 0.25)], \"my_awesome_site_2\": [(0.0, 1.5)], ... }, \"NNH\": ... } OR { \"my_default_site\": [(0.5, 0.5)], } Defaults to {\"origin\": [(0, 0)]} for all adsorbates. use_all_sites: Dictionary specifying if all symmetrically unique sites on the surface must be used for placing each adsorbate. Will generate a number of adsorbed structures equal to the number of symmetrically unique sites identified using the `AdsorbateSiteFinder` module from `pymatgen`. Alternatively, a single Boolean value can be provided as input to be used for all adsorbates. NB: If True, overrides any sites defined in `adsorption_sites` for each adsorbate. Defaults to False for all adsorbates. site_types: Dictionary of the types of adsorption sites to be searched for for each adsorbate. Options are \"ontop\", \"bridge\", and \"hollow\". Alternatively, a single list of adsorption sites can be provided as input to be used for all adsorbates. Ignored if `use_all_sites` is False. Defaults to [\"ontop\", \"bridge\", \"hollow\"] (if `use_all_sites` is True) for all adsorbates. heights: Dictionary of the height above surface where each adsorbate should be placed. Alternatively, a single float value can be provided as input to be used for all adsorbates. If None, will estimate initial height based on covalent radii of the nearest neighbor atoms for each adsorbate. anchor_atom_indices: Dictionary of the integer index of the atom in each adsorbate molecule that should be used as anchor when placing it on the surface. Alternatively, a single integer index can be provided as input to be used for all adsorbates. Defaults to the atom at index 0 for each adsorbate molecule. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: adsorbates/[adsorbate_1]/[site_label_1]/[xy_site_coord_1]/input.traj adsorbates/[adsorbate_1]/[site_label_1]/[xy_site_coord_2]/input.traj ... adsorbates/[adsorbate_1]/[site_label_2]/[xy_site_coord_1]/input.traj ... adsorbates/[adsorbate_2]/[site_label_1]/[xy_site_coord_1]/input.traj ... dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the `os.makedirs` builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns ------- ads_structures: Dictionary containing all of the reaction structures (with the input molecule/intermediate adsorbed onto the surface) as `ase.Atoms` object for all adsorbates and adsorption sites. Example: { \"NH\": { \"ontop\": { \"5.345_2.342\": { \"structure\": NH_on_surface_obj, \"traj_file_path\": \"/path/to/NH/adsorbed/on/surface/traj/file\" }, \"1.478_1.230\": { ... }, }, \"hollow\": { ... }, ... \"NNH\": { ... } } \"\"\" if surface is None : msg = \"Surface structure must be specified\" raise AutocatAdsorptionGenerationError ( msg ) elif not isinstance ( surface , Atoms ): msg = f \"Unrecognized input type for surface ( { type ( surface ) } )\" raise AutocatAdsorptionGenerationError ( msg ) # Input wrangling for the different types of parameter values allowed for # the same function arguments. if adsorbates is None or not adsorbates : msg = \"Adsorbate molecules/intermediates must be specified\" raise AutocatAdsorptionGenerationError ( msg ) elif isinstance ( adsorbates , ( list , tuple )): adsorbates = { ads_key : ads_key for ads_key in adsorbates } elif not isinstance ( adsorbates , dict ): msg = f \"Unrecognized input type for adsorbates ( { type ( adsorbates ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if rotations is None : rotations = {} elif isinstance ( rotations , ( list , tuple )): rotations = { ads_key : rotations for ads_key in adsorbates } elif not isinstance ( rotations , dict ): msg = f \"Unrecognized input type for rotations ( { type ( rotations ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if adsorption_sites is None : adsorption_sites = {} elif isinstance ( adsorption_sites , dict ): # check if the input is a single site for all adsorbates vs separate # sites for each adsorbate if all ([ ads_key not in adsorption_sites for ads_key in adsorbates ]): adsorption_sites = { ads_key : adsorption_sites for ads_key in adsorbates } elif not isinstance ( adsorption_sites , dict ): msg = f \"Unrecognized input type for adsorption_sites ( { type ( adsorption_sites ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if use_all_sites is None : use_all_sites = {} elif isinstance ( use_all_sites , bool ): use_all_sites = { ads_key : use_all_sites for ads_key in adsorbates } elif not isinstance ( use_all_sites , dict ): msg = f \"Unrecognized input type for use_all_sites ( { type ( use_all_sites ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if site_types is None : site_types = {} elif isinstance ( site_types , ( list , tuple )): site_types = { ads_key : site_types for ads_key in adsorbates } elif not isinstance ( site_types , dict ): msg = f \"Unrecognized input type for site_types ( { type ( site_types ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if heights is None : heights = {} elif isinstance ( heights , float ): heights = { ads_key : heights for ads_key in adsorbates } elif not isinstance ( heights , dict ): msg = f \"Unrecognized input type for heights ( { type ( heights ) } )\" raise AutocatAdsorptionGenerationError ( msg ) if anchor_atom_indices is None : anchor_atom_indices = {} elif isinstance ( anchor_atom_indices , int ): anchor_atom_indices = { ads_key : anchor_atom_indices for ads_key in adsorbates } elif not isinstance ( anchor_atom_indices , dict ): msg = f \"Unrecognized input type for anchor_atom_indices ( { type ( anchor_atom_indices ) } )\" raise AutocatAdsorptionGenerationError ( msg ) ads_structures = {} for ads_key in adsorbates : ads_structures [ ads_key ] = {} # generate the molecule if not already input adsorbate = adsorbates . get ( ads_key ) if isinstance ( adsorbate , str ): adsorbate = generate_molecule ( molecule_name = adsorbate )[ \"structure\" ] # get all adsorption sites for the adsorbate ads_adsorption_sites = adsorption_sites . get ( ads_key , { \"origin\" : [( 0 , 0 )]}) ads_use_all_sites = use_all_sites . get ( ads_key , False ) if ads_use_all_sites : ads_site_types = site_types . get ( ads_key , [ \"ontop\" , \"bridge\" , \"hollow\" ]) ads_adsorption_sites = get_adsorption_sites ( surface , site_types = ads_site_types ) # for each adsorption site type, for each (xy) coordinate, generate the # adsorbated (surface + adsorbate) structure for site in ads_adsorption_sites : ads_structures [ ads_key ][ site ] = {} for coords in ads_adsorption_sites [ site ]: # use only xy coords and ignore z if given here (handled by ads_height) coords = coords [: 2 ] rcoords = np . around ( coords , 3 ) scoords = f \" { str ( rcoords [ 0 ]) } _ { str ( rcoords [ 1 ]) } \" ads_height = heights . get ( ads_key , None ) ads_anchor_atom_index = anchor_atom_indices . get ( ads_key , 0 ) ads_rotations = rotations . get ( ads_key , [( 0 , \"x\" )]) adsorbed_structure = place_adsorbate ( surface = surface , adsorbate = adsorbate , adsorption_site = coords , anchor_atom_index = ads_anchor_atom_index , height = ads_height , rotations = ads_rotations , ) traj_file_path = None if write_to_disk : dir_path = os . path . join ( write_location , \"adsorbates\" , ads_key , site , scoords ) os . makedirs ( dir_path , exist_ok = dirs_exist_ok ) traj_file_path = os . path . join ( dir_path , \"input.traj\" ) adsorbed_structure . write ( traj_file_path ) print ( f \"Structure with { ads_key } adsorbed at { site } / { scoords } \" f \" written to { traj_file_path } \" ) ads_structures [ ads_key ][ site ] . update ( { scoords : { \"structure\" : adsorbed_structure , \"traj_file_path\" : traj_file_path , } } ) return ads_structures","title":"generate_adsorbed_structures()"},{"location":"API/Structure_Generation/adsorption/#autocat.adsorption.generate_molecule","text":"Generates an ase.Atoms object of an isolated molecule. If specified, can write out a .traj file containing the isolated molecule in a box. Parameters: Name Type Description Default molecule_name str String of the name of the molecule to be generated. Will search in autocat.intermediates data first and then ase g2 database. None rotations: List of rotation operations to be applied to the adsorbate molecule/intermediate before being placed on the host surface. Example: Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Defaults to [(0, \"x\")] (i.e., no rotations applied). cell: List of float specifying the dimensions of the box to place the molecule in, in Angstrom. Defaults to [15, 15, 15]. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the molecule structure must be written. The molecule is written to disk at [write_location]/references/[molecule_name]/input.traj dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the os.makedirs builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns: Type Description Dictionary containing Atoms object of the generated molecule and path to .traj file if written to disk. Example: { \"NH\": {\"structure\": NH_ase_obj, \"traj_file_path\": \"/path/to/NH/traj/file\"}, } Source code in autocat/adsorption.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def generate_molecule ( molecule_name : str = None , rotations : RotationOperations = None , cell : Sequence [ float ] = None , write_to_disk : bool = False , write_location : str = \".\" , dirs_exist_ok : bool = False , ) -> Dict [ str , Dict [ str , Any ]]: \"\"\" Generates an `ase.Atoms` object of an isolated molecule. If specified, can write out a .traj file containing the isolated molecule in a box. Parameters ---------- molecule_name (REQUIRED): String of the name of the molecule to be generated. Will search in `autocat.intermediates` data first and then `ase` g2 database. rotations: List of rotation operations to be applied to the adsorbate molecule/intermediate before being placed on the host surface. Example: Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Defaults to [(0, \"x\")] (i.e., no rotations applied). cell: List of float specifying the dimensions of the box to place the molecule in, in Angstrom. Defaults to [15, 15, 15]. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the molecule structure must be written. The molecule is written to disk at [write_location]/references/[molecule_name]/input.traj dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the `os.makedirs` builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns ------- Dictionary containing Atoms object of the generated molecule and path to .traj file if written to disk. Example: { \"NH\": {\"structure\": NH_ase_obj, \"traj_file_path\": \"/path/to/NH/traj/file\"}, } \"\"\" if molecule_name is None : msg = \"Molecule name must be specified\" raise AutocatAdsorptionGenerationError ( msg ) if rotations is None : rotations = [[ 0.0 , \"x\" ]] if cell is None : cell = [ 15 , 15 , 15 ] m = None if molecule_name in NRR_MOLS : m = NRR_MOLS [ molecule_name ] . copy () elif molecule_name in ORR_MOLS : m = ORR_MOLS [ molecule_name ] . copy () elif molecule_name in chemical_symbols : # atom-in-a-box m = Atoms ( molecule_name ) elif molecule_name in g2 . names : m = build_molecule ( molecule_name ) if m is None : msg = f \"Unable to construct molecule { molecule_name } \" raise NotImplementedError ( msg ) for r in rotations : m . rotate ( r [ 0 ], r [ 1 ]) m . cell = cell m . center () traj_file_path = None if write_to_disk : dir_path = os . path . join ( write_location , \"references\" , f \" { molecule_name } \" ) os . makedirs ( dir_path , exist_ok = dirs_exist_ok ) traj_file_path = os . path . join ( dir_path , \"input.traj\" ) m . write ( traj_file_path ) print ( f \" { molecule_name } molecule structure written to { traj_file_path } \" ) return { \"structure\" : m , \"traj_file_path\" : traj_file_path }","title":"generate_molecule()"},{"location":"API/Structure_Generation/adsorption/#autocat.adsorption.get_adsorbate_height_estimate","text":"Guess an initial height for the adsorbate to be placed on the surface, by summing covalent radii of the nearest neighbor atoms. Parameters: Name Type Description Default surface Atoms ase.Atoms object the surface for which adsorbate height must be estimated. None adsorbate (REQUIRED): ase.Atoms object of adsorbate to be placed on the surface. adsorption_site: Tuple or list of the xy cartesian coordinates for where the adsorbate would be placed. Defaults to [0, 0]. anchor_atom_index: Integer index of the atom in the adsorbate molecule that will be used as anchor when placing it on the surface. Defaults to the atom at index 0. scale: Float giving a scale factor to be applied to the calculated bond length. For example, scale = 1.1 -> bond length = 1.1 * (covalent_radius_1 + covalent_radius_2) Defaults to 1.0 (i.e., no scaling). Returns: Name Type Description height_estimate float Float with the estimated initial height for the adsorbate molecule from the surface. Returns a default of 1.5 Angstom if the guessing process using nearest-neighbor covalent radii fails. Source code in autocat/adsorption.py 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 def get_adsorbate_height_estimate ( surface : Atoms = None , adsorbate : Atoms = None , adsorption_site : Sequence [ float ] = None , anchor_atom_index : int = 0 , scale : float = 1.0 , ) -> float : \"\"\" Guess an initial height for the adsorbate to be placed on the surface, by summing covalent radii of the nearest neighbor atoms. Parameters ---------- surface (REQUIRED): `ase.Atoms` object the surface for which adsorbate height must be estimated. adsorbate (REQUIRED): `ase.Atoms` object of adsorbate to be placed on the surface. adsorption_site: Tuple or list of the xy cartesian coordinates for where the adsorbate would be placed. Defaults to [0, 0]. anchor_atom_index: Integer index of the atom in the adsorbate molecule that will be used as anchor when placing it on the surface. Defaults to the atom at index 0. scale: Float giving a scale factor to be applied to the calculated bond length. For example, scale = 1.1 -> bond length = 1.1 * (covalent_radius_1 + covalent_radius_2) Defaults to 1.0 (i.e., no scaling). Returns ------- height_estimate: Float with the estimated initial height for the adsorbate molecule from the surface. Returns a default of 1.5 Angstom if the guessing process using nearest-neighbor covalent radii fails. \"\"\" if adsorption_site is None : adsorption_site = ( 0 , 0 ) species , coords = get_adsorbate_slab_nn_list ( surface = surface , adsorption_site = adsorption_site ) ads_atom_radius = covalent_radii [ atomic_numbers [ adsorbate [ anchor_atom_index ] . symbol ] ] guessed_heights = [] for sp , coord in zip ( species , coords ): nn_radius = covalent_radii [ atomic_numbers [ sp ]] r_dist = scale * ( ads_atom_radius + nn_radius ) position_array = np . array ( adsorption_site ) if ( r_dist ** 2 - np . sum (( position_array - coord [: 2 ]) ** 2 )) >= 0.0 : height = np . sqrt ( r_dist ** 2 - np . sum (( position_array - coord [: 2 ]) ** 2 )) else : height = np . nan guessed_heights . append ( height ) if np . isnan ( np . nanmean ( guessed_heights )): height_estimate = 1.5 else : height_estimate = np . nanmean ( guessed_heights ) return height_estimate","title":"get_adsorbate_height_estimate()"},{"location":"API/Structure_Generation/adsorption/#autocat.adsorption.get_adsorbate_slab_nn_list","text":"Get list of nearest neighbors for the adsorbate on the surface at the specified position. Parameters: Name Type Description Default surface Atoms Atoms object the surface for which nearest neighbors of the adsorbate should be identified. None adsorption_site: Tuple or list of the xy cartesian coordinates for where the adsorbate would be placed. Defaults to [0, 0]. height: Float with the height of the adsorbate molecule from the surface in Angstrom. Returns: Type Description species_list , coordinates_list Two lists of length equal to the number of nearest neighbors identified: first with the list of species names, and second with the list of coordinates. Example: ([\"Fe\", \"Sr\"], [[0.4, 0.6], [0.2, 0.9]]) Source code in autocat/adsorption.py 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 def get_adsorbate_slab_nn_list ( surface : Atoms = None , adsorption_site : Sequence [ float ] = None , height : float = 0.5 ) -> Tuple [ List [ str ], List [ List [ float ]]]: \"\"\" Get list of nearest neighbors for the adsorbate on the surface at the specified position. Parameters ---------- surface (REQUIRED): Atoms object the surface for which nearest neighbors of the adsorbate should be identified. adsorption_site: Tuple or list of the xy cartesian coordinates for where the adsorbate would be placed. Defaults to [0, 0]. height: Float with the height of the adsorbate molecule from the surface in Angstrom. Returns ------- species_list, coordinates_list: Two lists of length equal to the number of nearest neighbors identified: first with the list of species names, and second with the list of coordinates. Example: ([\"Fe\", \"Sr\"], [[0.4, 0.6], [0.2, 0.9]]) \"\"\" if adsorption_site is None : adsorption_site = ( 0 , 0 ) surf = surface . copy () conv = AseAtomsAdaptor () add_adsorbate ( surf , \"X\" , height = height , position = adsorption_site ) init_guess = conv . get_structure ( surf ) nn = get_neighbors_of_site_with_index ( init_guess , - 1 ) species_list = [ _nn . species . hill_formula for _nn in nn ] coord_list = [ _nn . coords for _nn in nn ] return species_list , coord_list","title":"get_adsorbate_slab_nn_list()"},{"location":"API/Structure_Generation/adsorption/#autocat.adsorption.get_adsorption_sites","text":"For the given surface, returns all symmetrically unique sites of the specified type. Uses pymatgen.analysis.adsorption module to identify the sites. Parameters: Name Type Description Default surface Atoms Atoms object of the host surface for which the symmetrically unique surface sites should be identified. None site_types: List of types of adsorption sites to be searched for. Options are \"ontop\", \"bridge\", and \"hollow\". Defaults to [\"ontop\", \"bridge\", \"hollow\"]. **kwargs: Other miscellaneous keyword arguments. Will be passed on to AdsorbateSiteFinder.find_adsorption_sites in pymatgen . Returns: Name Type Description sites Dict [ str , Sequence [ float ]] Dictionary containing the reference structures with the identified sites for each desired site type Source code in autocat/adsorption.py 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 def get_adsorption_sites ( surface : Atoms = None , site_types : Sequence [ str ] = None , ** kwargs ) -> Dict [ str , Sequence [ float ]]: \"\"\" For the given surface, returns all symmetrically unique sites of the specified type. Uses `pymatgen.analysis.adsorption` module to identify the sites. Parameters ---------- surface (REQUIRED): Atoms object of the host surface for which the symmetrically unique surface sites should be identified. site_types: List of types of adsorption sites to be searched for. Options are \"ontop\", \"bridge\", and \"hollow\". Defaults to [\"ontop\", \"bridge\", \"hollow\"]. **kwargs: Other miscellaneous keyword arguments. Will be passed on to `AdsorbateSiteFinder.find_adsorption_sites` in `pymatgen`. Returns ------- sites: Dictionary containing the reference structures with the identified sites for each desired site type \"\"\" if site_types is None : site_types = [ \"ontop\" , \"bridge\" , \"hollow\" ] converter = AseAtomsAdaptor () pmg_structure = converter . get_structure ( surface ) finder = AdsorbateSiteFinder ( pmg_structure ) sites = finder . find_adsorption_sites ( positions = site_types , ** kwargs ) # Remove the extraneous \"all\" returned by default from pymatgen sites . pop ( \"all\" , None ) return sites","title":"get_adsorption_sites()"},{"location":"API/Structure_Generation/adsorption/#autocat.adsorption.place_adsorbate","text":"Places an adsorbate molecule/intermediate onto a given surface at the specified location. Parameters: Name Type Description Default surface Atoms Atoms object of the host surface. None adsorbate (REQUIRED): Atoms object of the adsorbate molecule/intermediate to be placed on the host surface. adsorption_site: Tuple or list of the xy cartesian coordinates on the surface where the adsorbate must be placed. Defaults to [0, 0]. rotations: List of rotation operations to be applied to the adsorbate molecule/intermediate before being placed on the host surface. Example: Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Defaults to [(0, \"x\")] (i.e., no rotations applied). anchor_atom_index: Integer index of the atom in the adsorbate molecule that will be used as anchor when placing it on the surface. Defaults to the atom at index 0. height: Float specifying the height above surface where adsorbate should be placed. If None, will estimate initial height based on covalent radii of the nearest neighbor atoms. Returns: Name Type Description surface Atoms Atoms object of the surface with the adsorbate molecule/intermediate placed on it as specified. Source code in autocat/adsorption.py 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 def place_adsorbate ( surface : Atoms = None , adsorbate : Atoms = None , adsorption_site : Sequence [ float ] = None , rotations : RotationOperations = None , anchor_atom_index : int = 0 , height : float = None , ) -> Atoms : \"\"\" Places an adsorbate molecule/intermediate onto a given surface at the specified location. Parameters ---------- surface (REQUIRED): Atoms object of the host surface. adsorbate (REQUIRED): Atoms object of the adsorbate molecule/intermediate to be placed on the host surface. adsorption_site: Tuple or list of the xy cartesian coordinates on the surface where the adsorbate must be placed. Defaults to [0, 0]. rotations: List of rotation operations to be applied to the adsorbate molecule/intermediate before being placed on the host surface. Example: Rotating 90 degrees around the z axis followed by 45 degrees around the y-axis can be specified as [(90.0, \"z\"), (45.0, \"y\")] Defaults to [(0, \"x\")] (i.e., no rotations applied). anchor_atom_index: Integer index of the atom in the adsorbate molecule that will be used as anchor when placing it on the surface. Defaults to the atom at index 0. height: Float specifying the height above surface where adsorbate should be placed. If None, will estimate initial height based on covalent radii of the nearest neighbor atoms. Returns ------- surface: Atoms object of the surface with the adsorbate molecule/intermediate placed on it as specified. \"\"\" if adsorption_site is None : adsorption_site = [ 0 , 0 ] if rotations is None : rotations = [( 0.0 , \"x\" )] _surface = surface . copy () _adsorbate = adsorbate . copy () for r in rotations : _adsorbate . rotate ( r [ 0 ], r [ 1 ]) if height is None : height = get_adsorbate_height_estimate ( _surface , _adsorbate , adsorption_site = adsorption_site , anchor_atom_index = anchor_atom_index , ) add_adsorbate ( _surface , _adsorbate , height , position = adsorption_site , mol_index = anchor_atom_index , ) return _surface","title":"place_adsorbate()"},{"location":"API/Structure_Generation/bulk/","text":"generate_bulk_structures ( species_list , crystal_structures = None , default_lat_param_lib = None , a_dict = None , c_dict = None , set_magnetic_moments = None , magnetic_moments = None , write_to_disk = False , write_location = '.' , dirs_exist_ok = False ) Generates bulk crystal structures and writes them to separate directories, if specified. Parameters: Name Type Description Default species_list List [ str ] List of chemical symbols of the bulk structures to be constructed. required cystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to ase.build.bulk . So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from ase.data will be used. default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Defaults to lattice constants defined in ase.data . Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in species_list that is NOT in the reference library specified, it will be pulled from `ase.data`. a_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. c_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. set_magnetic_moments: List of species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). magnetic_moments: Dictionary with the magnetic moments to be set for the chemical species listed previously. If not specified, default ground state magnetic moments from ase.data are used. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [species_1]_bulk_[crystal_structure_1]/input.traj [species_1]_bulk_[crystal_structure_2]/input.traj ... [species_2]_bulk_[crystal_structure_2]/input.traj ... Defaults to the current working directory. dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the os.makedirs builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns: Type Description Dictionary with bulk structures (as write-location (if any) for each input species. Example: { \"Pt\": {\"structure\": Pt_ase_obj, \"traj_file_path\": \"/path/to/Pt/traj/file\"}, \"Cr\": ... } Source code in autocat/bulk.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def generate_bulk_structures ( species_list : List [ str ], crystal_structures : Dict [ str , str ] = None , default_lat_param_lib : str = None , a_dict : Dict [ str , float ] = None , c_dict : Dict [ str , float ] = None , set_magnetic_moments : List [ str ] = None , magnetic_moments : Dict [ str , float ] = None , write_to_disk : bool = False , write_location : str = \".\" , dirs_exist_ok : bool = False , ) -> Dict [ str , Dict [ str , Any ]]: \"\"\" Generates bulk crystal structures and writes them to separate directories, if specified. Parameters ---------- species_list (REQUIRED): List of chemical symbols of the bulk structures to be constructed. cystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to `ase.build.bulk`. So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from `ase.data` will be used. default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Defaults to lattice constants defined in `ase.data`. Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in species_list that is NOT in the reference library specified, it will be pulled from `ase.data`. a_dict: Dictionary with lattice parameters <a> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. c_dict: Dictionary with lattice parameters <c> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. set_magnetic_moments: List of species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). magnetic_moments: Dictionary with the magnetic moments to be set for the chemical species listed previously. If not specified, default ground state magnetic moments from `ase.data` are used. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [species_1]_bulk_[crystal_structure_1]/input.traj [species_1]_bulk_[crystal_structure_2]/input.traj ... [species_2]_bulk_[crystal_structure_2]/input.traj ... Defaults to the current working directory. dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the `os.makedirs` builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns ------- Dictionary with bulk structures (as `ase.Atoms` objects) and write-location (if any) for each input species. Example: { \"Pt\": {\"structure\": Pt_ase_obj, \"traj_file_path\": \"/path/to/Pt/traj/file\"}, \"Cr\": ... } \"\"\" lpl = { \"pbe_fd\" : BULK_PBE_FD , \"beefvdw_fd\" : BULK_BEEFVDW_FD , \"pbe_pw\" : BULK_PBE_PW , \"beefvdw_pw\" : BULK_BEEFVDW_PW , } if crystal_structures is None : crystal_structures = {} if a_dict is None : a_dict = {} if c_dict is None : c_dict = {} if set_magnetic_moments is None : set_magnetic_moments = [ \"Fe\" , \"Co\" , \"Ni\" ] if magnetic_moments is None : magnetic_moments = {} # load crystal structure defaults from `ase.data`, override with user input cs_library = { species : reference_states [ atomic_numbers [ species ]] . get ( \"symmetry\" ) for species in species_list } cs_library . update ( crystal_structures ) # load lattice params <a>, <c> from reference library, override with user input a_library = {} c_library = {} if default_lat_param_lib is not None : a_library . update ( { species : lpl [ default_lat_param_lib ] . get ( species , {}) . get ( \"a\" ) for species in species_list } ) c_library . update ( { species : lpl [ default_lat_param_lib ] . get ( species , {}) . get ( \"c\" ) for species in species_list } ) a_library . update ( a_dict ) c_library . update ( c_dict ) # load magnetic moment defaults from `ase.data`, override with user input mm_library = { species : ground_state_magnetic_moments [ atomic_numbers [ species ]] for species in species_list } mm_library . update ( magnetic_moments ) bulk_structures = {} for species in species_list : cs = cs_library . get ( species ) a = a_library . get ( species ) c = c_library . get ( species ) bs = bulk ( species , crystalstructure = cs , a = a , c = c ) if species in set_magnetic_moments : bs . set_initial_magnetic_moments ([ mm_library [ species ]] * len ( bs )) traj_file_path = None if write_to_disk : dir_path = os . path . join ( write_location , f \" { species } _bulk_ { cs } \" ) os . makedirs ( dir_path , exist_ok = dirs_exist_ok ) traj_file_path = os . path . join ( dir_path , \"input.traj\" ) bs . write ( traj_file_path ) print ( f \" { species } _bulk_ { cs } structure written to { traj_file_path } \" ) bulk_structures [ species ] = { \"structure\" : bs , \"traj_file_path\" : traj_file_path , } return bulk_structures","title":"autocat.bulk"},{"location":"API/Structure_Generation/bulk/#autocat.bulk.generate_bulk_structures","text":"Generates bulk crystal structures and writes them to separate directories, if specified. Parameters: Name Type Description Default species_list List [ str ] List of chemical symbols of the bulk structures to be constructed. required cystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to ase.build.bulk . So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from ase.data will be used. default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Defaults to lattice constants defined in ase.data . Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in species_list that is NOT in the reference library specified, it will be pulled from `ase.data`. a_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. c_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. set_magnetic_moments: List of species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). magnetic_moments: Dictionary with the magnetic moments to be set for the chemical species listed previously. If not specified, default ground state magnetic moments from ase.data are used. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [species_1]_bulk_[crystal_structure_1]/input.traj [species_1]_bulk_[crystal_structure_2]/input.traj ... [species_2]_bulk_[crystal_structure_2]/input.traj ... Defaults to the current working directory. dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the os.makedirs builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns: Type Description Dictionary with bulk structures (as write-location (if any) for each input species. Example: { \"Pt\": {\"structure\": Pt_ase_obj, \"traj_file_path\": \"/path/to/Pt/traj/file\"}, \"Cr\": ... } Source code in autocat/bulk.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def generate_bulk_structures ( species_list : List [ str ], crystal_structures : Dict [ str , str ] = None , default_lat_param_lib : str = None , a_dict : Dict [ str , float ] = None , c_dict : Dict [ str , float ] = None , set_magnetic_moments : List [ str ] = None , magnetic_moments : Dict [ str , float ] = None , write_to_disk : bool = False , write_location : str = \".\" , dirs_exist_ok : bool = False , ) -> Dict [ str , Dict [ str , Any ]]: \"\"\" Generates bulk crystal structures and writes them to separate directories, if specified. Parameters ---------- species_list (REQUIRED): List of chemical symbols of the bulk structures to be constructed. cystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to `ase.build.bulk`. So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from `ase.data` will be used. default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Defaults to lattice constants defined in `ase.data`. Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in species_list that is NOT in the reference library specified, it will be pulled from `ase.data`. a_dict: Dictionary with lattice parameters <a> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. c_dict: Dictionary with lattice parameters <c> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. set_magnetic_moments: List of species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). magnetic_moments: Dictionary with the magnetic moments to be set for the chemical species listed previously. If not specified, default ground state magnetic moments from `ase.data` are used. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [species_1]_bulk_[crystal_structure_1]/input.traj [species_1]_bulk_[crystal_structure_2]/input.traj ... [species_2]_bulk_[crystal_structure_2]/input.traj ... Defaults to the current working directory. dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the `os.makedirs` builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns ------- Dictionary with bulk structures (as `ase.Atoms` objects) and write-location (if any) for each input species. Example: { \"Pt\": {\"structure\": Pt_ase_obj, \"traj_file_path\": \"/path/to/Pt/traj/file\"}, \"Cr\": ... } \"\"\" lpl = { \"pbe_fd\" : BULK_PBE_FD , \"beefvdw_fd\" : BULK_BEEFVDW_FD , \"pbe_pw\" : BULK_PBE_PW , \"beefvdw_pw\" : BULK_BEEFVDW_PW , } if crystal_structures is None : crystal_structures = {} if a_dict is None : a_dict = {} if c_dict is None : c_dict = {} if set_magnetic_moments is None : set_magnetic_moments = [ \"Fe\" , \"Co\" , \"Ni\" ] if magnetic_moments is None : magnetic_moments = {} # load crystal structure defaults from `ase.data`, override with user input cs_library = { species : reference_states [ atomic_numbers [ species ]] . get ( \"symmetry\" ) for species in species_list } cs_library . update ( crystal_structures ) # load lattice params <a>, <c> from reference library, override with user input a_library = {} c_library = {} if default_lat_param_lib is not None : a_library . update ( { species : lpl [ default_lat_param_lib ] . get ( species , {}) . get ( \"a\" ) for species in species_list } ) c_library . update ( { species : lpl [ default_lat_param_lib ] . get ( species , {}) . get ( \"c\" ) for species in species_list } ) a_library . update ( a_dict ) c_library . update ( c_dict ) # load magnetic moment defaults from `ase.data`, override with user input mm_library = { species : ground_state_magnetic_moments [ atomic_numbers [ species ]] for species in species_list } mm_library . update ( magnetic_moments ) bulk_structures = {} for species in species_list : cs = cs_library . get ( species ) a = a_library . get ( species ) c = c_library . get ( species ) bs = bulk ( species , crystalstructure = cs , a = a , c = c ) if species in set_magnetic_moments : bs . set_initial_magnetic_moments ([ mm_library [ species ]] * len ( bs )) traj_file_path = None if write_to_disk : dir_path = os . path . join ( write_location , f \" { species } _bulk_ { cs } \" ) os . makedirs ( dir_path , exist_ok = dirs_exist_ok ) traj_file_path = os . path . join ( dir_path , \"input.traj\" ) bs . write ( traj_file_path ) print ( f \" { species } _bulk_ { cs } structure written to { traj_file_path } \" ) bulk_structures [ species ] = { \"structure\" : bs , \"traj_file_path\" : traj_file_path , } return bulk_structures","title":"generate_bulk_structures()"},{"location":"API/Structure_Generation/saa/","text":"Single Atom Alloys generate_saa_structures ( host_species , dopant_species , crystal_structures = None , facets = None , supercell_dim = ( 3 , 3 , 4 ), default_lat_param_lib = None , a_dict = None , c_dict = None , set_host_magnetic_moments = None , host_magnetic_moments = None , set_dopant_magnetic_moments = None , dopant_magnetic_moments = None , vacuum = 10.0 , n_fixed_layers = 0 , place_dopant_at_center = True , write_to_disk = False , write_location = '.' , dirs_exist_ok = False ) Builds single-atom alloys for all combinations of host species and dopant species given. Will write the structures to separate directories if specified. Parameters: Name Type Description Default host_species List [ str ] List of chemical species of desired host (substrate) species. required dopant_species (REQUIRED): List of chemical symbols of desired single-atom dopant species. crystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to ase.build.bulk . So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from ase.data will be used. facets: Dictionary with the surface facets to be considered for each species. If not specified for a given species, the following defaults will be used based on the crystal structure: fcc/bcc: 100, 111, 110 hcp: 0001 supercell_dim: Tuple or List specifying the size of the supercell to be generated in the format (nx, ny, nz). Defaults to (3, 3, 4). default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in `host_species` that is NOT in the reference library specified, it will be pulled from `ase.data`. a_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. c_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. set_host_magnetic_moments: List of host species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). host_magnetic_moments: Dictionary with the magnetic moments to be set for the host chemical species listed previously. If not specified, default ground state magnetic moments from ase.data are used. set_dopant_magnetic_moments: List of single-atom species for which magnetic moments need to be set. If not specified, magnetic moments will guessed for all dopant species from ase.data . dopant_magnetic_moments: Dictionary with the magnetic moments to be set for the single-atom dopant species listed previously. If not specified, default ground state magnetic moments from ase.data are used. vacuum: Float specifying the amount of vacuum (in Angstrom) to be added to the slab (the slab is placed at the center of the supercell). Defaults to 10.0 Angstrom. n_fixed_layers: Integer giving the number of layers of the slab to be fixed starting from the bottom up (e.g., a value of 2 will fix the bottom 2 layers). Defaults to 0 (i.e., no layers in the slab fixed). place_dopant_at_center: Boolean specifying whether the single-atom should be placed at the center of the unit cell. If False, the single-atom will be placed at the origin. Defaults to True. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [host]/[dopant]/[facet]/substrate/input.traj dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the os.makedirs builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns: Type Description Dictionary with the single-atom alloy structures as write-location, if any, for each each input host and dopant species combination. Example: { \"Fe\": { \"Cu\": { \"bcc100\": { \"structure\": FeN-1_Cu1_saa_obj, \"traj_file_path\": \"/path/to/Cu/on/bcc/Fe/100/surface/traj/file\" }, \"bcc110\": ..., }, \"Ru\": { ... }, }, \"Rh\": { ... } } Source code in autocat/saa.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 def generate_saa_structures ( host_species : List [ str ], dopant_species : List [ str ], crystal_structures : Dict [ str , str ] = None , facets : Dict [ str , str ] = None , supercell_dim : Sequence [ int ] = ( 3 , 3 , 4 ), default_lat_param_lib : str = None , a_dict : Dict [ str , float ] = None , c_dict : Dict [ str , float ] = None , set_host_magnetic_moments : List [ str ] = None , host_magnetic_moments : Dict [ str , float ] = None , set_dopant_magnetic_moments : List [ str ] = None , dopant_magnetic_moments : Dict [ str , float ] = None , vacuum : float = 10.0 , n_fixed_layers : int = 0 , place_dopant_at_center : bool = True , write_to_disk : bool = False , write_location : str = \".\" , dirs_exist_ok : bool = False , ) -> Dict [ str , Dict [ str , Dict [ str , Dict [ str , Any ]]]]: \"\"\" Builds single-atom alloys for all combinations of host species and dopant species given. Will write the structures to separate directories if specified. Parameters ---------- host_species (REQUIRED): List of chemical species of desired host (substrate) species. dopant_species (REQUIRED): List of chemical symbols of desired single-atom dopant species. crystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to `ase.build.bulk`. So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from `ase.data` will be used. facets: Dictionary with the surface facets to be considered for each species. If not specified for a given species, the following defaults will be used based on the crystal structure: fcc/bcc: 100, 111, 110 hcp: 0001 supercell_dim: Tuple or List specifying the size of the supercell to be generated in the format (nx, ny, nz). Defaults to (3, 3, 4). default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in `host_species` that is NOT in the reference library specified, it will be pulled from `ase.data`. a_dict: Dictionary with lattice parameters <a> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. c_dict: Dictionary with lattice parameters <c> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. set_host_magnetic_moments: List of host species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). host_magnetic_moments: Dictionary with the magnetic moments to be set for the host chemical species listed previously. If not specified, default ground state magnetic moments from `ase.data` are used. set_dopant_magnetic_moments: List of single-atom species for which magnetic moments need to be set. If not specified, magnetic moments will guessed for all dopant species from `ase.data`. dopant_magnetic_moments: Dictionary with the magnetic moments to be set for the single-atom dopant species listed previously. If not specified, default ground state magnetic moments from `ase.data` are used. vacuum: Float specifying the amount of vacuum (in Angstrom) to be added to the slab (the slab is placed at the center of the supercell). Defaults to 10.0 Angstrom. n_fixed_layers: Integer giving the number of layers of the slab to be fixed starting from the bottom up (e.g., a value of 2 will fix the bottom 2 layers). Defaults to 0 (i.e., no layers in the slab fixed). place_dopant_at_center: Boolean specifying whether the single-atom should be placed at the center of the unit cell. If False, the single-atom will be placed at the origin. Defaults to True. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [host]/[dopant]/[facet]/substrate/input.traj dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the `os.makedirs` builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns ------- Dictionary with the single-atom alloy structures as `ase.Atoms` objects and write-location, if any, for each {crystal structure and facet} specified for each input host and dopant species combination. Example: { \"Fe\": { \"Cu\": { \"bcc100\": { \"structure\": FeN-1_Cu1_saa_obj, \"traj_file_path\": \"/path/to/Cu/on/bcc/Fe/100/surface/traj/file\" }, \"bcc110\": ..., }, \"Ru\": { ... }, }, \"Rh\": { ... } } \"\"\" hosts = generate_surface_structures ( host_species , crystal_structures = crystal_structures , facets = facets , supercell_dim = supercell_dim , default_lat_param_lib = default_lat_param_lib , a_dict = a_dict , c_dict = c_dict , set_magnetic_moments = set_host_magnetic_moments , magnetic_moments = host_magnetic_moments , vacuum = vacuum , n_fixed_layers = n_fixed_layers , ) if set_dopant_magnetic_moments is None : set_dopant_magnetic_moments = dopant_species if dopant_magnetic_moments is None : dopant_magnetic_moments = {} dop_mm_library = { dop : ground_state_magnetic_moments [ atomic_numbers [ dop ]] for dop in dopant_species } dop_mm_library . update ( dopant_magnetic_moments ) saa_structures = {} # iterate over hosts for host in hosts : saa_structures [ host ] = {} # iterate over single-atoms for dopant in dopant_species : # ensure host != single-atom if dopant == host : continue saa_structures [ host ][ dopant ] = {} # iterate over surface facets for facet in hosts [ host ]: host_structure = hosts [ host ][ facet ] . get ( \"structure\" ) doped_structure = substitute_single_atom_on_surface ( host_structure , dopant , place_dopant_at_center = place_dopant_at_center , dopant_magnetic_moment = dop_mm_library . get ( dopant ), ) traj_file_path = None if write_to_disk : dir_path = os . path . join ( write_location , host , dopant , facet , \"substrate\" ) os . makedirs ( dir_path , exist_ok = dirs_exist_ok ) traj_file_path = os . path . join ( dir_path , \"input.traj\" ) doped_structure . write ( traj_file_path ) print ( f \" { dopant } / { host } ( { facet } ) structure written to { traj_file_path } \" ) saa_structures [ host ][ dopant ][ facet ] = { \"structure\" : doped_structure , \"traj_file_path\" : traj_file_path , } return saa_structures substitute_single_atom_on_surface ( host_structure , dopant_element , place_dopant_at_center = True , dopant_magnetic_moment = 0.0 ) For a given host ( elemental surface ) structure and a dopant element, returns a slab with one host atom on the surface substituted with the specified dopant element with a specified magnetic moment. Note that for the current implementation (single-atom alloys), there will exist only one symmetrically unique site to substitute on the surface of the elemental slab. Parameters: Name Type Description Default host_structure Atoms ase.Atoms object of the host slab to be doped. required dopant_element (REQUIRED): String of the elemental species to be substitutionally doped into the host structure. place_dopant_at_center: Boolean specifying whether the single-atom dopant should be placed at the center of the unit cell. If False, the dopant atom will be placed at the origin. Defaults to True. dopant_magnetic_moment: Float with the initial magnetic moment on the doped single-atom. Defaults to no spin polarization (i.e., magnetic moment of 0). Returns: Type Description The elemental slab with a single-atom dopant on the surface as an Atoms Raises: Type Description NotImplementedError If multiple symmetrically equivalent sites are found on the surface to dope. Note that is intended more as a \"guardrail\" on current functionality to match the maturity/implementation of other modules in autocat than an actual error. The error should no longer be present when the substitution functionality is folded into a more general form. Source code in autocat/saa.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def substitute_single_atom_on_surface ( host_structure : Atoms , dopant_element : str , place_dopant_at_center : bool = True , dopant_magnetic_moment : float = 0.0 , ) -> Atoms : \"\"\" For a given host (**elemental surface**) structure and a dopant element, returns a slab with one host atom on the surface substituted with the specified dopant element with a specified magnetic moment. Note that for the current implementation (single-atom alloys), there will exist only one symmetrically unique site to substitute on the surface of the elemental slab. Parameters ---------- host_structure (REQUIRED): ase.Atoms object of the host slab to be doped. dopant_element (REQUIRED): String of the elemental species to be substitutionally doped into the host structure. place_dopant_at_center: Boolean specifying whether the single-atom dopant should be placed at the center of the unit cell. If False, the dopant atom will be placed at the origin. Defaults to True. dopant_magnetic_moment: Float with the initial magnetic moment on the doped single-atom. Defaults to no spin polarization (i.e., magnetic moment of 0). Returns ------- The elemental slab with a single-atom dopant on the surface as an `ase.Atoms` object. Raises ------ NotImplementedError If multiple symmetrically equivalent sites are found on the surface to dope. Note that is intended more as a \"guardrail\" on current functionality to match the maturity/implementation of other modules in `autocat` than an actual error. The error should no longer be present when the substitution functionality is folded into a more general form. \"\"\" all_surface_indices = _find_all_surface_atom_indices ( host_structure ) ase_all_doped_structures = [] for idx in all_surface_indices : dop_struct = host_structure . copy () dop_struct [ idx ] . symbol = dopant_element dop_struct [ idx ] . magmom = dopant_magnetic_moment ase_all_doped_structures . append ( dop_struct ) # convert ase substrate to pymatgen structure converter = AseAtomsAdaptor () pmg_doped_structures = [ converter . get_structure ( struct ) for struct in ase_all_doped_structures ] # check that only one unique surface doped structure matcher = StructureMatcher () pmg_symm_equiv_doped_structure = [ s [ 0 ] for s in matcher . group_structures ( pmg_doped_structures ) ] if len ( pmg_symm_equiv_doped_structure ) > 1 : msg = \"Multiple symmetrically unique sites to dope found.\" raise NotImplementedError ( msg ) # assumes only a single unique doped structure ase_substituted_structure = ase_all_doped_structures [ 0 ] # center the single-atom dopant if place_dopant_at_center : cent_x = ( ase_substituted_structure . cell [ 0 ][ 0 ] / 2 + ase_substituted_structure . cell [ 1 ][ 0 ] / 2 ) cent_y = ( ase_substituted_structure . cell [ 0 ][ 1 ] / 2 + ase_substituted_structure . cell [ 1 ][ 1 ] / 2 ) cent = ( cent_x , cent_y , 0 ) ase_substituted_structure . translate ( cent ) ase_substituted_structure . wrap () return ase_substituted_structure","title":"autocat.saa"},{"location":"API/Structure_Generation/saa/#single-atom-alloys","text":"","title":"Single Atom Alloys"},{"location":"API/Structure_Generation/saa/#autocat.saa.generate_saa_structures","text":"Builds single-atom alloys for all combinations of host species and dopant species given. Will write the structures to separate directories if specified. Parameters: Name Type Description Default host_species List [ str ] List of chemical species of desired host (substrate) species. required dopant_species (REQUIRED): List of chemical symbols of desired single-atom dopant species. crystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to ase.build.bulk . So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from ase.data will be used. facets: Dictionary with the surface facets to be considered for each species. If not specified for a given species, the following defaults will be used based on the crystal structure: fcc/bcc: 100, 111, 110 hcp: 0001 supercell_dim: Tuple or List specifying the size of the supercell to be generated in the format (nx, ny, nz). Defaults to (3, 3, 4). default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in `host_species` that is NOT in the reference library specified, it will be pulled from `ase.data`. a_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. c_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. set_host_magnetic_moments: List of host species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). host_magnetic_moments: Dictionary with the magnetic moments to be set for the host chemical species listed previously. If not specified, default ground state magnetic moments from ase.data are used. set_dopant_magnetic_moments: List of single-atom species for which magnetic moments need to be set. If not specified, magnetic moments will guessed for all dopant species from ase.data . dopant_magnetic_moments: Dictionary with the magnetic moments to be set for the single-atom dopant species listed previously. If not specified, default ground state magnetic moments from ase.data are used. vacuum: Float specifying the amount of vacuum (in Angstrom) to be added to the slab (the slab is placed at the center of the supercell). Defaults to 10.0 Angstrom. n_fixed_layers: Integer giving the number of layers of the slab to be fixed starting from the bottom up (e.g., a value of 2 will fix the bottom 2 layers). Defaults to 0 (i.e., no layers in the slab fixed). place_dopant_at_center: Boolean specifying whether the single-atom should be placed at the center of the unit cell. If False, the single-atom will be placed at the origin. Defaults to True. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [host]/[dopant]/[facet]/substrate/input.traj dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the os.makedirs builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns: Type Description Dictionary with the single-atom alloy structures as write-location, if any, for each each input host and dopant species combination. Example: { \"Fe\": { \"Cu\": { \"bcc100\": { \"structure\": FeN-1_Cu1_saa_obj, \"traj_file_path\": \"/path/to/Cu/on/bcc/Fe/100/surface/traj/file\" }, \"bcc110\": ..., }, \"Ru\": { ... }, }, \"Rh\": { ... } } Source code in autocat/saa.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 def generate_saa_structures ( host_species : List [ str ], dopant_species : List [ str ], crystal_structures : Dict [ str , str ] = None , facets : Dict [ str , str ] = None , supercell_dim : Sequence [ int ] = ( 3 , 3 , 4 ), default_lat_param_lib : str = None , a_dict : Dict [ str , float ] = None , c_dict : Dict [ str , float ] = None , set_host_magnetic_moments : List [ str ] = None , host_magnetic_moments : Dict [ str , float ] = None , set_dopant_magnetic_moments : List [ str ] = None , dopant_magnetic_moments : Dict [ str , float ] = None , vacuum : float = 10.0 , n_fixed_layers : int = 0 , place_dopant_at_center : bool = True , write_to_disk : bool = False , write_location : str = \".\" , dirs_exist_ok : bool = False , ) -> Dict [ str , Dict [ str , Dict [ str , Dict [ str , Any ]]]]: \"\"\" Builds single-atom alloys for all combinations of host species and dopant species given. Will write the structures to separate directories if specified. Parameters ---------- host_species (REQUIRED): List of chemical species of desired host (substrate) species. dopant_species (REQUIRED): List of chemical symbols of desired single-atom dopant species. crystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to `ase.build.bulk`. So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from `ase.data` will be used. facets: Dictionary with the surface facets to be considered for each species. If not specified for a given species, the following defaults will be used based on the crystal structure: fcc/bcc: 100, 111, 110 hcp: 0001 supercell_dim: Tuple or List specifying the size of the supercell to be generated in the format (nx, ny, nz). Defaults to (3, 3, 4). default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in `host_species` that is NOT in the reference library specified, it will be pulled from `ase.data`. a_dict: Dictionary with lattice parameters <a> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. c_dict: Dictionary with lattice parameters <c> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. set_host_magnetic_moments: List of host species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). host_magnetic_moments: Dictionary with the magnetic moments to be set for the host chemical species listed previously. If not specified, default ground state magnetic moments from `ase.data` are used. set_dopant_magnetic_moments: List of single-atom species for which magnetic moments need to be set. If not specified, magnetic moments will guessed for all dopant species from `ase.data`. dopant_magnetic_moments: Dictionary with the magnetic moments to be set for the single-atom dopant species listed previously. If not specified, default ground state magnetic moments from `ase.data` are used. vacuum: Float specifying the amount of vacuum (in Angstrom) to be added to the slab (the slab is placed at the center of the supercell). Defaults to 10.0 Angstrom. n_fixed_layers: Integer giving the number of layers of the slab to be fixed starting from the bottom up (e.g., a value of 2 will fix the bottom 2 layers). Defaults to 0 (i.e., no layers in the slab fixed). place_dopant_at_center: Boolean specifying whether the single-atom should be placed at the center of the unit cell. If False, the single-atom will be placed at the origin. Defaults to True. write_to_disk: Boolean specifying whether the bulk structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [host]/[dopant]/[facet]/substrate/input.traj dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the `os.makedirs` builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns ------- Dictionary with the single-atom alloy structures as `ase.Atoms` objects and write-location, if any, for each {crystal structure and facet} specified for each input host and dopant species combination. Example: { \"Fe\": { \"Cu\": { \"bcc100\": { \"structure\": FeN-1_Cu1_saa_obj, \"traj_file_path\": \"/path/to/Cu/on/bcc/Fe/100/surface/traj/file\" }, \"bcc110\": ..., }, \"Ru\": { ... }, }, \"Rh\": { ... } } \"\"\" hosts = generate_surface_structures ( host_species , crystal_structures = crystal_structures , facets = facets , supercell_dim = supercell_dim , default_lat_param_lib = default_lat_param_lib , a_dict = a_dict , c_dict = c_dict , set_magnetic_moments = set_host_magnetic_moments , magnetic_moments = host_magnetic_moments , vacuum = vacuum , n_fixed_layers = n_fixed_layers , ) if set_dopant_magnetic_moments is None : set_dopant_magnetic_moments = dopant_species if dopant_magnetic_moments is None : dopant_magnetic_moments = {} dop_mm_library = { dop : ground_state_magnetic_moments [ atomic_numbers [ dop ]] for dop in dopant_species } dop_mm_library . update ( dopant_magnetic_moments ) saa_structures = {} # iterate over hosts for host in hosts : saa_structures [ host ] = {} # iterate over single-atoms for dopant in dopant_species : # ensure host != single-atom if dopant == host : continue saa_structures [ host ][ dopant ] = {} # iterate over surface facets for facet in hosts [ host ]: host_structure = hosts [ host ][ facet ] . get ( \"structure\" ) doped_structure = substitute_single_atom_on_surface ( host_structure , dopant , place_dopant_at_center = place_dopant_at_center , dopant_magnetic_moment = dop_mm_library . get ( dopant ), ) traj_file_path = None if write_to_disk : dir_path = os . path . join ( write_location , host , dopant , facet , \"substrate\" ) os . makedirs ( dir_path , exist_ok = dirs_exist_ok ) traj_file_path = os . path . join ( dir_path , \"input.traj\" ) doped_structure . write ( traj_file_path ) print ( f \" { dopant } / { host } ( { facet } ) structure written to { traj_file_path } \" ) saa_structures [ host ][ dopant ][ facet ] = { \"structure\" : doped_structure , \"traj_file_path\" : traj_file_path , } return saa_structures","title":"generate_saa_structures()"},{"location":"API/Structure_Generation/saa/#autocat.saa.substitute_single_atom_on_surface","text":"For a given host ( elemental surface ) structure and a dopant element, returns a slab with one host atom on the surface substituted with the specified dopant element with a specified magnetic moment. Note that for the current implementation (single-atom alloys), there will exist only one symmetrically unique site to substitute on the surface of the elemental slab. Parameters: Name Type Description Default host_structure Atoms ase.Atoms object of the host slab to be doped. required dopant_element (REQUIRED): String of the elemental species to be substitutionally doped into the host structure. place_dopant_at_center: Boolean specifying whether the single-atom dopant should be placed at the center of the unit cell. If False, the dopant atom will be placed at the origin. Defaults to True. dopant_magnetic_moment: Float with the initial magnetic moment on the doped single-atom. Defaults to no spin polarization (i.e., magnetic moment of 0). Returns: Type Description The elemental slab with a single-atom dopant on the surface as an Atoms Raises: Type Description NotImplementedError If multiple symmetrically equivalent sites are found on the surface to dope. Note that is intended more as a \"guardrail\" on current functionality to match the maturity/implementation of other modules in autocat than an actual error. The error should no longer be present when the substitution functionality is folded into a more general form. Source code in autocat/saa.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def substitute_single_atom_on_surface ( host_structure : Atoms , dopant_element : str , place_dopant_at_center : bool = True , dopant_magnetic_moment : float = 0.0 , ) -> Atoms : \"\"\" For a given host (**elemental surface**) structure and a dopant element, returns a slab with one host atom on the surface substituted with the specified dopant element with a specified magnetic moment. Note that for the current implementation (single-atom alloys), there will exist only one symmetrically unique site to substitute on the surface of the elemental slab. Parameters ---------- host_structure (REQUIRED): ase.Atoms object of the host slab to be doped. dopant_element (REQUIRED): String of the elemental species to be substitutionally doped into the host structure. place_dopant_at_center: Boolean specifying whether the single-atom dopant should be placed at the center of the unit cell. If False, the dopant atom will be placed at the origin. Defaults to True. dopant_magnetic_moment: Float with the initial magnetic moment on the doped single-atom. Defaults to no spin polarization (i.e., magnetic moment of 0). Returns ------- The elemental slab with a single-atom dopant on the surface as an `ase.Atoms` object. Raises ------ NotImplementedError If multiple symmetrically equivalent sites are found on the surface to dope. Note that is intended more as a \"guardrail\" on current functionality to match the maturity/implementation of other modules in `autocat` than an actual error. The error should no longer be present when the substitution functionality is folded into a more general form. \"\"\" all_surface_indices = _find_all_surface_atom_indices ( host_structure ) ase_all_doped_structures = [] for idx in all_surface_indices : dop_struct = host_structure . copy () dop_struct [ idx ] . symbol = dopant_element dop_struct [ idx ] . magmom = dopant_magnetic_moment ase_all_doped_structures . append ( dop_struct ) # convert ase substrate to pymatgen structure converter = AseAtomsAdaptor () pmg_doped_structures = [ converter . get_structure ( struct ) for struct in ase_all_doped_structures ] # check that only one unique surface doped structure matcher = StructureMatcher () pmg_symm_equiv_doped_structure = [ s [ 0 ] for s in matcher . group_structures ( pmg_doped_structures ) ] if len ( pmg_symm_equiv_doped_structure ) > 1 : msg = \"Multiple symmetrically unique sites to dope found.\" raise NotImplementedError ( msg ) # assumes only a single unique doped structure ase_substituted_structure = ase_all_doped_structures [ 0 ] # center the single-atom dopant if place_dopant_at_center : cent_x = ( ase_substituted_structure . cell [ 0 ][ 0 ] / 2 + ase_substituted_structure . cell [ 1 ][ 0 ] / 2 ) cent_y = ( ase_substituted_structure . cell [ 0 ][ 1 ] / 2 + ase_substituted_structure . cell [ 1 ][ 1 ] / 2 ) cent = ( cent_x , cent_y , 0 ) ase_substituted_structure . translate ( cent ) ase_substituted_structure . wrap () return ase_substituted_structure","title":"substitute_single_atom_on_surface()"},{"location":"API/Structure_Generation/surface/","text":"generate_surface_structures ( species_list , crystal_structures = None , facets = None , supercell_dim = None , default_lat_param_lib = None , a_dict = None , c_dict = None , set_magnetic_moments = None , magnetic_moments = None , vacuum = 10.0 , n_fixed_layers = 0 , write_to_disk = False , write_location = '.' , dirs_exist_ok = False ) Generates mono-element slabs and writes them to separate directories, if specified. Parameters: Name Type Description Default species_list List [ str ] List of chemical symbols of the slabs to be built. required crystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to ase.build.bulk . So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from ase.data will be used. facets: Dictionary with the surface facets to be considered for each species. If not specified for a given species, the following defaults will be used based on the crystal structure: fcc/bcc: 100, 111, 110 hcp: 0001 supercell_dim: List specifying the size of the supercell to be generated in the format (nx, ny, nz). default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Defaults to lattice constants defined in ase.data . Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in species_list that is NOT in the reference library specified, it will be pulled from `ase.data` a_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. c_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. set_magnetic_moments: List of species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). magnetic_moments: Dictionary with the magnetic moments to be set for the chemical species listed previously. If not specified, default ground state magnetic moments from ase.data are used. vacuum: Float specifying the amount of vacuum (in Angstrom) to be added to the slab (the slab is placed at the center of the supercell). n_fixed_layers: Integer giving the number of layers of the slab to be fixed starting from the bottom up (e.g. a value of 2 will fix the bottom 2 layers). write_to_disk: Boolean specifying whether the surface structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [species]/[crystal_structure + facet]/substrate/input.traj Defaults to the current working directory. dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the os.makedirs builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns: Type Description Dictionary with surface structures (as write-location (if any) for each each input species. Example: { \"Pt\": { \"fcc111\": { \"structure\": Pt_surface_obj, \"traj_file_path\": \"/path/to/Pt/fcc111/surface/traj/file\" }, \"fcc100\": ... , \"Cu\": ... } } Source code in autocat/surface.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def generate_surface_structures ( species_list : List [ str ], crystal_structures : Dict [ str , str ] = None , facets : Dict [ str , str ] = None , supercell_dim : List [ int ] = None , default_lat_param_lib : str = None , a_dict : Dict [ str , float ] = None , c_dict : Dict [ str , float ] = None , set_magnetic_moments : List [ str ] = None , magnetic_moments : Dict [ str , float ] = None , vacuum : float = 10.0 , n_fixed_layers : int = 0 , write_to_disk : bool = False , write_location : str = \".\" , dirs_exist_ok : bool = False , ) -> Dict [ str , Dict [ str , Dict [ str , Any ]]]: \"\"\" Generates mono-element slabs and writes them to separate directories, if specified. Parameters ---------- species_list (REQUIRED): List of chemical symbols of the slabs to be built. crystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to `ase.build.bulk`. So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from `ase.data` will be used. facets: Dictionary with the surface facets to be considered for each species. If not specified for a given species, the following defaults will be used based on the crystal structure: fcc/bcc: 100, 111, 110 hcp: 0001 supercell_dim: List specifying the size of the supercell to be generated in the format (nx, ny, nz). default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Defaults to lattice constants defined in `ase.data`. Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in species_list that is NOT in the reference library specified, it will be pulled from `ase.data` a_dict: Dictionary with lattice parameters <a> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. c_dict: Dictionary with lattice parameters <c> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. set_magnetic_moments: List of species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). magnetic_moments: Dictionary with the magnetic moments to be set for the chemical species listed previously. If not specified, default ground state magnetic moments from `ase.data` are used. vacuum: Float specifying the amount of vacuum (in Angstrom) to be added to the slab (the slab is placed at the center of the supercell). n_fixed_layers: Integer giving the number of layers of the slab to be fixed starting from the bottom up (e.g. a value of 2 will fix the bottom 2 layers). write_to_disk: Boolean specifying whether the surface structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [species]/[crystal_structure + facet]/substrate/input.traj Defaults to the current working directory. dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the `os.makedirs` builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns ------- Dictionary with surface structures (as `ase.Atoms` objects) and write-location (if any) for each {crystal structure and facet} specified for each input species. Example: { \"Pt\": { \"fcc111\": { \"structure\": Pt_surface_obj, \"traj_file_path\": \"/path/to/Pt/fcc111/surface/traj/file\" }, \"fcc100\": ... , \"Cu\": ... } } \"\"\" lpl = { \"pbe_fd\" : BULK_PBE_FD , \"beefvdw_fd\" : BULK_BEEFVDW_FD , \"pbe_pw\" : BULK_PBE_PW , \"beefvdw_pw\" : BULK_BEEFVDW_PW , } ase_build_funcs = { \"fcc100\" : ase . build . fcc100 , \"fcc110\" : ase . build . fcc110 , \"fcc111\" : ase . build . fcc111 , \"bcc100\" : ase . build . bcc100 , \"bcc110\" : ase . build . bcc110 , \"bcc111\" : ase . build . bcc111 , \"hcp0001\" : ase . build . hcp0001 , } if crystal_structures is None : crystal_structures = {} if facets is None : facets = {} if supercell_dim is None : supercell_dim = [ 3 , 3 , 4 ] if a_dict is None : a_dict = {} if c_dict is None : c_dict = {} if set_magnetic_moments is None : set_magnetic_moments = [ \"Fe\" , \"Co\" , \"Ni\" ] if magnetic_moments is None : magnetic_moments = {} # load crystal structure defaults from `ase.data`, override with user input cs_library = { species : reference_states [ atomic_numbers [ species ]] . get ( \"symmetry\" ) for species in species_list } cs_library . update ( crystal_structures ) # load lattice params <a>, <c> from reference library, override with user input a_library = {} c_library = {} if default_lat_param_lib is not None : a_library . update ( { species : lpl [ default_lat_param_lib ] . get ( species , {}) . get ( \"a\" ) for species in species_list } ) c_library . update ( { species : lpl [ default_lat_param_lib ] . get ( species , {}) . get ( \"c\" ) for species in species_list } ) a_library . update ( a_dict ) c_library . update ( c_dict ) # load magnetic moment defaults from `ase.data`, override with user input mm_library = { species : ground_state_magnetic_moments [ atomic_numbers [ species ]] for species in species_list } mm_library . update ( magnetic_moments ) # set default facets for each crystal structure, override with user input ft_defaults = { \"fcc\" : [ \"100\" , \"111\" , \"110\" ], \"bcc\" : [ \"100\" , \"111\" , \"110\" ], \"hcp\" : [ \"0001\" ], } ft_library = { species : ft_defaults [ cs_library [ species ]] for species in species_list } ft_library . update ( facets ) surface_structures = {} for species in species_list : cs = cs_library . get ( species ) a = a_library . get ( species ) c = c_library . get ( species ) surf = {} for facet in ft_library [ species ]: if c is not None : struct = ase_build_funcs [ f \" { cs }{ facet } \" ]( species , size = supercell_dim , vacuum = vacuum , a = a , c = c ) else : struct = ase_build_funcs [ f \" { cs }{ facet } \" ]( species , size = supercell_dim , vacuum = vacuum , a = a ) if n_fixed_layers > 0 : f = FixAtoms ( mask = [ atom . tag > ( supercell_dim [ - 1 ] - n_fixed_layers ) for atom in struct ] ) struct . set_constraint ([ f ]) if species in set_magnetic_moments : struct . set_initial_magnetic_moments ([ mm_library [ species ]] * len ( struct )) traj_file_path = None if write_to_disk : dir_path = os . path . join ( write_location , f \" { species } \" , f \" { cs }{ facet } \" , \"substrate\" ) os . makedirs ( dir_path , exist_ok = dirs_exist_ok ) traj_file_path = os . path . join ( dir_path , \"input.traj\" ) struct . write ( traj_file_path ) print ( f \" { species } _ { cs }{ facet } structure written to { traj_file_path } \" ) surf [ f \" { cs }{ facet } \" ] = { \"structure\" : struct , \"traj_file_path\" : traj_file_path , } surface_structures [ species ] = surf return surface_structures","title":"autocat.surface"},{"location":"API/Structure_Generation/surface/#autocat.surface.generate_surface_structures","text":"Generates mono-element slabs and writes them to separate directories, if specified. Parameters: Name Type Description Default species_list List [ str ] List of chemical symbols of the slabs to be built. required crystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to ase.build.bulk . So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from ase.data will be used. facets: Dictionary with the surface facets to be considered for each species. If not specified for a given species, the following defaults will be used based on the crystal structure: fcc/bcc: 100, 111, 110 hcp: 0001 supercell_dim: List specifying the size of the supercell to be generated in the format (nx, ny, nz). default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Defaults to lattice constants defined in ase.data . Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in species_list that is NOT in the reference library specified, it will be pulled from `ase.data` a_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. c_dict: Dictionary with lattice parameters to be used for each species. If not specified, defaults from default_lat_param_lib are used. set_magnetic_moments: List of species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). magnetic_moments: Dictionary with the magnetic moments to be set for the chemical species listed previously. If not specified, default ground state magnetic moments from ase.data are used. vacuum: Float specifying the amount of vacuum (in Angstrom) to be added to the slab (the slab is placed at the center of the supercell). n_fixed_layers: Integer giving the number of layers of the slab to be fixed starting from the bottom up (e.g. a value of 2 will fix the bottom 2 layers). write_to_disk: Boolean specifying whether the surface structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [species]/[crystal_structure + facet]/substrate/input.traj Defaults to the current working directory. dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the os.makedirs builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns: Type Description Dictionary with surface structures (as write-location (if any) for each each input species. Example: { \"Pt\": { \"fcc111\": { \"structure\": Pt_surface_obj, \"traj_file_path\": \"/path/to/Pt/fcc111/surface/traj/file\" }, \"fcc100\": ... , \"Cu\": ... } } Source code in autocat/surface.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def generate_surface_structures ( species_list : List [ str ], crystal_structures : Dict [ str , str ] = None , facets : Dict [ str , str ] = None , supercell_dim : List [ int ] = None , default_lat_param_lib : str = None , a_dict : Dict [ str , float ] = None , c_dict : Dict [ str , float ] = None , set_magnetic_moments : List [ str ] = None , magnetic_moments : Dict [ str , float ] = None , vacuum : float = 10.0 , n_fixed_layers : int = 0 , write_to_disk : bool = False , write_location : str = \".\" , dirs_exist_ok : bool = False , ) -> Dict [ str , Dict [ str , Dict [ str , Any ]]]: \"\"\" Generates mono-element slabs and writes them to separate directories, if specified. Parameters ---------- species_list (REQUIRED): List of chemical symbols of the slabs to be built. crystal_structures: Dictionary with crystal structure to be used for each species. These will be passed on as input to `ase.build.bulk`. So, must be one of sc, fcc, bcc, tetragonal, bct, hcp, rhombohedral, orthorhombic, diamond, zincblende, rocksalt, cesiumchloride, fluorite or wurtzite. If not specified, the default reference crystal structure for each species from `ase.data` will be used. facets: Dictionary with the surface facets to be considered for each species. If not specified for a given species, the following defaults will be used based on the crystal structure: fcc/bcc: 100, 111, 110 hcp: 0001 supercell_dim: List specifying the size of the supercell to be generated in the format (nx, ny, nz). default_lat_param_lib: String indicating which library the lattice constants should be pulled from if not specified in either a_dict or c_dict. Defaults to lattice constants defined in `ase.data`. Options: pbe_fd: parameters calculated using xc=PBE and finite-difference beefvdw_fd: parameters calculated using xc=BEEF-vdW and finite-difference pbe_pw: parameters calculated using xc=PBE and a plane-wave basis set beefvdw_fd: parameters calculated using xc=BEEF-vdW and a plane-wave basis set N.B. if there is a species present in species_list that is NOT in the reference library specified, it will be pulled from `ase.data` a_dict: Dictionary with lattice parameters <a> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. c_dict: Dictionary with lattice parameters <c> to be used for each species. If not specified, defaults from `default_lat_param_lib` are used. set_magnetic_moments: List of species for which magnetic moments need to be set. If not specified, magnetic moments will be set only for Fe, Co, Ni (the ferromagnetic elements). magnetic_moments: Dictionary with the magnetic moments to be set for the chemical species listed previously. If not specified, default ground state magnetic moments from `ase.data` are used. vacuum: Float specifying the amount of vacuum (in Angstrom) to be added to the slab (the slab is placed at the center of the supercell). n_fixed_layers: Integer giving the number of layers of the slab to be fixed starting from the bottom up (e.g. a value of 2 will fix the bottom 2 layers). write_to_disk: Boolean specifying whether the surface structures generated should be written to disk. Defaults to False. write_location: String with the location where the per-species/per-crystal structure directories must be constructed and structure files written to disk. In the specified write_location, the following directory structure will be created: [species]/[crystal_structure + facet]/substrate/input.traj Defaults to the current working directory. dirs_exist_ok: Boolean specifying whether existing directories/files should be overwritten or not. This is passed on to the `os.makedirs` builtin. Defaults to False (raises an error if directories corresponding the species and crystal structure already exist). Returns ------- Dictionary with surface structures (as `ase.Atoms` objects) and write-location (if any) for each {crystal structure and facet} specified for each input species. Example: { \"Pt\": { \"fcc111\": { \"structure\": Pt_surface_obj, \"traj_file_path\": \"/path/to/Pt/fcc111/surface/traj/file\" }, \"fcc100\": ... , \"Cu\": ... } } \"\"\" lpl = { \"pbe_fd\" : BULK_PBE_FD , \"beefvdw_fd\" : BULK_BEEFVDW_FD , \"pbe_pw\" : BULK_PBE_PW , \"beefvdw_pw\" : BULK_BEEFVDW_PW , } ase_build_funcs = { \"fcc100\" : ase . build . fcc100 , \"fcc110\" : ase . build . fcc110 , \"fcc111\" : ase . build . fcc111 , \"bcc100\" : ase . build . bcc100 , \"bcc110\" : ase . build . bcc110 , \"bcc111\" : ase . build . bcc111 , \"hcp0001\" : ase . build . hcp0001 , } if crystal_structures is None : crystal_structures = {} if facets is None : facets = {} if supercell_dim is None : supercell_dim = [ 3 , 3 , 4 ] if a_dict is None : a_dict = {} if c_dict is None : c_dict = {} if set_magnetic_moments is None : set_magnetic_moments = [ \"Fe\" , \"Co\" , \"Ni\" ] if magnetic_moments is None : magnetic_moments = {} # load crystal structure defaults from `ase.data`, override with user input cs_library = { species : reference_states [ atomic_numbers [ species ]] . get ( \"symmetry\" ) for species in species_list } cs_library . update ( crystal_structures ) # load lattice params <a>, <c> from reference library, override with user input a_library = {} c_library = {} if default_lat_param_lib is not None : a_library . update ( { species : lpl [ default_lat_param_lib ] . get ( species , {}) . get ( \"a\" ) for species in species_list } ) c_library . update ( { species : lpl [ default_lat_param_lib ] . get ( species , {}) . get ( \"c\" ) for species in species_list } ) a_library . update ( a_dict ) c_library . update ( c_dict ) # load magnetic moment defaults from `ase.data`, override with user input mm_library = { species : ground_state_magnetic_moments [ atomic_numbers [ species ]] for species in species_list } mm_library . update ( magnetic_moments ) # set default facets for each crystal structure, override with user input ft_defaults = { \"fcc\" : [ \"100\" , \"111\" , \"110\" ], \"bcc\" : [ \"100\" , \"111\" , \"110\" ], \"hcp\" : [ \"0001\" ], } ft_library = { species : ft_defaults [ cs_library [ species ]] for species in species_list } ft_library . update ( facets ) surface_structures = {} for species in species_list : cs = cs_library . get ( species ) a = a_library . get ( species ) c = c_library . get ( species ) surf = {} for facet in ft_library [ species ]: if c is not None : struct = ase_build_funcs [ f \" { cs }{ facet } \" ]( species , size = supercell_dim , vacuum = vacuum , a = a , c = c ) else : struct = ase_build_funcs [ f \" { cs }{ facet } \" ]( species , size = supercell_dim , vacuum = vacuum , a = a ) if n_fixed_layers > 0 : f = FixAtoms ( mask = [ atom . tag > ( supercell_dim [ - 1 ] - n_fixed_layers ) for atom in struct ] ) struct . set_constraint ([ f ]) if species in set_magnetic_moments : struct . set_initial_magnetic_moments ([ mm_library [ species ]] * len ( struct )) traj_file_path = None if write_to_disk : dir_path = os . path . join ( write_location , f \" { species } \" , f \" { cs }{ facet } \" , \"substrate\" ) os . makedirs ( dir_path , exist_ok = dirs_exist_ok ) traj_file_path = os . path . join ( dir_path , \"input.traj\" ) struct . write ( traj_file_path ) print ( f \" { species } _ { cs }{ facet } structure written to { traj_file_path } \" ) surf [ f \" { cs }{ facet } \" ] = { \"structure\" : struct , \"traj_file_path\" : traj_file_path , } surface_structures [ species ] = surf return surface_structures","title":"generate_surface_structures()"},{"location":"Tutorials/pred_h/","text":"In this tutorial we are going to show how to use the learning tools within AutoCat to train a regressor that can predict adsorption energies of hydrogen on a set of single-atom alloys. Creating a DesignSpace Let's start by creating a DesignSpace . Normally each of these structures would be optimized via DFT, but for demo purposes we'll use the generated structures directly. First we need to generate the single-atom alloys. Here, we can use AutoCat's generate_saa_structures function. >>> # Generate the clean single-atom alloy structures >>> from autocat.saa import generate_saa_structures >>> from autocat.utils import flatten_structures_dict >>> saa_struct_dict = generate_saa_structures ( ... [ \"Fe\" , \"Cu\" , \"Au\" ], ... [ \"Pt\" , \"Pd\" , \"Ni\" ], ... facets = { \"Fe\" :[ \"110\" ], \"Cu\" :[ \"111\" ], \"Au\" :[ \"111\" ]}, ... n_fixed_layers = 2 , ... ) >>> saa_structs = flatten_structures_dict ( saa_struct_dict ) Now that we have the clean structures, let's adsorb hydrogen on the surface. For convenience let's place H at the origin instead of considering all symmetry sites. To accomplish this we can make use of AutoCat's place_adsorbate function. >>> # Adsorb hydrogen onto each of the generated SAA surfaces >>> from autocat.adsorption import place_adsorbate >>> from ase import Atoms >>> ads_structs = [] >>> for clean_struct in saa_structs : ... ads_struct = place_adsorbate ( clean_struct , Atoms ( \"H\" )) ... ads_structs . append ( ads_struct ) This has collected all of the single-atom alloys with hydrogen adsorbed into a single list of ase.Atoms objects, ads_structs . Ideally at this stage we'd have adsorption energies for each of the generated structures after relaxation. As a proxy in this demo we'll create random labels, but this should be adsorption energies if you want to train a meaningful Predictor! >>> # Generate the labels for each structure >>> import numpy as np >>> labels = np . random . uniform ( - 1.5 , 1.5 , size = len ( ads_structs )) Finally, using both our structures and labels we can define a DesignSpace . In practice, if any of the labels for a structure are unknown, it can be included as a numpy.nan >>> from autocat.learning.sequential import DesignSpace >>> design_space = DesignSpace ( ads_structs , labels ) >>> design_space +-------------------------+-------------------------------------------+ | | DesignSpace | +-------------------------+-------------------------------------------+ | total # of systems | 9 | | # of unlabelled systems | 0 | | unique species present | [ 'Fe' , 'H' , 'Pt' , 'Pd' , 'Ni' , 'Cu' , 'Au' ] | | maximum label | 1.0173326963281424 | | minimum label | - 1.4789390894451206 | +-------------------------+-------------------------------------------+ Setting up a Predictor When setting up our Predictor we now have two choices to make: The technique to be used for featurizing the systems The regression model to be used for training and predictions Internally, the Predictor will contain a Featurizer object (that the user supplies) which stores all of our choices for how to featurize the systems. Our choice of featurizer class and the associated kwargs are specified via the featurizer_class and kwargs arguments, respectively. By providing the design space structures some of the kwargs related to the featurization (e.g. maximum structure size) can be automatically obtained. Let's featurize the hydrogen environment via dscribe 's SOAP class >>> from autocat.learning.featurizers import Featurizer >>> from dscribe.descriptors.soap import SOAP >>> featurizer = Featurizer ( ... featurizer_class = SOAP , ... kwargs = { \"rcut\" : 7.0 , \"nmax\" : 8 , \"lmax\" : 8 }, ... design_space_structures = design_space . design_space_structures ... ) >>> featurizer +-----------------------------------+-------------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------------+ | class | dscribe . descriptors . soap . SOAP | | kwargs | { 'rcut' : 7.0 , 'nmax' : 8 , 'lmax' : 8 } | | species list | [ 'Fe' , 'Ni' , 'Pt' , 'Pd' , 'Au' , 'Cu' , 'H' ] | | maximum structure size | 37 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------------+ Similarly, we can specify the regressor to be used. The class should be \" sklearn -like\" with fit and predict methods. Here we will use sklearn 's GaussianProcessRegressor for regression. >>> from sklearn.gaussian_process import GaussianProcessRegressor >>> from sklearn.gaussian_process.kernels import RBF >>> kernel = RBF ( 1.5 ) >>> regressor = GaussianProcessRegressor ( kernel = kernel ) Now that we have both our Featurizer and regressor, we can construct a Predictor object. >>> from autocat.learning.predictors import Predictor >>> predictor = Predictor ( ... regressor = regressor , ... featurizer = featurizer , ... ) >>> predictor +-----------+------------------------------------------------------------------+ | | Predictor | +-----------+------------------------------------------------------------------+ | regressor | < class ' sklearn . gaussian_process . _gpr . GaussianProcessRegressor '> | | is fit ? | False | +-----------+------------------------------------------------------------------+ +-----------------------------------+-------------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------------+ | class | dscribe . descriptors . soap . SOAP | | kwargs | { 'rcut' : 7.0 , 'nmax' : 8 , 'lmax' : 8 } | | species list | [ 'Fe' , 'Ni' , 'Pt' , 'Pd' , 'Au' , 'Cu' , 'H' ] | | maximum structure size | 37 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------------+ Training and making predictions With our newly defined Predictor we can train it using data from our DesignSpace and the fit method. Again, please note we are using random labels here, solely for demonstration purposes. >>> train_structures = design_space . design_space_structures [: 5 ] >>> train_labels = design_space . design_space_labels [: 5 ] >>> predictor . fit ( train_structures , train_labels ) Making predictions is a similar process except using the predict method. >>> test_structures = design_space . design_space_structures [ 5 :] >>> predicted_labels = predictor . predict ( test_structures ) In this example, since we already have the labels for the test structures, we can also use the score method to calculate a prediction score. >>> test_labels = design_space . design_space_labels [ 5 :] >>> mae = predictor . score ( test_structures , test_labels )","title":"Training a Predictor on hydrogen adsorption energies"},{"location":"Tutorials/pred_h/#creating-a-designspace","text":"Let's start by creating a DesignSpace . Normally each of these structures would be optimized via DFT, but for demo purposes we'll use the generated structures directly. First we need to generate the single-atom alloys. Here, we can use AutoCat's generate_saa_structures function. >>> # Generate the clean single-atom alloy structures >>> from autocat.saa import generate_saa_structures >>> from autocat.utils import flatten_structures_dict >>> saa_struct_dict = generate_saa_structures ( ... [ \"Fe\" , \"Cu\" , \"Au\" ], ... [ \"Pt\" , \"Pd\" , \"Ni\" ], ... facets = { \"Fe\" :[ \"110\" ], \"Cu\" :[ \"111\" ], \"Au\" :[ \"111\" ]}, ... n_fixed_layers = 2 , ... ) >>> saa_structs = flatten_structures_dict ( saa_struct_dict ) Now that we have the clean structures, let's adsorb hydrogen on the surface. For convenience let's place H at the origin instead of considering all symmetry sites. To accomplish this we can make use of AutoCat's place_adsorbate function. >>> # Adsorb hydrogen onto each of the generated SAA surfaces >>> from autocat.adsorption import place_adsorbate >>> from ase import Atoms >>> ads_structs = [] >>> for clean_struct in saa_structs : ... ads_struct = place_adsorbate ( clean_struct , Atoms ( \"H\" )) ... ads_structs . append ( ads_struct ) This has collected all of the single-atom alloys with hydrogen adsorbed into a single list of ase.Atoms objects, ads_structs . Ideally at this stage we'd have adsorption energies for each of the generated structures after relaxation. As a proxy in this demo we'll create random labels, but this should be adsorption energies if you want to train a meaningful Predictor! >>> # Generate the labels for each structure >>> import numpy as np >>> labels = np . random . uniform ( - 1.5 , 1.5 , size = len ( ads_structs )) Finally, using both our structures and labels we can define a DesignSpace . In practice, if any of the labels for a structure are unknown, it can be included as a numpy.nan >>> from autocat.learning.sequential import DesignSpace >>> design_space = DesignSpace ( ads_structs , labels ) >>> design_space +-------------------------+-------------------------------------------+ | | DesignSpace | +-------------------------+-------------------------------------------+ | total # of systems | 9 | | # of unlabelled systems | 0 | | unique species present | [ 'Fe' , 'H' , 'Pt' , 'Pd' , 'Ni' , 'Cu' , 'Au' ] | | maximum label | 1.0173326963281424 | | minimum label | - 1.4789390894451206 | +-------------------------+-------------------------------------------+","title":"Creating a DesignSpace"},{"location":"Tutorials/pred_h/#setting-up-a-predictor","text":"When setting up our Predictor we now have two choices to make: The technique to be used for featurizing the systems The regression model to be used for training and predictions Internally, the Predictor will contain a Featurizer object (that the user supplies) which stores all of our choices for how to featurize the systems. Our choice of featurizer class and the associated kwargs are specified via the featurizer_class and kwargs arguments, respectively. By providing the design space structures some of the kwargs related to the featurization (e.g. maximum structure size) can be automatically obtained. Let's featurize the hydrogen environment via dscribe 's SOAP class >>> from autocat.learning.featurizers import Featurizer >>> from dscribe.descriptors.soap import SOAP >>> featurizer = Featurizer ( ... featurizer_class = SOAP , ... kwargs = { \"rcut\" : 7.0 , \"nmax\" : 8 , \"lmax\" : 8 }, ... design_space_structures = design_space . design_space_structures ... ) >>> featurizer +-----------------------------------+-------------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------------+ | class | dscribe . descriptors . soap . SOAP | | kwargs | { 'rcut' : 7.0 , 'nmax' : 8 , 'lmax' : 8 } | | species list | [ 'Fe' , 'Ni' , 'Pt' , 'Pd' , 'Au' , 'Cu' , 'H' ] | | maximum structure size | 37 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------------+ Similarly, we can specify the regressor to be used. The class should be \" sklearn -like\" with fit and predict methods. Here we will use sklearn 's GaussianProcessRegressor for regression. >>> from sklearn.gaussian_process import GaussianProcessRegressor >>> from sklearn.gaussian_process.kernels import RBF >>> kernel = RBF ( 1.5 ) >>> regressor = GaussianProcessRegressor ( kernel = kernel ) Now that we have both our Featurizer and regressor, we can construct a Predictor object. >>> from autocat.learning.predictors import Predictor >>> predictor = Predictor ( ... regressor = regressor , ... featurizer = featurizer , ... ) >>> predictor +-----------+------------------------------------------------------------------+ | | Predictor | +-----------+------------------------------------------------------------------+ | regressor | < class ' sklearn . gaussian_process . _gpr . GaussianProcessRegressor '> | | is fit ? | False | +-----------+------------------------------------------------------------------+ +-----------------------------------+-------------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------------+ | class | dscribe . descriptors . soap . SOAP | | kwargs | { 'rcut' : 7.0 , 'nmax' : 8 , 'lmax' : 8 } | | species list | [ 'Fe' , 'Ni' , 'Pt' , 'Pd' , 'Au' , 'Cu' , 'H' ] | | maximum structure size | 37 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------------+","title":"Setting up a Predictor"},{"location":"Tutorials/pred_h/#training-and-making-predictions","text":"With our newly defined Predictor we can train it using data from our DesignSpace and the fit method. Again, please note we are using random labels here, solely for demonstration purposes. >>> train_structures = design_space . design_space_structures [: 5 ] >>> train_labels = design_space . design_space_labels [: 5 ] >>> predictor . fit ( train_structures , train_labels ) Making predictions is a similar process except using the predict method. >>> test_structures = design_space . design_space_structures [ 5 :] >>> predicted_labels = predictor . predict ( test_structures ) In this example, since we already have the labels for the test structures, we can also use the score method to calculate a prediction score. >>> test_labels = design_space . design_space_labels [ 5 :] >>> mae = predictor . score ( test_structures , test_labels )","title":"Training and making predictions"},{"location":"Tutorials/sl/","text":"In this tutorial we will show how to conduct a simulated sequential learning run over a fully explored design space. Creating a fully explored DesignSpace Following a similar procedure as in the previous tutorial, we will create a fully explored DesignSpace (ie. no unknown labels). This time the structures will be clean mono-elemental surfaces which we can generate via generate_surface_structures . >>> # Generate the clean surfaces >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import flatten_structures_dict >>> surfs_dict = generate_surface_structures ( ... [ \"Pt\" , \"Cu\" , \"Li\" , \"Ti\" ], ... n_fixed_layers = 2 , ... default_lat_param_lib = \"pbe_fd\" ... ) >>> surfs = flatten_structures_dict ( surfs_dict ) In this case we specified that the default lattice parameters from the library calculated with the PBE XC functional and a finite difference basis set. As before, we will create random labels for all structures. But if you want meaningful sequential learning runs these must be actual labels relevant to your design space! >>> # Generate the labels for each structure >>> import numpy as np >>> labels = np . random . uniform ( - 1.5 , 1.5 , size = len ( surfs )) Taking the structures and labels we can define our DesignSpace . >>> from autocat.learning.sequential import DesignSpace >>> design_space = DesignSpace ( surfs , labels ) >>> design_space +-------------------------+--------------------------+ | | DesignSpace | +-------------------------+--------------------------+ | total # of systems | 10 | | # of unlabelled systems | 0 | | unique species present | [ 'Pt' , 'Cu' , 'Li' , 'Ti' ] | | maximum label | 1.1205404366846423 | | minimum label | - 1.3259701029215702 | +-------------------------+--------------------------+ Doing a single simulated sequential learning run Given our fully explored DesignSpace , we can simulate a sequential learning search over it to gain insights into guided searches within this context. To do this simulated run we can make use of the simulated_sequential_learning function. This will internally drive a SequentialLearner object which will be returned at the end of the run. As before, we will need to make choices with regard to the Predictor settings. In this case we will use a SineMatrix featurizer alongside a GaussianProcessRegressor . >>> from sklearn.gaussian_process import GaussianProcessRegressor >>> from sklearn.gaussian_process.kernels import RBF >>> from dscribe.descriptors.sinematrix import SineMatrix >>> from autocat.learning.featurizers import Featurizer >>> from autocat.learning.predictors import Predictor >>> kernel = RBF ( 1.5 ) >>> regressor = GaussianProcessRegressor ( kernel = kernel ) >>> featurizer = Featurizer ( ... featurizer_class = SineMatrix , ... design_space_structures = design_space . design_space_structures ... ) >>> predictor = Predictor ( regressor = regressor , featurizer = featurizer ) >>> predictor +-----------+------------------------------------------------------------------+ | | Predictor | +-----------+------------------------------------------------------------------+ | regressor | < class ' sklearn . gaussian_process . _gpr . GaussianProcessRegressor '> | | is fit ? | False | +-----------+------------------------------------------------------------------+ +-----------------------------------+-------------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------------+ | class | dscribe . descriptors . sinematrix . SineMatrix | | kwargs | None | | species list | [ 'Li' , 'Ti' , 'Pt' , 'Cu' ] | | maximum structure size | 36 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------------+ We also need to select parameters with regard to candidate selection. This includes the acquisition function to be used, target window (if applicable), and number of candidates to pick at each iteration. This can be done via the CandidateSelector object. Let's use a maximum uncertainty acquisition function to pick candidates based on their associated uncertainty values. >>> from autocat.learning.sequential import CandidateSelector >>> candidate_selector = CandidateSelector ( ... acquisition_function = \"MU\" , ... num_candidates_to_pick = 1 ... ) >>> candidate_selector +-------------------------------+--------------------+ | | Candidate Selector | +-------------------------------+--------------------+ | acquisition function | MU | | # of candidates to pick | 1 | | target window | None | | include hhi ? | False | | include segregation energies ? | False | +-------------------------------+--------------------+ Now we have everything we need to conduct a simulated sequential learning loop. We'll restrict the run to conduct 5 iterations. >>> from autocat.learning.sequential import simulated_sequential_learning >>> sim_seq_learn = simulated_sequential_learning ( ... full_design_space = design_space , ... candidate_selector = candidate_selector , ... predictor = predictor , ... init_training_size = 1 , ... number_of_sl_loops = 5 , ... ) >>> sim_seq_learn +----------------------------------+--------------------+ | | Sequential Learner | +----------------------------------+--------------------+ | iteration count | 6 | | next candidate system structures | [ 'Cu36' ] | | next candidate system indices | [ 5 ] | +----------------------------------+--------------------+ +-------------------------------+--------------------+ | | Candidate Selector | +-------------------------------+--------------------+ | acquisition function | MU | | # of candidates to pick | 1 | | target window | None | | include hhi ? | False | | include segregation energies ? | False | +-------------------------------+--------------------+ +-------------------------+--------------------------+ | | DesignSpace | +-------------------------+--------------------------+ | total # of systems | 10 | | # of unlabelled systems | 4 | | unique species present | [ 'Pt' , 'Cu' , 'Li' , 'Ti' ] | | maximum label | 0.9712050050259604 | | minimum label | - 1.3259701029215702 | +-------------------------+--------------------------+ +-----------+------------------------------------------------------------------+ | | Predictor | +-----------+------------------------------------------------------------------+ | regressor | < class ' sklearn . gaussian_process . _gpr . GaussianProcessRegressor '> | | is fit ? | True | +-----------+------------------------------------------------------------------+ +-----------------------------------+-------------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------------+ | class | dscribe . descriptors . sinematrix . SineMatrix | | kwargs | None | | species list | [ 'Li' , 'Ti' , 'Pt' , 'Cu' ] | | maximum structure size | 36 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------------+ Within the returned SequentialLearner object we now have information we can use for further analysis including prediction and uncertainty histories as well as the candidate selection history. Doing multiple simulated sequential learning runs It is often useful to consider the statistics of multiple independent simulated sequential learning runs. For this purpose we can make use of the multiple_simulated_sequential_learning_runs function. This acts in the same manner as for the single run verion, but will return a SequentialLearner object for each of the independent runs in a list. Moreover, the inputs remain the same except with the added option of running in parallel (since this is an embarrassingly parallel operation). Here we will conduct three independent runs in serial. >>> from autocat.learning.sequential import multiple_simulated_sequential_learning_runs >>> runs_history = multiple_simulated_sequential_learning_runs ( ... full_design_space = design_space , ... candidate_selector = candidate_selector , ... predictor = predictor , ... init_training_size = 1 , ... number_of_sl_loops = 5 , ... number_of_runs = 3 , ... # number_of_parallel_jobs=N if you wanted to run in parallel ... ) Taking the SequentialLearner s from within runs_history , their histories may be used to calculate more robust statistics into the simulated searches.","title":"Conducting a simulated sequential learning run"},{"location":"Tutorials/sl/#creating-a-fully-explored-designspace","text":"Following a similar procedure as in the previous tutorial, we will create a fully explored DesignSpace (ie. no unknown labels). This time the structures will be clean mono-elemental surfaces which we can generate via generate_surface_structures . >>> # Generate the clean surfaces >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import flatten_structures_dict >>> surfs_dict = generate_surface_structures ( ... [ \"Pt\" , \"Cu\" , \"Li\" , \"Ti\" ], ... n_fixed_layers = 2 , ... default_lat_param_lib = \"pbe_fd\" ... ) >>> surfs = flatten_structures_dict ( surfs_dict ) In this case we specified that the default lattice parameters from the library calculated with the PBE XC functional and a finite difference basis set. As before, we will create random labels for all structures. But if you want meaningful sequential learning runs these must be actual labels relevant to your design space! >>> # Generate the labels for each structure >>> import numpy as np >>> labels = np . random . uniform ( - 1.5 , 1.5 , size = len ( surfs )) Taking the structures and labels we can define our DesignSpace . >>> from autocat.learning.sequential import DesignSpace >>> design_space = DesignSpace ( surfs , labels ) >>> design_space +-------------------------+--------------------------+ | | DesignSpace | +-------------------------+--------------------------+ | total # of systems | 10 | | # of unlabelled systems | 0 | | unique species present | [ 'Pt' , 'Cu' , 'Li' , 'Ti' ] | | maximum label | 1.1205404366846423 | | minimum label | - 1.3259701029215702 | +-------------------------+--------------------------+","title":"Creating a fully explored DesignSpace"},{"location":"Tutorials/sl/#doing-a-single-simulated-sequential-learning-run","text":"Given our fully explored DesignSpace , we can simulate a sequential learning search over it to gain insights into guided searches within this context. To do this simulated run we can make use of the simulated_sequential_learning function. This will internally drive a SequentialLearner object which will be returned at the end of the run. As before, we will need to make choices with regard to the Predictor settings. In this case we will use a SineMatrix featurizer alongside a GaussianProcessRegressor . >>> from sklearn.gaussian_process import GaussianProcessRegressor >>> from sklearn.gaussian_process.kernels import RBF >>> from dscribe.descriptors.sinematrix import SineMatrix >>> from autocat.learning.featurizers import Featurizer >>> from autocat.learning.predictors import Predictor >>> kernel = RBF ( 1.5 ) >>> regressor = GaussianProcessRegressor ( kernel = kernel ) >>> featurizer = Featurizer ( ... featurizer_class = SineMatrix , ... design_space_structures = design_space . design_space_structures ... ) >>> predictor = Predictor ( regressor = regressor , featurizer = featurizer ) >>> predictor +-----------+------------------------------------------------------------------+ | | Predictor | +-----------+------------------------------------------------------------------+ | regressor | < class ' sklearn . gaussian_process . _gpr . GaussianProcessRegressor '> | | is fit ? | False | +-----------+------------------------------------------------------------------+ +-----------------------------------+-------------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------------+ | class | dscribe . descriptors . sinematrix . SineMatrix | | kwargs | None | | species list | [ 'Li' , 'Ti' , 'Pt' , 'Cu' ] | | maximum structure size | 36 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------------+ We also need to select parameters with regard to candidate selection. This includes the acquisition function to be used, target window (if applicable), and number of candidates to pick at each iteration. This can be done via the CandidateSelector object. Let's use a maximum uncertainty acquisition function to pick candidates based on their associated uncertainty values. >>> from autocat.learning.sequential import CandidateSelector >>> candidate_selector = CandidateSelector ( ... acquisition_function = \"MU\" , ... num_candidates_to_pick = 1 ... ) >>> candidate_selector +-------------------------------+--------------------+ | | Candidate Selector | +-------------------------------+--------------------+ | acquisition function | MU | | # of candidates to pick | 1 | | target window | None | | include hhi ? | False | | include segregation energies ? | False | +-------------------------------+--------------------+ Now we have everything we need to conduct a simulated sequential learning loop. We'll restrict the run to conduct 5 iterations. >>> from autocat.learning.sequential import simulated_sequential_learning >>> sim_seq_learn = simulated_sequential_learning ( ... full_design_space = design_space , ... candidate_selector = candidate_selector , ... predictor = predictor , ... init_training_size = 1 , ... number_of_sl_loops = 5 , ... ) >>> sim_seq_learn +----------------------------------+--------------------+ | | Sequential Learner | +----------------------------------+--------------------+ | iteration count | 6 | | next candidate system structures | [ 'Cu36' ] | | next candidate system indices | [ 5 ] | +----------------------------------+--------------------+ +-------------------------------+--------------------+ | | Candidate Selector | +-------------------------------+--------------------+ | acquisition function | MU | | # of candidates to pick | 1 | | target window | None | | include hhi ? | False | | include segregation energies ? | False | +-------------------------------+--------------------+ +-------------------------+--------------------------+ | | DesignSpace | +-------------------------+--------------------------+ | total # of systems | 10 | | # of unlabelled systems | 4 | | unique species present | [ 'Pt' , 'Cu' , 'Li' , 'Ti' ] | | maximum label | 0.9712050050259604 | | minimum label | - 1.3259701029215702 | +-------------------------+--------------------------+ +-----------+------------------------------------------------------------------+ | | Predictor | +-----------+------------------------------------------------------------------+ | regressor | < class ' sklearn . gaussian_process . _gpr . GaussianProcessRegressor '> | | is fit ? | True | +-----------+------------------------------------------------------------------+ +-----------------------------------+-------------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------------+ | class | dscribe . descriptors . sinematrix . SineMatrix | | kwargs | None | | species list | [ 'Li' , 'Ti' , 'Pt' , 'Cu' ] | | maximum structure size | 36 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------------+ Within the returned SequentialLearner object we now have information we can use for further analysis including prediction and uncertainty histories as well as the candidate selection history.","title":"Doing a single simulated sequential learning run"},{"location":"Tutorials/sl/#doing-multiple-simulated-sequential-learning-runs","text":"It is often useful to consider the statistics of multiple independent simulated sequential learning runs. For this purpose we can make use of the multiple_simulated_sequential_learning_runs function. This acts in the same manner as for the single run verion, but will return a SequentialLearner object for each of the independent runs in a list. Moreover, the inputs remain the same except with the added option of running in parallel (since this is an embarrassingly parallel operation). Here we will conduct three independent runs in serial. >>> from autocat.learning.sequential import multiple_simulated_sequential_learning_runs >>> runs_history = multiple_simulated_sequential_learning_runs ( ... full_design_space = design_space , ... candidate_selector = candidate_selector , ... predictor = predictor , ... init_training_size = 1 , ... number_of_sl_loops = 5 , ... number_of_runs = 3 , ... # number_of_parallel_jobs=N if you wanted to run in parallel ... ) Taking the SequentialLearner s from within runs_history , their histories may be used to calculate more robust statistics into the simulated searches.","title":"Doing multiple simulated sequential learning runs"},{"location":"User_Guide/Data/hhi/","text":"The Herfindahl-Hirschman Index (HHI) is an index that measures market concentration. Thus, in the context of different elements, it can be used as a proxy for cost, as proposed by M. Gaultois, et. al. . From the tabulated values in the reference above, we provide HHI values for both reserves as well as production.","title":"HHI"},{"location":"User_Guide/Data/intermediates/","text":"When characterizing a surface in the context of a specific reaction, calculating adsorption energies for all of the reaction intermediates is often important. Here, AutoCat has default structures for adsorbates of both the oxygen reduction reaction (ORR) and nitrogen reduction reaction (NRR) intermediates. The names of all of the reaction intermediates can be imported and fed directly into AutoCat functions: >>> from autocat.data.intermediates import ORR_INTERMEDIATE_NAMES >>> from autocat.data.intermediates import NRR_INTERMEDIATE_NAMES >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import extract_structures >>> from autocat.adsorption import generate_adsorbed_structures >>> pt_dict = generate_surface_structures ([ \"Pt\" ]) >>> pt_struct = extract_structures ( pt_dict )[ 0 ] >>> orr_structs = generate_adsorbed_structures ( ... surface = pt_struct , ... adsorbates = ORR_INTERMEDIATE_NAMES , ... use_all_sites = True ... ) >>> nrr_structs = generate_adsorbed_structures ( ... surface = pt_struct , ... ads = NRR_INTERMEDIATE_NAMES , ... use_all_sites = True ... ) In the above example, orr_structs and nrr_structs have all of the corresponding intermediates at every identified unique surface site. Alternatively, if you would like to access the ase.Atoms objects for the intermediates directly, they can be imported as a dict : >>> from autocat.data.intermediates import ORR_MOLS >>> from autocat.data.intermediates import NRR_MOLS ORR Intermediates : OOH*, O*, OH* NRR Intermediates : NNH*, NNH \\(_2\\) *, N*, NH*, NH \\(_2\\) *, NHNH*, NHNH \\(_2\\) *, NH \\(_2\\) NH \\(_2\\) *","title":"Reaction Intermediates"},{"location":"User_Guide/Data/lattice_parameters/","text":"In some codes, optimizing cell parameters on the fly during geometry relaxations is not available. For this reason we have compiled calculated lattice parameters using multiple different calculation schemes as a convenience for high-throughput studies. Every calculation was conducted with GPAW . There are two axes to the settings applied here: exchange-correlation functional basis set mode (finite difference or plane-wave). Available sets are as follows: BULK_PBE_FD / BULK_BEEFVDW_FD : These are parameters using the finite difference scheme and PBE / BEEF-vdW XC functionals. Obtained via fits to an equation of state (https://wiki.fysik.dtu.dk/ase/ase/eos.html) FCC/BCC h = 0.16, kpts = (12,12,12) fit to an SJ EOS HCP h=0.16, kpts = (12,12,6) fit to a Birch-Murnaghan EO BULK_PBE_PW / BULK_BEEFVDW_PW : These are parameters are obatined with a plane-wave basis set and using the Exponential Cell Filter to minimize the stress tensor and atomic forces (https://wiki.fysik.dtu.dk/ase/ase/constraints.html#the-expcellfilter-class) FCC/BCC mode=PW(550), kpts = (12,12,12), fmax = 0.05 eV/A HCP mode=PW(550), kpts = (12,12,6), fmax = 0.05 eV/A All of these lattice parameters are available within autocat.data.lattice_parameters","title":"Lattice Parameters"},{"location":"User_Guide/Data/segregation_energies/","text":"When determining the stability of dopants within a host, one important factor to consider is the segregation energy. This predicts the thermodynamic preference towards pinning the dopant at the surface of the host versus burying itself in the bulk. Segregation energy values are tabulated as reported by A.V. Ruban, et. al. for multiple combinations of transition metal hosts and dopants. By definition more negative values indicate more stability towards keeping the dopant at the surface. Values where the host is the same as the dopant is the surface energy for that species. In addition, for specifically SAAs, K. K. Rao, et. al. studied the stability of various different host and dopant combinations. The different configurations included SAA, subsurface, dimers, adatoms, and adatom + SAA. Here for most preferential configuration we attributed the following scores as per the results shown in figure 3 of the above reference: SAA is the most stable: 1 SAA is not the most stable but is within: <0.1 eV: 0.9 <0.2 eV: 0.8 <0.5 eV: 0.5 SAA is not the most stable by >0.5 eV: 0","title":"Segregation Energies"},{"location":"User_Guide/Learning/featurizers/","text":"The Featurizer object allows for the featurization of systems into a format that can be fed into machine learning models. Specified within this object are all the desired settings for when featurizing systems. More specifically this includes: featurizer_class : the desired class for featurization preset : if the featurizer class can be instantiated by a preset, that preset can be specified here. (e.g. the magpie feature set for the ElementProperty featurizer class) design_space_structures : if the design space is already known, the structures can be specified here to extract the max_size and species_list parameters. supercedes max_size and species_list upon instantiation max_size : the largest structure size that the featurizer can encounter species_list : all possible species that the featurizer can encounter Applying the Featurizer there are two main methods: featurize_single and featurize_multiple . The former is intended for featurizing a single structure. On the other hand, the latter can take multiple structures and returns them in a single feature matrix. Below are three examples using structure, site, and compositional featurization methods: >>> from autocat.learning.featurizers import Featurizer >>> from autocat.utils import flatten_structures_dict >>> from autocat.surface import generate_surface_structures >>> from dscribe.descriptors import SineMatrix >>> surfs = flatten_structures_dict ( generate_surface_structures ([ \"Li\" , \"Na\" ])) >>> f = Featurizer ( SineMatrix , design_space_structures = surfs ) >>> f +-----------------------------------+-------------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------------+ | class | dscribe . descriptors . sinematrix . SineMatrix | | kwargs | None | | species list | [ 'Na' , 'Li' ] | | maximum structure size | 36 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------------+ >>> X = f . featurize_multiple ( surfs ) >>> from ase import Atoms >>> from dscribe.descriptors import SOAP >>> from autocat.learning.featurizers import Featurizer >>> from autocat.utils import flatten_structures_dict >>> from autocat.surface import generate_surface_structures >>> from autocat.adsorption import place_adsorbate >>> surf = flatten_structures_dict ( generate_surface_structures ([ \"Cu\" ]))[ 0 ] >>> ads_struct = place_adsorbate ( surf , Atoms ( \"OH\" )) >>> f = Featurizer ( ... SOAP , ... max_size = 36 , ... species_list = [ \"Cu\" , \"O\" , \"H\" ], ... kwargs = { \"rcut\" : 6. , \"lmax\" : 8 , \"nmax\" : 8 } ... ) >>> f +-----------------------------------+-------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------+ | class | dscribe . descriptors . soap . SOAP | | kwargs | { 'rcut' : 6.0 , 'lmax' : 8 , 'nmax' : 8 } | | species list | [ 'Cu' , 'O' , 'H' ] | | maximum structure size | 36 | | preset | None | | design space structures provided ? | False | +-----------------------------------+-------------------------------------+ >>> X = f . featurize_single ( ads_struct ) >>> from autocat.learning.featurizers import Featurizer >>> from autocat.utils import flatten_structures_dict >>> from autocat.saa import generate_saa_structures >>> from matminer.featurizers.composition import ElementProperty >>> saas = flatten_structures_dict ( generate_saa_structures ([ \"Cu\" , \"Au\" ],[ \"Pt\" , \"Pd\" ])) >>> f = Featurizer ( ElementProperty , preset = \"magpie\" , design_space_structures = saas ) >>> f +-----------------------------------+------------------------------------------------------------+ | | Featurizer | +-----------------------------------+------------------------------------------------------------+ | class | matminer . featurizers . composition . composite . ElementProperty | | kwargs | None | | species list | [ 'Pt' , 'Pd' , 'Au' , 'Cu' ] | | maximum structure size | 36 | | preset | magpie | | design space structures provided ? | True | +-----------------------------------+------------------------------------------------------------+ >>> X = f . featurize_multiple ( saas ) The goal of this Featurizer object is to provide a unified class across different featurization techniques. At present the following featurizer classes are supported: dscribe : SineMatrix CoulombMatrix ACSF SOAP matminer : ElementProperty ChemicalSRO OPSiteFingerprint CrystalNNFingerprint N.B. ACSF , SOAP , CrystalNNFingerprint , OPSiteFingerprint , and ChemicalSRO are all implemented to featurize locally around specified atoms indicated with ase.Atoms.tags <= 0 . The remaining implemented featurizer classes consider the full structure by definition","title":"Featurizers"},{"location":"User_Guide/Learning/predictors/","text":"In order to iterate a sequential learning pipeline, a regressor is needed to select subsequent candidate systems. For this purpose, there is the Predictor object class. This contains two key attributes: a regressor that can be fit to data and used for predictions (the class provided must have fit and predict methods) a Featurizer to be used for featurizing the structures. There are two currently implemented approaches, structure methods that featurize the entire structure (e.g. SineMatrix , ElementProperty ) and adsorbate methods that featurize locally (e.g. SOAP ). Generally, this predictor object behaves similarly to regressors found in sklearn with its own fit , predict , and score methods. As an example, let's train a random forest regressor on some single atom alloys. >>> import numpy as np >>> from dscribe.descriptors import SineMatrix >>> from sklearn.ensemble import RandomForestRegressor >>> from autocat.saa import generate_saa_structures >>> from autocat.utils import flatten_structures_dict >>> from autocat.learning.featurizers import Featurizer >>> from autocat.learning.predictors import Predictor >>> saa_dict = generate_saa_structures ([ \"Cu\" , \"Au\" , \"Fe\" ], [ \"Pt\" , \"Ru\" , \"Ni\" ]) >>> saa_structs = flatten_structures_dict ( saa_dict ) >>> labels = np . random . randint ( 1 , size = ( len ( saa_structs ) - 1 )) >>> featurizer = Featurizer ( ... featurizer_class = SineMatrix ... ) >>> regressor = RandomForestRegressor () >>> acp = Predictor ( ... regressor = regressor , ... featurizer = featurizer , ... ) >>> acp . fit ( saa_structs [: - 1 ], labels ) >>> acp +-----------+----------------------------------------------------------+ | | Predictor | +-----------+----------------------------------------------------------+ | regressor | < class ' sklearn . ensemble . _forest . RandomForestRegressor '> | | is fit ? | True | +-----------+----------------------------------------------------------+ +-----------------------------------+----------------------------------------------------+ | | Featurizer | +-----------------------------------+----------------------------------------------------+ | class | dscribe . descriptors . sinematrix . SineMatrix | | kwargs | None | | species list | [ 'Fe' , 'Ni' , 'Pt' , 'Pd' , 'Cu' , 'C' , 'N' , 'O' , 'H' ] | | maximum structure size | 100 | | preset | None | | design space structures provided ? | False | +-----------------------------------+----------------------------------------------------+ >>> pred , _ = acp . predict ([ saa_structs [ - 1 ]]) >>> pred array ([ 0. ]) Here we have chosen to featurize the structures as a SineMatrix . Note as well that the predict method will return uncertainty estimates if available. To see this, let's train a gaussian process regressor with an RBF kernel. Let's also featurize using SOAP to see how featurization kwargs are passed >>> import numpy as np >>> from ase import Atoms >>> from dscribe.descriptors import SOAP >>> from sklearn.gaussian_process import GaussianProcessRegressor >>> from sklearn.gaussian_process.kernels import RBF >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import flatten_structures_dict >>> from autocat.adsorption import place_adsorbate >>> from autocat.learning.featurizers import Featurizer >>> from autocat.learning.predictors import Predictor >>> subs = flatten_structures_dict ( generate_surface_structures ([ \"Pt\" , \"Fe\" , \"Ru\" ])) >>> structs = [ place_adsorbate ( s , Atoms ( \"OH\" )) for s in subs ] >>> labels = np . random . randint ( 1 , size = ( len ( structs ) - 1 )) >>> featurizer = Featurizer ( ... featurizer_class = SOAP , ... design_space_structures = structs , ... kwargs = { \"rcut\" : 6.0 , \"nmax\" : 6 , \"lmax\" : 6 } ... ) >>> kernel = RBF () >>> regressor = GaussianProcessRegressor ( kernel = kernel ) >>> acp = Predictor ( ... featurizer = featurizer , ... regressor = regressor ... ) >>> acp . fit ( structs [: - 1 ], labels ) >>> acp +-----------+------------------------------------------------------------------+ | | Predictor | +-----------+------------------------------------------------------------------+ | regressor | < class ' sklearn . gaussian_process . _gpr . GaussianProcessRegressor '> | | is fit ? | True | +-----------+------------------------------------------------------------------+ +-----------------------------------+-------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------+ | class | dscribe . descriptors . soap . SOAP | | kwargs | { 'rcut' : 6.0 , 'nmax' : 6 , 'lmax' : 6 } | | species list | [ 'Fe' , 'Ru' , 'Pt' , 'O' , 'H' ] | | maximum structure size | 38 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------+ >>> pred , unc = acp . predict ([ structs [ - 1 ]]) >>> pred array ([ 0. ]) >>> unc array ([ 1. ])","title":"Predictors"},{"location":"User_Guide/Learning/sequential/","text":"DesignSpace The DesignSpace class object is intended to store the entire design space. As the sequential learning loop is iterated, this can be continuously updated with the newly found labels. There are two key components required for this object: design_space_structures : all systems to be considered as ase.Atoms objects in a list design_space_labels : numpy array of the same length as the above list with the corresponding labels. If the label is not yet known, set it to numpy.nan NB: The order of the list of design space structures must be in the same order as the labels given in the design space labels. >>> import numpy as np >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import flatten_structures_dict >>> from autocat.learning.sequential import DesignSpace >>> surf_dict = generate_surface_structures ([ \"Pt\" , \"Pd\" , \"Cu\" , \"Ni\" ]) >>> surf_structs = flatten_structures_dict ( surf_dict ) >>> labels = np . array ([ 0.95395024 , 0.63504885 , np . nan , 0.08320879 , np . nan , ... 0.32423194 , 0.55570785 , np . nan , np . nan , np . nan , ... 0.18884186 , np . nan ]) >>> acds = DesignSpace ( surf_structs , labels ) >>> acds +-------------------------+--------------------------+ | | DesignSpace | +-------------------------+--------------------------+ | total # of systems | 12 | | # of unlabelled systems | 6 | | unique species present | [ 'Pt' , 'Pd' , 'Cu' , 'Ni' ] | | maximum label | 0.95395024 | | minimum label | 0.08320879 | +-------------------------+--------------------------+ >>> len ( acds ) 12 >>> acds . design_space_structures [ Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... )] >>> acds . design_space_labels array ([ 0.95395024 , 0.63504885 , nan , 0.08320879 , nan , 0.32423194 , 0.55570785 , nan , nan , nan , 0.18884186 , nan ]) CandidateSelector The CandidateSelector object stores information about the methodology for candidate selection, and can apply this to choose candidates from a design space. Key properties specified within this object include: Acquisition function to be used for calculating scores. Currently supported functions: maximum likelihood of improvement (MLI) maximum uncertainty (MU) random Number of candidates that should be proposed for each iteration Target window that the candidate should ideally fall within (this is only applicable for MLI) Whether to weight each system's score by its HHI and/or segregation energies For example, let's define a CandidateSelector that chooses the 3 systems based on MLI with a target window of between 0.25 and 0.3, and weights the scores by the HHI values. >>> from autocat.learning.sequential import CandidateSelector >>> candidate_selector = CandidateSelector ( ... acquisition_function = \"MLI\" , ... num_candidates_to_pick = 3 , ... target_window = ( 0.25 , 0.3 ), ... include_hhi = True , ... ) >>> candidate_selector +----------------------------------+--------------------+ | | Candidate Selector | +----------------------------------+--------------------+ | acquisition function | MLI | | # of candidates to pick | 3 | | target window | [ 0.25 0.3 ] | | include hhi ? | True | | hhi type | production | | include segregation energies ? | False | | segregation energies data source | raban1999 | +----------------------------------+--------------------+ The method choose_candidate applies these options to calculate the scores and propose the desired number of candidate systems to evaluate. A DesignSpace must be supplied along with optionally a combination of predictions and/or uncertainties depending on the acquisition function chosen. Using the DesignSpace above, and making up some prediction and uncertainty values (in practice these should be from your own trained Predictor !), we can see how this works. >>> predictions = np . array ([ 0.95395024 , 0.63504885 , 0.46160089 , 0.08320879 , 0.81524182 , ... 0.32423194 , 0.55570785 , 0.75537232 , 0.21824507 , 0.89147292 , ... 0.18884186 , 0.47473003 ]) >>> uncertainties = np . array ([ 0.01035017 , 0.01171273 , 0.00688497 , 0.00514248 , 0.01254998 , ... 0.01047033 , 0.01268476 , 0.01017691 , 0.01436907 , 0.00878836 , ... 0.00786345 , 0.01341667 ]) >>> parent_idx , max_scores , aq_scores = candidate_selector . choose_candidate ( ... design_space = acds , ... predictions = predictions , ... uncertainties = uncertainties ... ) Here, parent_idx is the indices of the proposed candidate systems in the given DesignSpace , max_scores are the scores attributed to these identified candidates, and aq_scores are the scores for all systems. N.B. : If there are np.nan labels within the DesignSpace , by default the candidates will be chosen exclusively from these unlabelled systems. Otherwise, in the case of a fully labelled DesignSpace the default is to consider all systems. However, these defaults may be overridden via the allowed_idx parameter. SequentialLearner The SequentialLearner object stores information regarding the latest iteration of the sequential learning loop including: A Predictor (and its kwargs for both the regressor and featurizer) A CandidateSelector for choosing the candidate systems Iteration number Latest DesignSpace Candidate system(s) that is identified for the next loop. Histories for predictions, uncertainties, and training indices This object can be thought of as a central hub for the sequential learning workflow, with an external driver (either automated or manual) triggering iteration. The first iterate trains the model and identifies candidate(s) to start the loop. >>> import numpy as np >>> from ase import Atoms >>> from dscribe.descriptors import SOAP >>> from sklearn.gaussian_process import GaussianProcessRegressor >>> from sklearn.gaussian_process.kernels import RBF >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import flatten_structures_dict >>> from autocat.adsorption import place_adsorbate >>> from autocat.learning.featurizers import Featurizer >>> from autocat.learning.predictors import Predictor >>> from autocat.learning.sequential import CandidateSelector >>> from autocat.learning.sequential import DesignSpace >>> from autocat.learning.sequential import SequentialLearner >>> # make the DesignSpace >>> subs_dict = generate_surface_structures ([ \"Pt\" , \"Pd\" , \"Cu\" , \"Ni\" ]) >>> subs = flatten_structures_dict ( subs_dict ) >>> ads_structs = [ place_adsorbate ( s , Atoms ( \"Li\" )) for s in subs ] >>> labels = np . array ([ 0.95395024 , 0.63504885 , np . nan , 0.08320879 , np . nan , ... 0.32423194 , 0.55570785 , np . nan , np . nan , np . nan , ... 0.18884186 , np . nan ]) >>> acds = DesignSpace ( ads_structs , labels ) >>> # specify the featurization details >>> featurizer = Featurizer ( ... featurizer_class = SOAP , ... design_space_structures = acds . design_space_structures , ... kwargs = { \"rcut\" : 5.0 , \"lmax\" : 6 , \"nmax\" : 6 } ... ) >>> # define the predictor >>> kernel = RBF () >>> regressor = GaussianProcessRegressor ( kernel = kernel ) >>> predictor = Predictor ( ... regressor = regressor , ... featurizer = featurizer ... ) >>> # choose how candidates will be selected on each loop >>> candidate_selector = CandidateSelector ( ... acquisition_function = \"MLI\" , ... target_window = ( 0.1 , 0.2 ), ... include_hhi = True , ... hhi_type = \"reserves\" , ... include_segregation_energies = False ... ) >>> # set up the sequential learner >>> acsl = SequentialLearner ( ... design_space = acds , ... predictor = predictor , ... candidate_selector = candidate_selector , ... ) >>> acsl . iteration_count 0 >>> acsl . iterate () >>> acsl . iteration_count 1 >>> acsl +----------------------------------+--------------------+ | | Sequential Learner | +----------------------------------+--------------------+ | iteration count | 1 | | next candidate system structures | [ 'Cu36Li' ] | | next candidate system indices | [ 7 ] | +----------------------------------+--------------------+ +----------------------------------+--------------------+ | | Candidate Selector | +----------------------------------+--------------------+ | acquisition function | MLI | | # of candidates to pick | 1 | | target window | [ 0.1 0.2 ] | | include hhi ? | True | | hhi type | reserves | | include segregation energies ? | False | | segregation energies data source | raban1999 | +----------------------------------+--------------------+ +-------------------------+--------------------------------+ | | DesignSpace | +-------------------------+--------------------------------+ | total # of systems | 12 | | # of unlabelled systems | 6 | | unique species present | [ 'Li' , 'Pt' , 'Pd' , 'Cu' , 'Ni' ] | | maximum label | 0.95395024 | | minimum label | 0.08320879 | +-------------------------+--------------------------------+ +-----------+------------------------------------------------------------------+ | | Predictor | +-----------+------------------------------------------------------------------+ | regressor | < class ' sklearn . gaussian_process . _gpr . GaussianProcessRegressor '> | | is fit ? | True | +-----------+------------------------------------------------------------------+ +-----------------------------------+-------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------+ | class | dscribe . descriptors . soap . SOAP | | kwargs | { 'rcut' : 5.0 , 'lmax' : 6 , 'nmax' : 6 } | | species list | [ 'Li' , 'Ni' , 'Pt' , 'Pd' , 'Cu' ] | | maximum structure size | 37 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------+ Simulated Sequential Learning If you already have a fully explored design space and want to simulate exploration over it, the simulated_sequential_learning function may be used. Internally this function acts a driver on a SequentialLearner object, and can be viewed as an example for how a driver can be set up for an exploratory simulated sequential learning loop. As inputs it requires all parameters needed to instantiate a SequentialLearner and returns the object that has been iterated. For further analysis of the search, histories of the predictions, uncertainties, and the training indices for each iteration are kept. >>> import numpy as np >>> from dscribe.descriptors import SineMatrix >>> from sklearn.gaussian_process import GaussianProcessRegressor >>> from sklearn.gaussian_process.kernels import RBF >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import flatten_structures_dict >>> from autocat.learning.featurizers import Featurizer >>> from autocat.learning.predictors import Predictor >>> from autocat.learning.sequential import CandidateSelector >>> from autocat.learning.sequential import DesignSpace >>> from autocat.learning.sequential import simulated_sequential_learning >>> surf_dict = generate_surface_structures ([ \"Pt\" , \"Pd\" , \"Cu\" , \"Ni\" ]) >>> surf_structs = flatten_structures_dict ( surf_dict ) >>> labels = np . array ([ 0.95395024 , 0.63504885 , 0.4567 , 0.08320879 , 0.87779 , ... 0.32423194 , 0.55570785 , 0.325 , 0.43616 , 0.321632 , ... 0.18884186 , 0.1114 ]) >>> acds = DesignSpace ( surf_structs , labels ) >>> # specify the featurization details >>> featurizer = Featurizer ( ... featurizer_class = SineMatrix , ... design_space_structures = acds . design_space_structures , ... ) >>> # define the predictor >>> kernel = RBF () >>> regressor = GaussianProcessRegressor ( kernel = kernel ) >>> predictor = Predictor ( ... regressor = regressor , ... featurizer = featurizer ... ) >>> # choose how candidates will be selected on each loop >>> candidate_selector = CandidateSelector ( ... acquisition_function = \"MLI\" , ... target_window = ( 0.1 , 0.2 ), ... include_hhi = True , ... hhi_type = \"reserves\" , ... include_segregation_energies = False ... ) >>> # conduct the simulated sequential learning loop >>> sim_sl = simulated_sequential_learning ( ... full_design_space = acds , ... predictor = predictor , ... candidate_selector = candidate_selector , ... init_training_size = 5 , ... number_of_sl_loops = 3 , ... ) Sequential Learning Iteration #1 Sequential Learning Iteration #2 Sequential Learning Iteration #3 Additionally, simulated searches are typically most useful when repeated to obtain statistics that are less dependent on the initialization of the design space. For this purpose there is the multiple_simulated_sequential_learning_runs function. This returns a list of SequentialLearner corresponding to each individual run. Optionally, this function can also initiate the multiple runs across parallel processes via the number_of_parallel_jobs parameter. >>> import numpy as np >>> from matminer.featurizers.composition import ElementProperty >>> from sklearn.gaussian_process import GaussianProcessRegressor >>> from sklearn.gaussian_process.kernels import RBF >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import flatten_structures_dict >>> from autocat.learning.featurizers import Featurizer >>> from autocat.learning.predictors import Predictor >>> from autocat.learning.sequential import CandidateSelector >>> from autocat.learning.sequential import DesignSpace >>> from autocat.learning.sequential import multiple_simulated_sequential_learning_runs >>> surf_dict = generate_surface_structures ([ \"Pt\" , \"Pd\" , \"Cu\" , \"Ni\" ]) >>> surf_structs = flatten_structures_dict ( surf_dict ) >>> labels = np . array ([ 0.95395024 , 0.63504885 , 0.4567 , 0.08320879 , 0.87779 , ... 0.32423194 , 0.55570785 , 0.325 , 0.43616 , 0.321632 , ... 0.18884186 , 0.1114 ]) >>> acds = DesignSpace ( surf_structs , labels ) >>> # specify the featurization details >>> featurizer = Featurizer ( ... featurizer_class = ElementProperty , ... preset = \"matminer\" , ... design_space_structures = acds . design_space_structures , ... ) >>> # define the predictor >>> kernel = RBF () >>> regressor = GaussianProcessRegressor ( kernel = kernel ) >>> predictor = Predictor ( ... regressor = regressor , ... featurizer = featurizer ... ) >>> # choose how candidates will be selected on each loop >>> candidate_selector = CandidateSelector ( ... acquisition_function = \"MLI\" , ... target_window = ( 0.1 , 0.2 ), ... include_hhi = True , ... hhi_type = \"reserves\" , ... include_segregation_energies = False ... ) >>> # conduct the multiple simulated sequential learning loop >>> multi_sim_sl = multiple_simulated_sequential_learning_runs ( ... full_design_space = acds , ... predictor = predictor , ... candidate_selector = candidate_selector , ... init_training_size = 5 , ... number_of_sl_loops = 2 , ... number_of_runs = 3 , ... ) Sequential Learning Iteration #1 Sequential Learning Iteration #2 Sequential Learning Iteration #1 Sequential Learning Iteration #2 Sequential Learning Iteration #1 Sequential Learning Iteration #2 >>> len ( multi_sim_sl ) 3","title":"Sequential"},{"location":"User_Guide/Learning/sequential/#designspace","text":"The DesignSpace class object is intended to store the entire design space. As the sequential learning loop is iterated, this can be continuously updated with the newly found labels. There are two key components required for this object: design_space_structures : all systems to be considered as ase.Atoms objects in a list design_space_labels : numpy array of the same length as the above list with the corresponding labels. If the label is not yet known, set it to numpy.nan NB: The order of the list of design space structures must be in the same order as the labels given in the design space labels. >>> import numpy as np >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import flatten_structures_dict >>> from autocat.learning.sequential import DesignSpace >>> surf_dict = generate_surface_structures ([ \"Pt\" , \"Pd\" , \"Cu\" , \"Ni\" ]) >>> surf_structs = flatten_structures_dict ( surf_dict ) >>> labels = np . array ([ 0.95395024 , 0.63504885 , np . nan , 0.08320879 , np . nan , ... 0.32423194 , 0.55570785 , np . nan , np . nan , np . nan , ... 0.18884186 , np . nan ]) >>> acds = DesignSpace ( surf_structs , labels ) >>> acds +-------------------------+--------------------------+ | | DesignSpace | +-------------------------+--------------------------+ | total # of systems | 12 | | # of unlabelled systems | 6 | | unique species present | [ 'Pt' , 'Pd' , 'Cu' , 'Ni' ] | | maximum label | 0.95395024 | | minimum label | 0.08320879 | +-------------------------+--------------------------+ >>> len ( acds ) 12 >>> acds . design_space_structures [ Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... ), Atoms ( ... )] >>> acds . design_space_labels array ([ 0.95395024 , 0.63504885 , nan , 0.08320879 , nan , 0.32423194 , 0.55570785 , nan , nan , nan , 0.18884186 , nan ])","title":"DesignSpace"},{"location":"User_Guide/Learning/sequential/#candidateselector","text":"The CandidateSelector object stores information about the methodology for candidate selection, and can apply this to choose candidates from a design space. Key properties specified within this object include: Acquisition function to be used for calculating scores. Currently supported functions: maximum likelihood of improvement (MLI) maximum uncertainty (MU) random Number of candidates that should be proposed for each iteration Target window that the candidate should ideally fall within (this is only applicable for MLI) Whether to weight each system's score by its HHI and/or segregation energies For example, let's define a CandidateSelector that chooses the 3 systems based on MLI with a target window of between 0.25 and 0.3, and weights the scores by the HHI values. >>> from autocat.learning.sequential import CandidateSelector >>> candidate_selector = CandidateSelector ( ... acquisition_function = \"MLI\" , ... num_candidates_to_pick = 3 , ... target_window = ( 0.25 , 0.3 ), ... include_hhi = True , ... ) >>> candidate_selector +----------------------------------+--------------------+ | | Candidate Selector | +----------------------------------+--------------------+ | acquisition function | MLI | | # of candidates to pick | 3 | | target window | [ 0.25 0.3 ] | | include hhi ? | True | | hhi type | production | | include segregation energies ? | False | | segregation energies data source | raban1999 | +----------------------------------+--------------------+ The method choose_candidate applies these options to calculate the scores and propose the desired number of candidate systems to evaluate. A DesignSpace must be supplied along with optionally a combination of predictions and/or uncertainties depending on the acquisition function chosen. Using the DesignSpace above, and making up some prediction and uncertainty values (in practice these should be from your own trained Predictor !), we can see how this works. >>> predictions = np . array ([ 0.95395024 , 0.63504885 , 0.46160089 , 0.08320879 , 0.81524182 , ... 0.32423194 , 0.55570785 , 0.75537232 , 0.21824507 , 0.89147292 , ... 0.18884186 , 0.47473003 ]) >>> uncertainties = np . array ([ 0.01035017 , 0.01171273 , 0.00688497 , 0.00514248 , 0.01254998 , ... 0.01047033 , 0.01268476 , 0.01017691 , 0.01436907 , 0.00878836 , ... 0.00786345 , 0.01341667 ]) >>> parent_idx , max_scores , aq_scores = candidate_selector . choose_candidate ( ... design_space = acds , ... predictions = predictions , ... uncertainties = uncertainties ... ) Here, parent_idx is the indices of the proposed candidate systems in the given DesignSpace , max_scores are the scores attributed to these identified candidates, and aq_scores are the scores for all systems. N.B. : If there are np.nan labels within the DesignSpace , by default the candidates will be chosen exclusively from these unlabelled systems. Otherwise, in the case of a fully labelled DesignSpace the default is to consider all systems. However, these defaults may be overridden via the allowed_idx parameter.","title":"CandidateSelector"},{"location":"User_Guide/Learning/sequential/#sequentiallearner","text":"The SequentialLearner object stores information regarding the latest iteration of the sequential learning loop including: A Predictor (and its kwargs for both the regressor and featurizer) A CandidateSelector for choosing the candidate systems Iteration number Latest DesignSpace Candidate system(s) that is identified for the next loop. Histories for predictions, uncertainties, and training indices This object can be thought of as a central hub for the sequential learning workflow, with an external driver (either automated or manual) triggering iteration. The first iterate trains the model and identifies candidate(s) to start the loop. >>> import numpy as np >>> from ase import Atoms >>> from dscribe.descriptors import SOAP >>> from sklearn.gaussian_process import GaussianProcessRegressor >>> from sklearn.gaussian_process.kernels import RBF >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import flatten_structures_dict >>> from autocat.adsorption import place_adsorbate >>> from autocat.learning.featurizers import Featurizer >>> from autocat.learning.predictors import Predictor >>> from autocat.learning.sequential import CandidateSelector >>> from autocat.learning.sequential import DesignSpace >>> from autocat.learning.sequential import SequentialLearner >>> # make the DesignSpace >>> subs_dict = generate_surface_structures ([ \"Pt\" , \"Pd\" , \"Cu\" , \"Ni\" ]) >>> subs = flatten_structures_dict ( subs_dict ) >>> ads_structs = [ place_adsorbate ( s , Atoms ( \"Li\" )) for s in subs ] >>> labels = np . array ([ 0.95395024 , 0.63504885 , np . nan , 0.08320879 , np . nan , ... 0.32423194 , 0.55570785 , np . nan , np . nan , np . nan , ... 0.18884186 , np . nan ]) >>> acds = DesignSpace ( ads_structs , labels ) >>> # specify the featurization details >>> featurizer = Featurizer ( ... featurizer_class = SOAP , ... design_space_structures = acds . design_space_structures , ... kwargs = { \"rcut\" : 5.0 , \"lmax\" : 6 , \"nmax\" : 6 } ... ) >>> # define the predictor >>> kernel = RBF () >>> regressor = GaussianProcessRegressor ( kernel = kernel ) >>> predictor = Predictor ( ... regressor = regressor , ... featurizer = featurizer ... ) >>> # choose how candidates will be selected on each loop >>> candidate_selector = CandidateSelector ( ... acquisition_function = \"MLI\" , ... target_window = ( 0.1 , 0.2 ), ... include_hhi = True , ... hhi_type = \"reserves\" , ... include_segregation_energies = False ... ) >>> # set up the sequential learner >>> acsl = SequentialLearner ( ... design_space = acds , ... predictor = predictor , ... candidate_selector = candidate_selector , ... ) >>> acsl . iteration_count 0 >>> acsl . iterate () >>> acsl . iteration_count 1 >>> acsl +----------------------------------+--------------------+ | | Sequential Learner | +----------------------------------+--------------------+ | iteration count | 1 | | next candidate system structures | [ 'Cu36Li' ] | | next candidate system indices | [ 7 ] | +----------------------------------+--------------------+ +----------------------------------+--------------------+ | | Candidate Selector | +----------------------------------+--------------------+ | acquisition function | MLI | | # of candidates to pick | 1 | | target window | [ 0.1 0.2 ] | | include hhi ? | True | | hhi type | reserves | | include segregation energies ? | False | | segregation energies data source | raban1999 | +----------------------------------+--------------------+ +-------------------------+--------------------------------+ | | DesignSpace | +-------------------------+--------------------------------+ | total # of systems | 12 | | # of unlabelled systems | 6 | | unique species present | [ 'Li' , 'Pt' , 'Pd' , 'Cu' , 'Ni' ] | | maximum label | 0.95395024 | | minimum label | 0.08320879 | +-------------------------+--------------------------------+ +-----------+------------------------------------------------------------------+ | | Predictor | +-----------+------------------------------------------------------------------+ | regressor | < class ' sklearn . gaussian_process . _gpr . GaussianProcessRegressor '> | | is fit ? | True | +-----------+------------------------------------------------------------------+ +-----------------------------------+-------------------------------------+ | | Featurizer | +-----------------------------------+-------------------------------------+ | class | dscribe . descriptors . soap . SOAP | | kwargs | { 'rcut' : 5.0 , 'lmax' : 6 , 'nmax' : 6 } | | species list | [ 'Li' , 'Ni' , 'Pt' , 'Pd' , 'Cu' ] | | maximum structure size | 37 | | preset | None | | design space structures provided ? | True | +-----------------------------------+-------------------------------------+","title":"SequentialLearner"},{"location":"User_Guide/Learning/sequential/#simulated-sequential-learning","text":"If you already have a fully explored design space and want to simulate exploration over it, the simulated_sequential_learning function may be used. Internally this function acts a driver on a SequentialLearner object, and can be viewed as an example for how a driver can be set up for an exploratory simulated sequential learning loop. As inputs it requires all parameters needed to instantiate a SequentialLearner and returns the object that has been iterated. For further analysis of the search, histories of the predictions, uncertainties, and the training indices for each iteration are kept. >>> import numpy as np >>> from dscribe.descriptors import SineMatrix >>> from sklearn.gaussian_process import GaussianProcessRegressor >>> from sklearn.gaussian_process.kernels import RBF >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import flatten_structures_dict >>> from autocat.learning.featurizers import Featurizer >>> from autocat.learning.predictors import Predictor >>> from autocat.learning.sequential import CandidateSelector >>> from autocat.learning.sequential import DesignSpace >>> from autocat.learning.sequential import simulated_sequential_learning >>> surf_dict = generate_surface_structures ([ \"Pt\" , \"Pd\" , \"Cu\" , \"Ni\" ]) >>> surf_structs = flatten_structures_dict ( surf_dict ) >>> labels = np . array ([ 0.95395024 , 0.63504885 , 0.4567 , 0.08320879 , 0.87779 , ... 0.32423194 , 0.55570785 , 0.325 , 0.43616 , 0.321632 , ... 0.18884186 , 0.1114 ]) >>> acds = DesignSpace ( surf_structs , labels ) >>> # specify the featurization details >>> featurizer = Featurizer ( ... featurizer_class = SineMatrix , ... design_space_structures = acds . design_space_structures , ... ) >>> # define the predictor >>> kernel = RBF () >>> regressor = GaussianProcessRegressor ( kernel = kernel ) >>> predictor = Predictor ( ... regressor = regressor , ... featurizer = featurizer ... ) >>> # choose how candidates will be selected on each loop >>> candidate_selector = CandidateSelector ( ... acquisition_function = \"MLI\" , ... target_window = ( 0.1 , 0.2 ), ... include_hhi = True , ... hhi_type = \"reserves\" , ... include_segregation_energies = False ... ) >>> # conduct the simulated sequential learning loop >>> sim_sl = simulated_sequential_learning ( ... full_design_space = acds , ... predictor = predictor , ... candidate_selector = candidate_selector , ... init_training_size = 5 , ... number_of_sl_loops = 3 , ... ) Sequential Learning Iteration #1 Sequential Learning Iteration #2 Sequential Learning Iteration #3 Additionally, simulated searches are typically most useful when repeated to obtain statistics that are less dependent on the initialization of the design space. For this purpose there is the multiple_simulated_sequential_learning_runs function. This returns a list of SequentialLearner corresponding to each individual run. Optionally, this function can also initiate the multiple runs across parallel processes via the number_of_parallel_jobs parameter. >>> import numpy as np >>> from matminer.featurizers.composition import ElementProperty >>> from sklearn.gaussian_process import GaussianProcessRegressor >>> from sklearn.gaussian_process.kernels import RBF >>> from autocat.surface import generate_surface_structures >>> from autocat.utils import flatten_structures_dict >>> from autocat.learning.featurizers import Featurizer >>> from autocat.learning.predictors import Predictor >>> from autocat.learning.sequential import CandidateSelector >>> from autocat.learning.sequential import DesignSpace >>> from autocat.learning.sequential import multiple_simulated_sequential_learning_runs >>> surf_dict = generate_surface_structures ([ \"Pt\" , \"Pd\" , \"Cu\" , \"Ni\" ]) >>> surf_structs = flatten_structures_dict ( surf_dict ) >>> labels = np . array ([ 0.95395024 , 0.63504885 , 0.4567 , 0.08320879 , 0.87779 , ... 0.32423194 , 0.55570785 , 0.325 , 0.43616 , 0.321632 , ... 0.18884186 , 0.1114 ]) >>> acds = DesignSpace ( surf_structs , labels ) >>> # specify the featurization details >>> featurizer = Featurizer ( ... featurizer_class = ElementProperty , ... preset = \"matminer\" , ... design_space_structures = acds . design_space_structures , ... ) >>> # define the predictor >>> kernel = RBF () >>> regressor = GaussianProcessRegressor ( kernel = kernel ) >>> predictor = Predictor ( ... regressor = regressor , ... featurizer = featurizer ... ) >>> # choose how candidates will be selected on each loop >>> candidate_selector = CandidateSelector ( ... acquisition_function = \"MLI\" , ... target_window = ( 0.1 , 0.2 ), ... include_hhi = True , ... hhi_type = \"reserves\" , ... include_segregation_energies = False ... ) >>> # conduct the multiple simulated sequential learning loop >>> multi_sim_sl = multiple_simulated_sequential_learning_runs ( ... full_design_space = acds , ... predictor = predictor , ... candidate_selector = candidate_selector , ... init_training_size = 5 , ... number_of_sl_loops = 2 , ... number_of_runs = 3 , ... ) Sequential Learning Iteration #1 Sequential Learning Iteration #2 Sequential Learning Iteration #1 Sequential Learning Iteration #2 Sequential Learning Iteration #1 Sequential Learning Iteration #2 >>> len ( multi_sim_sl ) 3","title":"Simulated Sequential Learning"},{"location":"User_Guide/Structure_Generation/adsorption/","text":"Tools within autocat.adsorption are geared towards generating structures with adsorbates placed on a candidate catalyst surface. The core function of this module is generate_adsorbed_structures for generating multiple adsorbed structures with a single function call. For the oxygen reduction (ORR) and nitrogen reduction (NRR) reactions, AutoCat has default starting geometries for all of these intermediates which can be found in autocat.data.intermediates . In addition, by default initial heights of the adsorbates are guessed based upon the vdW radii of the nearest neighbors to the anchoring atom. In the example below we are generating adsorption structures for all ORR intermediates on all of the identified unique symmetry sites on a Pt111 slab. The unique sites are identified using the Delaunay triangulation, as implemented in pymatgen . Additionally, by default initial heights of the adsorbates are guessed based upon the vdW radii of the nearest neighbors to the anchoring atom. >>> from autocat.surface import generate_surface_structures >>> from autocat.data.intermediates import ORR_INTERMEDIATE_NAMES >>> from autocat.adsorption import generate_adsorbed_structures >>> surface_dict = generate_surface_structures ( ... species_list = [ \"Pt\" ], facets = { \"Pt\" : [ \"111\" ]}, n_fixed_layers = 2 ... ) >>> surface = surface_dict [ \"Pt\" ][ \"fcc111\" ][ \"structure\" ] >>> ads_dict = generate_adsorbed_structures ( ... surface = surface , ... use_all_sites = True , ... ads = ORR_INTERMEDIATE_NAMES , ... write_to_disk = True , ... ) Structure with OOH adsorbed at ontop / 0.0_0.0 written to ./ adsorbates / OOH / ontop / 0.0_0.0 / input . traj Structure with OOH adsorbed at bridge / 8.316_2.4 written to ./ adsorbates / OOH / bridge / 8.316_2.4 / input . traj Structure with OOH adsorbed at hollow / 8.316_1.6 written to ./ adsorbates / OOH / hollow / 8.316_1.6 / input . traj Structure with OOH adsorbed at hollow / 5.544_3.201 written to ./ adsorbates / OOH / hollow / 5.544_3.201 / input . traj Structure with O adsorbed at ontop / 0.0_0.0 written to ./ adsorbates / O / ontop / 0.0_0.0 / input . traj Structure with O adsorbed at bridge / 8.316_2.4 written to ./ adsorbates / O / bridge / 8.316_2.4 / input . traj Structure with O adsorbed at hollow / 8.316_1.6 written to ./ adsorbates / O / hollow / 8.316_1.6 / input . traj Structure with O adsorbed at hollow / 5.544_3.201 written to ./ adsorbates / O / hollow / 5.544_3.201 / input . traj Structure with OH adsorbed at ontop / 0.0_0.0 written to ./ adsorbates / OH / ontop / 0.0_0.0 / input . traj Structure with OH adsorbed at bridge / 8.316_2.4 written to ./ adsorbates / OH / bridge / 8.316_2.4 / input . traj Structure with OH adsorbed at hollow / 8.316_1.6 written to ./ adsorbates / OH / hollow / 8.316_1.6 / input . traj Structure with OH adsorbed at hollow / 5.544_3.201 written to ./ adsorbates / OH / hollow / 5.544_3.201 / input . traj >>> ads_dict { 'OOH' : { 'ontop' : { '0.0_0.0' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/OOH/ontop/0.0_0.0/input.traj' }}, 'bridge' : { '7.623_6.001' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/OOH/bridge/7.623_6.001/input.traj' }}, 'hollow' : { '6.93_5.601' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/OOH/hollow/6.93_5.601/input.traj' }, '9.702_4.001' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/OOH/hollow/9.702_4.001/input.traj' }}}, 'O' : { 'ontop' : { '0.0_0.0' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/O/ontop/0.0_0.0/input.traj' }}, 'bridge' : { '7.623_6.001' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/O/bridge/7.623_6.001/input.traj' }}, 'hollow' : { '6.93_5.601' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/O/hollow/6.93_5.601/input.traj' }, '9.702_4.001' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/O/hollow/9.702_4.001/input.traj' }}}, 'OH' : { 'ontop' : { '0.0_0.0' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/OH/ontop/0.0_0.0/input.traj' }}, 'bridge' : { '7.623_6.001' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/OH/bridge/7.623_6.001/input.traj' }}, 'hollow' : { '6.93_5.601' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/OH/hollow/6.93_5.601/input.traj' }, '9.702_4.001' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/OH/hollow/9.702_4.001/input.traj' }}}, In general the dictionary generated has the following organization: {ADSORBATE_SPECIES: {SITE_LABEL: {XY: {\"structure\": Atoms, \"traj_file_path\": TRAJFILEPATH}}}, When writing these adsorbed structures to disk it is done with the following subdirectory format (mimicing the organization of the dictionary). . \u251c\u2500\u2500 adsorbates \u2502 \u251c\u2500\u2500 O \u2502 \u2502 \u251c\u2500\u2500 bridge \u2502 \u2502 \u2502 \u2514\u2500\u2500 7.623_6.001 \u2502 \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2502 \u251c\u2500\u2500 hollow \u2502 \u2502 \u2502 \u251c\u2500\u2500 6.93_5.601 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2502 \u2502 \u2514\u2500\u2500 9.702_4.001 \u2502 \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2502 \u2514\u2500\u2500 ontop \u2502 \u2502 \u2514\u2500\u2500 0.0_0.0 \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u251c\u2500\u2500 OH \u2502 \u2502 \u251c\u2500\u2500 bridge \u2502 \u2502 \u2502 \u2514\u2500\u2500 7.623_6.001 \u2502 \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2502 \u251c\u2500\u2500 hollow \u2502 \u2502 \u2502 \u251c\u2500\u2500 6.93_5.601 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2502 \u2502 \u2514\u2500\u2500 9.702_4.001 \u2502 \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2502 \u2514\u2500\u2500 ontop \u2502 \u2502 \u2514\u2500\u2500 0.0_0.0 \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2514\u2500\u2500 OOH \u2502 \u251c\u2500\u2500 bridge \u2502 \u2502 \u2514\u2500\u2500 7.623_6.001 \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u251c\u2500\u2500 hollow \u2502 \u2502 \u251c\u2500\u2500 6.93_5.601 \u2502 \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2502 \u2514\u2500\u2500 9.702_4.001 \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2514\u2500\u2500 ontop \u2502 \u2514\u2500\u2500 0.0_0.0 \u2502 \u2514\u2500\u2500 input.traj Instead of generating the adsorption structures for all unique sites, the xy coordinates of individual sites may be specified using the adsorption_sites parameter. Here we can give each of these sites custom labels to be used for referencing and writing to disk. >>> from autocat.surface import generate_surface_structures >>> from autocat.adsorption import generate_adsorbed_structures >>> surface_dict = generate_surface_structures ( ... species_list = [ \"Pt\" ], facets = { \"Pt\" : [ \"111\" ]}, n_fixed_layers = 2 ... ) >>> surface = surface_dict [ \"Pt\" ][ \"fcc111\" ][ \"structure\" ] >>> x = surface [ 15 ] . x >>> x 4.1577878733769 >>> y = surface [ 15 ] . y >>> y 5.6011665451642 >>> sites = { \"Li\" : { \"custom\" : [( x , y )]}} >>> ads_dict = generate_adsorbed_structures ( ... surface = surface , ... adsorbates = [ \"Li\" ], ... use_all_sites = False , ... adsorption_sites = site , ... write_to_disk = True , ... ) Structure with Li adsorbed at custom / 4.158_5.601 written to ./ adsorbates / Li / custom / 4.158_5.601 / input . traj >>> ads_dict { 'Li' : { 'custom' : { '4.158_5.601' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './adsorbates/Li/custom/4.158_5.601/input.traj' }}}} If we are dealing with multiple adsorbates, adsorption sites, heights, etc.. that we want to treat differently depending on the combination, we can leverage the dict option for each of these inputs. The example below illustrates this capability, where can be used to specify adsorbates. >>> from autocat.surface import generate_surface_structures >>> from autocat.adsorption import generate_adsorbed_structures >>> surface_dict = generate_surface_structures ( ... species_list = [ \"Pt\" ], facets = { \"Pt\" : [ \"111\" ]}, n_fixed_layers = 2 ... ) >>> surface = surface_dict [ \"Pt\" ][ \"fcc111\" ][ \"structure\" ] >>> sites = { \"Li\" : { \"origin\" : [( 0. , 0. )]}, \"H\" : { \"custom\" : [( 0.5 , 0.5 )]}} >>> ads_dict = generate_adsorbed_structures ( ... surface = surface , ... adsorbates = [ \"Li\" , \"H\" , \"N\" ], ... use_all_sites = { \"Li\" : False , \"H\" : False , \"N\" : True }, ... heights = { \"H\" : 1.2 } ... adsorption_sites = sites , ... write_to_disk = True , ... )","title":"Adsorption"},{"location":"User_Guide/Structure_Generation/bulk/","text":"autocat.bulk provides tools to automatically generate mono-element bulk structures. These are structures containing only a single chemical species with no vacuum and 3D periodicity. Multiple of these systems can be generated and written to disk via a single call of generate_bulk_structures . >>> from autocat.bulk import generate_bulk_structures >>> bulk_dict = generate_bulk_structures ([ \"Pt\" , \"Fe\" , \"Ru\" ], write_to_disk = True ) Pt_bulk_fcc structure written to ./ Pt_bulk_fcc / input . traj Fe_bulk_bcc structure written to ./ Fe_bulk_bcc / input . traj Ru_bulk_hcp structure written to ./ Ru_bulk_hcp / input . traj >>> bulk_dict { 'Pt' : { 'crystal_structure' : Atoms ( ... ), 'traj_file_path' : './Pt_bulk_fcc/input.traj' }, 'Fe' : { 'crystal_structure' : Atoms ( ... ), 'traj_file_path' : './Fe_bulk_bcc/input.traj' }, 'Ru' : { 'crystal_structure' : Atoms ( ... ), 'traj_file_path' : './Ru_bulk_hcp/input.traj' }} In general the following structure of the resulting dict is generated: {SPECIES: {\"crystal_structure\": Atoms, \"traj_file_path\": TRAJFILEPATH}} If writing to disk structures to disk via write_to_disk = True , then the following directory structure then a similar organization is maintained on the disk: . \u251c\u2500\u2500 Fe_bulk_bcc \u2502 \u2514\u2500\u2500 input.traj \u251c\u2500\u2500 Pt_bulk_fcc \u2502 \u2514\u2500\u2500 input.traj \u251c\u2500\u2500 Ru_bulk_hcp \u2502 \u2514\u2500\u2500 input.traj where each input.traj contains the bulk structure. N.B. by default initial magnetic moments will be set for Fe, Co, and Ni, otherwise no spin will be given","title":"Bulk"},{"location":"User_Guide/Structure_Generation/saa/","text":"Single atom alloys (SAA) consist of a transition-metal host with lone dopant atoms embedded at the surface. This dispersion leads to unique electronic properties. With the autocat.saa module, we can generate structures of these systems to study them further. The main function for this purpose is generate_saa_structures where multiple SAA structures can be generated simultaneously. >>> from autocat.saa import generate_saa_structures >>> saa_dict = generate_saa_structures ( ... host_species = [ \"Fe\" , \"Cu\" ], ... dopant_species = [ \"Pt\" , \"Au\" ], ... facets = { \"Fe\" : [ \"110\" ], \"Cu\" : [ \"111\" ]}, ... n_fixed_layers = 2 , ... write_to_disk = True , ... ) Pt1 / Fe ( bcc110 ) structure written to ./ Fe / Pt / bcc110 / substrate / input . traj Au1 / Fe ( bcc110 ) structure written to ./ Fe / Au / bcc110 / substrate / input . traj Pt1 / Cu ( fcc111 ) structure written to ./ Cu / Pt / fcc111 / substrate / input . traj Au1 / Cu ( fcc111 ) structure written to ./ Cu / Au / fcc111 / substrate / input . traj >>> saa_dict { 'Fe' : { 'Pt' : { 'bcc110' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './Fe/Pt/bcc110/substrate/input.traj' }}, 'Au' : { 'bcc110' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './Fe/Au/bcc110/substrate/input.traj' }}}, 'Cu' : { 'Pt' : { 'fcc111' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './Cu/Pt/fcc111/substrate/input.traj' }}, 'Au' : { 'fcc111' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './Cu/Au/fcc111/substrate/input.traj' }}}} Here we generated SAA slabs with Fe and Cu as hosts and Pt and Au dopants under the following conditions: for Fe (Cu) we only need the 110 (111) facet the bottom 2 layers are held fixed When writing to disk the following directory structure is used: . \u251c\u2500\u2500 Cu \u2502 \u251c\u2500\u2500 Au \u2502 \u2502 \u2514\u2500\u2500 fcc111 \u2502 \u2502 \u2514\u2500\u2500 substrate \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2514\u2500\u2500 Pt \u2502 \u2514\u2500\u2500 fcc111 \u2502 \u2514\u2500\u2500 substrate \u2502 \u2514\u2500\u2500 input.traj \u251c\u2500\u2500 Fe \u2502 \u251c\u2500\u2500 Au \u2502 \u2502 \u2514\u2500\u2500 bcc110 \u2502 \u2502 \u2514\u2500\u2500 substrate \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2514\u2500\u2500 Pt \u2502 \u2514\u2500\u2500 bcc110 \u2502 \u2514\u2500\u2500 substrate \u2502 \u2514\u2500\u2500 input.traj N.B. by default, initial magnetic moments are given to the dopant species based upon the ground state magnetic moment of the species","title":"Single Atom Alloys"},{"location":"User_Guide/Structure_Generation/surface/","text":"It is crucial for many heterogeneous catalysis studies to be able to model a catalyst surface where the desired reaction can take place. autocat.surface provides tools for generating low miller index surfaces for mono-element surfaces with a vacuum in the \\(z\\) -direction. The core function of this module is generate_surface_structures where multiple slabs can be generated at once. >>> from autocat.surface import generate_surface_structures >>> surf_dict = generate_surface_structures ( ... [ \"Li\" , \"Cu\" ], ... facets = { \"Li\" : [ \"110\" ]}, ... supercell_dim = [ 5 , 5 , 4 ], ... n_fixed_layers = 2 , ... default_lat_param_lib = \"beefvdw_fd\" , ... write_to_disk = True , ... ) Li_bcc110 structure written to ./ Li / bcc110 / substrate / input . traj Cu_fcc100 structure written to ./ Cu / fcc100 / substrate / input . traj Cu_fcc111 structure written to ./ Cu / fcc111 / substrate / input . traj Cu_fcc110 structure written to ./ Cu / fcc110 / substrate / input . traj >>> surf_dict { 'Li' : { 'bcc110' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './Li/bcc110/substrate/input.traj' }}, 'Cu' : { 'fcc100' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './Cu/fcc100/substrate/input.traj' }, 'fcc111' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './Cu/fcc111/substrate/input.traj' }, 'fcc110' : { 'structure' : Atoms ( ... ), 'traj_file_path' : './Cu/fcc110/substrate/input.traj' }}} Here we generated surface slabs for Cu and Li under the following conditions: for Li we only need the 110 facet generate all default facets for Cu fcc/bcc: [\"100\", \"110\", \"111\"] hcp: [\"0001\"] the supercell dimensions of the slabs are 5 \\(\\times\\) 5 \\(\\times\\) 4 the bottom 2 layers are held fixed for structures where the lattice parameter is not explicitly specified, their default values are pulled from the autocat.data.lattice_parameters library that used a BEEF-vdW XC and finite difference basis set When using the write_to_disk functionality the structures will be written into the following directory structure: . \u251c\u2500\u2500 Cu \u2502 \u251c\u2500\u2500 fcc100 \u2502 \u2502 \u2514\u2500\u2500 substrate \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u251c\u2500\u2500 fcc110 \u2502 \u2502 \u2514\u2500\u2500 substrate \u2502 \u2502 \u2514\u2500\u2500 input.traj \u2502 \u2514\u2500\u2500 fcc111 \u2502 \u2514\u2500\u2500 substrate \u2502 \u2514\u2500\u2500 input.traj \u251c\u2500\u2500 Li \u2502 \u2514\u2500\u2500 bcc110 \u2502 \u2514\u2500\u2500 substrate \u2502 \u2514\u2500\u2500 input.traj N.B. by default, initial magnetic moments are given to Fe, Ni and Co","title":"Surfaces"}]}